#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass report
\begin_preamble
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "urlcolor=urlcolor,linkcolor=linkcolor,citecolor=citecolor,"
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{chapter}{3}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Least Squares: Finite Sample Theory
\end_layout

\begin_layout Standard
We continue with properties of OLS.
 We will show that OLS coincides with the maximum likelihood estimator if
 the error term follows a normal distribution.
 We derive its finite-sample exact distribution which can be used for statistica
l inference.
 The Gauss-Markov theorem justifies the optimality of OLS under the classical
 assumptions.
\end_layout

\begin_layout Section
Maximum Likelihood
\end_layout

\begin_layout Standard
There are very few 
\emph on
principles
\emph default
 in statistics, and maximum likelihood is one of them.
 In this chapter, we first give an introduction of the maximum likelihood
 estimation.
 Consider a random sample of 
\begin_inset Formula $Z=\left(z_{1},z_{2},\ldots,z_{n}\right)$
\end_inset

 drawn from a parametric distribution with density 
\begin_inset Formula $f_{z}\left(z_{i};\theta\right)$
\end_inset

, where 
\begin_inset Formula $z_{i}$
\end_inset

 is either a scalar random variable or a random vector.
 A parametric distribution is completely characterized by a finite-dimensional
 parameter 
\begin_inset Formula $\theta$
\end_inset

.
 We know that 
\begin_inset Formula $\theta$
\end_inset

 belongs to a parameter space 
\begin_inset Formula $\Theta$
\end_inset

.
 We use the data to estimate 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
The log-likelihood of observing the entire sample 
\begin_inset Formula $Z$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
L_{n}\left(\theta;Z\right):=\log\left(\prod_{i=1}^{n}f_{z}\left(z_{i};\theta\right)\right)=\sum_{i=1}^{n}\log f_{z}\left(z_{i};\theta\right).\label{eq:raw_likelihood}
\end{equation}

\end_inset

In reality the sample 
\begin_inset Formula $Z$
\end_inset

 is given and for each 
\begin_inset Formula $\theta\in\Theta$
\end_inset

 we can evaluate 
\begin_inset Formula $L_{n}\left(\theta;Z\right)$
\end_inset

.
 The maximum likelihood estimator is 
\begin_inset Formula 
\[
\widehat{\theta}_{MLE}:=\arg\max_{\theta\in\Theta}L_{n}\left(\theta;Z\right).
\]

\end_inset

Why maximizing the log-likelihood function is desirable? An intuitive explanatio
n is that 
\begin_inset Formula $\widehat{\theta}_{MLE}$
\end_inset

 makes observing 
\begin_inset Formula $Z$
\end_inset

 the 
\begin_inset Quotes eld
\end_inset

most likely
\begin_inset Quotes erd
\end_inset

 in the entire parametric space.
 
\end_layout

\begin_layout Standard
A more formal justification requires an explicitly defined distance.
 Suppose that the true parameter value that generates the data is 
\begin_inset Formula $\theta_{0}$
\end_inset

, so that the true distribution is 
\begin_inset Formula $f_{z}\left(z_{i};\theta_{0}\right)$
\end_inset

.
 Any generic point 
\begin_inset Formula $\theta\in\Theta$
\end_inset

 produces 
\begin_inset Formula $f_{z}\left(z_{i};\theta\right)$
\end_inset

.
 To measure their difference, we introduce the 
\emph on
Kullback-Leibler divergence
\emph default
, or the Kullback-Leibler distance, defined as the logarithms of the expected
 log-likelihood ratio
\begin_inset Formula 
\begin{align*}
D_{f}\left(\theta_{0}\Vert\theta\right) & =D\left(f_{z}\left(z_{i};\theta_{0}\right)\Vert f_{z}\left(z_{i};\theta\right)\right):=E_{\theta_{0}}\left[\log\frac{f_{z}\left(z_{i};\theta_{0}\right)}{f_{z}\left(z_{i};\theta\right)}\right]\\
 & =E_{\theta_{0}}\left[\log f_{z}\left(z_{i};\theta_{0}\right)\right]-E_{\theta_{0}}\left[\log f_{z}\left(z_{i};\theta\right)\right].
\end{align*}

\end_inset

We call it a 
\begin_inset Quotes eld
\end_inset

distance
\begin_inset Quotes erd
\end_inset

 because it is non-negative, although it is not symmetric in that 
\begin_inset Formula $D_{f}\left(\theta_{1}\Vert\theta_{2}\right)\neq D_{f}\left(\theta_{2}\vert\theta_{1}\right)$
\end_inset

 and it does not satisfy the triangle inequality.
 To see 
\begin_inset Formula $D_{f}\left(\theta_{0}\Vert\theta\right)$
\end_inset

 is non-negative, notice that 
\begin_inset Formula $-\log\left(\cdot\right)$
\end_inset

 is strictly convex and then by Jensen's inequality
\begin_inset Formula 
\begin{align*}
E_{\theta_{0}}\left[\log\frac{f_{z}\left(z_{i};\theta_{0}\right)}{f_{z}\left(z_{i};\theta\right)}\right] & =E_{\theta_{0}}\left[-\log\frac{f_{z}\left(z_{i};\theta\right)}{f_{z}\left(z_{i};\theta_{0}\right)}\right]\geq-\log\left(E_{\theta_{0}}\left[\frac{f_{z}\left(z_{i};\theta\right)}{f_{z}\left(z_{i};\theta_{0}\right)}\right]\right)\\
 & =-\log\left(\int\frac{f_{z}\left(z_{i};\theta\right)}{f_{z}\left(z_{i};\theta_{0}\right)}f_{z}\left(z_{i};\theta_{0}\right)dz_{i}\right)=-\log\left(\int f_{z}\left(z_{i};\theta\right)dz_{i}\right)\\
 & =-\log1=0,
\end{align*}

\end_inset

where 
\begin_inset Formula $\int f_{z}\left(z_{i};\theta\right)dz_{i}=1$
\end_inset

 for any pdf.
 The equality holds if and only if 
\begin_inset Formula $f_{z}\left(z_{i};\theta\right)=f_{z}\left(z_{i};\theta_{0}\right)$
\end_inset

 almost everywhere.
 Furthermore, if there is a one-to-one mapping between 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $f_{z}\left(z_{i};\theta\right)$
\end_inset

 on 
\begin_inset Formula $\Theta$
\end_inset

 (identification), then 
\begin_inset Formula $\theta_{0}=\arg\min_{\theta\in\Theta}D_{f}\left(\theta_{0}\Vert\theta\right)$
\end_inset

 is the unique solution.
\end_layout

\begin_layout Standard
In information theory, 
\begin_inset Formula $-E_{\theta_{0}}\left[\log f_{z}\left(z_{i};\theta_{0}\right)\right]$
\end_inset

 is the 
\emph on
entropy
\emph default
 of the continuous distribution of 
\begin_inset Formula $f_{z}\left(z_{i};\theta_{0}\right)$
\end_inset

.
 Entropy measures the uncertainty of a random variable; the larger is the
 value, the more chaotic is the random variable.
 The Kullback-Leibler distance is the 
\emph on
relative entropy
\emph default
 between the distribution 
\begin_inset Formula $f_{z}\left(z_{i};\theta_{0}\right)$
\end_inset

 and 
\begin_inset Formula $f_{z}\left(z_{i};\theta\right)$
\end_inset

.
 It measures the inefficiency of assuming that the distribution is 
\begin_inset Formula $f_{z}\left(z_{i};\theta\right)$
\end_inset

 when the true distribution is indeed 
\begin_inset Formula $f_{z}\left(z_{i};\theta_{0}\right)$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citep
after "p.19"
key "cover2006elements"
literal "false"

\end_inset


\end_layout

\begin_layout Example
Consider the Gaussian location model 
\begin_inset Formula $z_{i}\sim N\left(\mu,1\right)$
\end_inset

, where 
\begin_inset Formula $\mu$
\end_inset

 is the unknown parameter to be estimated.
 The likelihood of observing 
\begin_inset Formula $z_{i}$
\end_inset

 is 
\begin_inset Formula $f_{z}\left(z_{i};\mu\right)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(z_{i}-\mu\right)^{2}\right)$
\end_inset

.
 The likelihood of observing the sample 
\begin_inset Formula $Z$
\end_inset

 is 
\begin_inset Formula 
\[
f_{Z}\left(Z;\mu\right)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(z_{i}-\mu\right)^{2}\right)
\]

\end_inset

 and the log-likelihood is 
\begin_inset Formula 
\[
L_{n}\left(\mu;Z\right)=-\frac{n}{2}\log\left(2\pi\right)-\frac{1}{2}\sum_{i=1}^{n}\left(z_{i}-\mu\right)^{2}.
\]

\end_inset

The (averaged) log-likelihood function for the 
\begin_inset Formula $n$
\end_inset

 observations is 
\begin_inset Formula 
\begin{align*}
\ell_{n}\left(\mu\right) & =-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2n}\sum_{i=1}^{n}\left(z_{i}-\mu\right)^{2}.
\end{align*}

\end_inset

We work with the averaged log-likelihood 
\begin_inset Formula $\ell_{n}$
\end_inset

, instead of the (raw) log-likelihood 
\begin_inset Formula $L_{n}$
\end_inset

, to make it directly comparable with the expected log density 
\begin_inset Formula 
\begin{align*}
E_{\mu_{0}}\left[\log f_{z}\left(z;\mu\right)\right] & =E_{\mu_{0}}\left[\ell_{n}\left(\mu\right)\right]\\
 & =-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2}E_{\mu_{0}}\left[\left(z_{i}-\mu\right)^{2}\right]\\
 & =-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2}E_{\mu_{0}}\left[\left(\left(z_{i}-\mu_{0}\right)+\left(\mu_{0}-\mu\right)\right)^{2}\right]\\
 & =-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2}E_{\mu_{0}}\left[\left(z_{i}-\mu_{0}\right)^{2}\right]-E_{\mu_{0}}\left[z_{i}-\mu_{0}\right]\left(\mu_{0}-\mu\right)-\frac{1}{2}\left(\mu_{0}-\mu\right)^{2}\\
 & =-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2}-\frac{1}{2}\left(\mu-\mu_{0}\right)^{2}.
\end{align*}

\end_inset

where the first equality holds because of random sampling.
 Obviously, 
\begin_inset Formula $\ell_{n}\left(\mu\right)$
\end_inset

 is maximized at 
\begin_inset Formula $\bar{z}=\frac{1}{n}\sum_{i=1}^{n}z_{i}$
\end_inset

 while 
\begin_inset Formula $E_{\mu_{0}}\left[\ell_{n}\left(\mu\right)\right]$
\end_inset

 is maximized at 
\begin_inset Formula $\mu=\mu_{0}$
\end_inset

.
 The Kullback-Leibler divergence in this example is 
\begin_inset Formula 
\[
D\left(\mu_{0}\Vert\mu\right)=E_{\mu_{0}}\left[\ell_{n}\left(\mu_{0}\right)\right]-E_{\mu_{0}}\left[\ell_{n}\left(\mu\right)\right]=\frac{1}{2}\left(\mu-\mu_{0}\right)^{2},
\]

\end_inset

 where 
\begin_inset Formula $-E_{\mu_{0}}\left[\ell_{n}\left(\mu_{0}\right)\right]=\frac{1}{2}\left(\log\left(2\pi\right)+1\right)$
\end_inset

 is the entropy of the normal distribution with unit variance.
\end_layout

\begin_layout Example
We use the following code to demonstrate the population log-likelihood 
\begin_inset Formula $E\left[\ell_{n}\left(\mu\right)\right]$
\end_inset

 when 
\begin_inset Formula $\mu_{0}=2$
\end_inset

 (solid line) and the 3 sample realizations when 
\begin_inset Formula $n=4$
\end_inset

 (dashed lines).
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<graph-size, echo=FALSE>>= 
\end_layout

\begin_layout Plain Layout

my_pdf = function(file, width, height) {
\end_layout

\begin_layout Plain Layout

  pdf(file, width = 5, height = 5) 
\end_layout

\begin_layout Plain Layout

} 
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<mle, dev='my_pdf', fig.ext='pdf'>>=
\end_layout

\begin_layout Plain Layout

set.seed(2020-10-7)
\end_layout

\begin_layout Plain Layout

mu0 <- 2; gamma0 <- 1
\end_layout

\begin_layout Plain Layout

# population likelihood function
\end_layout

\begin_layout Plain Layout

L <- function(mu) {
\end_layout

\begin_layout Plain Layout

  ell = -0.5 * log(2*pi*gamma0) - 0.5 / gamma0 *( 1 + (mu - mu0)^2 ) 
\end_layout

\begin_layout Plain Layout

  return(ell) }
\end_layout

\begin_layout Plain Layout

# sample likelihood function 
\end_layout

\begin_layout Plain Layout

Ln <- function(mu) {
\end_layout

\begin_layout Plain Layout

  elln = -0.5 * log(2*pi*gamma0) - 0.5 / gamma0 * mean( (z - mu)^2 )  
\end_layout

\begin_layout Plain Layout

  return(elln) }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

mu_base = mu0 + seq(-3, 3, by = 0.01)
\end_layout

\begin_layout Plain Layout

# draw sample log-likelihood graph
\end_layout

\begin_layout Plain Layout

n = 4
\end_layout

\begin_layout Plain Layout

lnz = matrix(0, length(mu_base), 3)
\end_layout

\begin_layout Plain Layout

for (rr in 1:3){
\end_layout

\begin_layout Plain Layout

  z <- rnorm(n, mu0, sqrt(gamma0) )
\end_layout

\begin_layout Plain Layout

  lnz[,rr] <- plyr::laply(.data = mu_base, .fun = Ln)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

# draw the graph
\end_layout

\begin_layout Plain Layout

matplot(x = mu_base, y = cbind( L(mu_base), lnz), 
\end_layout

\begin_layout Plain Layout

        type = "l", lty = c(1, rep(2,3)), 
\end_layout

\begin_layout Plain Layout

        lwd =  c(2,rep(1,3)), col = 1:4, ylim = c(-6, -1), 
\end_layout

\begin_layout Plain Layout

        xlab = "mu", ylab = "log-likelihood")
\end_layout

\begin_layout Plain Layout

abline(v = mu0, lty = 3)
\end_layout

\begin_layout Plain Layout

abline(h = L(mu0), lty = 3)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Section
Likelihood Estimation for Regression
\end_layout

\begin_layout Standard
Notation: 
\begin_inset Formula $y_{i}$
\end_inset

 is a scalar, and 
\begin_inset Formula $x_{i}=\left(x_{i1},\ldots,x_{iK}\right)'$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 vector.
 
\begin_inset Formula $Y$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector, and 
\begin_inset Formula $X$
\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 matrix.
\end_layout

\begin_layout Standard
In this chapter we employ the classical statistical framework under restrictive
 distributional assumption 
\begin_inset Formula 
\begin{equation}
y_{i}|x_{i}\sim N\left(x_{i}'\beta,\gamma\right),\label{eq:normal_yx}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\gamma=\sigma^{2}$
\end_inset

 to ease the differentiation.
 This assumption is equivalent to 
\begin_inset Formula $e_{i}|x_{i}=\left(y_{i}-x_{i}'\beta\right)|x_{i}\sim N\left(0,\gamma\right)$
\end_inset

.
 Because the distribution of 
\begin_inset Formula $e_{i}$
\end_inset

 is invariant to 
\begin_inset Formula $x_{i}$
\end_inset

, the error term 
\begin_inset Formula $e_{i}\sim N\left(0,\gamma\right)$
\end_inset

 and is statistically independent of 
\begin_inset Formula $x_{i}$
\end_inset

.
 This is a very strong assumption.
\end_layout

\begin_layout Standard
The likelihood of observing a pair 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
f_{yx}\left(y_{i},x_{i}\right) & =f_{y|x}\left(y_{i}|x_{i}\right)f_{x}\left(x\right)\\
 & =\frac{1}{\sqrt{2\pi\gamma}}\exp\left(-\frac{1}{2\gamma}\left(y_{i}-x_{i}'\beta\right)^{2}\right)\times f_{x}\left(x\right),
\end{align*}

\end_inset

 where 
\begin_inset Formula $f_{yx}$
\end_inset

 is the joint pdf, 
\begin_inset Formula $f_{y|x}$
\end_inset

 is the conditional pdf and 
\begin_inset Formula $f_{x}$
\end_inset

 is the marginal pdf of 
\begin_inset Formula $x$
\end_inset

, and the second equality holds under (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:normal_yx"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The likelihood of the random sample 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
\prod_{i=1}^{n}f_{yx}\left(y_{i},x_{i}\right) & =\prod_{i=1}^{n}f_{y|x}\left(y_{i}|x_{i}\right)f_{x}\left(x\right)\\
 & =\prod_{i=1}^{n}f_{y|x}\left(y_{i}|x_{i}\right)\times\prod_{i=1}^{n}f_{x}\left(x\right)\\
 & =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\gamma}}\exp\left(-\frac{1}{2\gamma}\left(y_{i}-x_{i}'\beta\right)^{2}\right)\times\prod_{i=1}^{n}f_{x}\left(x\right).
\end{align*}

\end_inset

The parameters of interest 
\begin_inset Formula $\left(\beta,\gamma\right)$
\end_inset

 are irrelevant to the second term 
\begin_inset Formula $\prod_{i=1}^{n}f_{x}\left(x\right)$
\end_inset

 for they appear only in the 
\emph on
conditional likelihood 
\emph default

\begin_inset Formula 
\[
\prod_{i=1}^{n}f_{y|x}\left(y_{i}|x_{i}\right)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\gamma}}\exp\left(-\frac{1}{2\gamma}\left(y_{i}-x_{i}'\beta\right)^{2}\right).
\]

\end_inset

We focus on the conditional likelihood.
 To facilitate derivation, we work with the (averaged) conditional log-likelihoo
d function 
\begin_inset Formula 
\[
\ell_{n}\left(\beta,\gamma\right)=-\frac{1}{2}\log2\pi-\frac{1}{2}\log\gamma-\frac{1}{2n\gamma}\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2},
\]

\end_inset

for 
\begin_inset Formula $\log\left(\cdot\right)$
\end_inset

 is a monotonic transformation that does not change the maximizer.
 The maximum likelihood estimator 
\begin_inset Formula $\widehat{\beta}_{MLE}$
\end_inset

 can be found using the FOC:
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\beta}\ell_{n}\left(\beta,\gamma\right) & =\frac{1}{n\gamma}\sum_{i=1}^{n}x_{i}\left(y_{i}-x_{i}'\beta\right)=0\\
\frac{\partial}{\partial\gamma}\ell_{n}\left(\beta,\gamma\right) & =-\frac{1}{2\gamma}+\frac{1}{2n\gamma^{2}}\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2}=0.
\end{align*}

\end_inset

Rearranging the above equations in matrix form: 
\begin_inset Formula 
\begin{align*}
X'X\beta & =X'Y\\
\gamma & =\frac{1}{n}\left(Y-X\beta\right)'\left(Y-X\beta\right).
\end{align*}

\end_inset

We solve
\begin_inset Formula 
\begin{align*}
\widehat{\beta}_{MLE} & =(X'X)^{-1}X'Y\\
\widehat{\gamma}_{\mathrm{MLE}} & =\frac{1}{n}\left(Y-X\widehat{\beta}_{MLE}\right)'\left(Y-X\widehat{\beta}_{MLE}\right)=\widehat{e}'\widehat{e}/n
\end{align*}

\end_inset

when 
\begin_inset Formula $X'X$
\end_inset

 is invertible.
 The MLE of the slope coefficient 
\begin_inset Formula $\widehat{\beta}_{MLE}$
\end_inset

 coincides with the OLS estimator, and 
\begin_inset Formula $\widehat{e}$
\end_inset

 is exactly the OLS residual.
\end_layout

\begin_layout Section
Finite Sample Distribution
\end_layout

\begin_layout Standard
We can show the finite-sample exact distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 assuming the error term follows a Gaussian distribution.
 
\emph on
Finite sample distribution
\emph default
 means that the distribution holds for any 
\begin_inset Formula $n$
\end_inset

; it is in contrast to 
\emph on
asymptotic distribution
\emph default
, which is a large sample approximation to the finite sample distribution.
 We first review some properties of a generic jointly normal random vector.
 
\end_layout

\begin_layout Fact
\begin_inset CommandInset label
LatexCommand label
name "fact31"

\end_inset

 Let 
\begin_inset Formula $z\sim N\left(\mu,\Omega\right)$
\end_inset

 be an 
\begin_inset Formula $l\times1$
\end_inset

 random vector with a positive definite variance-covariance matrix 
\begin_inset Formula $\Omega$
\end_inset

.
 Let 
\begin_inset Formula $A$
\end_inset

 be an 
\begin_inset Formula $m\times l$
\end_inset

 non-random matrix where 
\begin_inset Formula $m\leq l$
\end_inset

.
 Then 
\begin_inset Formula $Az\sim N\left(A\mu,A\Omega A'\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Fact
\begin_inset CommandInset label
LatexCommand label
name "fact32"

\end_inset

If 
\begin_inset Formula $z\sim N\left(0,1\right)$
\end_inset

, 
\begin_inset Formula $w\sim\chi^{2}\left(d\right)$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $w$
\end_inset

 are independent.
 Then 
\begin_inset Formula $\frac{z}{\sqrt{w/d}}\sim t\left(d\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The OLS estimator
\begin_inset Formula 
\[
\widehat{\beta}=\left(X'X\right)^{-1}X'Y=\left(X'X\right)^{-1}X'\left(X'\beta+e\right)=\beta+\left(X'X\right)^{-1}X'e,
\]

\end_inset

and its conditional distribution can be written as 
\begin_inset Formula 
\begin{align*}
\widehat{\beta}|X & =\beta+\left(X'X\right)^{-1}X'e|X\\
 & \sim\beta+\left(X'X\right)^{-1}X'\cdot N\left(0_{n},\gamma I_{n}\right)\\
 & \sim N\left(\beta,\gamma\left(X'X\right)^{-1}X'X\left(X'X\right)^{-1}\right)\sim N\left(\beta,\gamma\left(X'X\right)^{-1}\right)
\end{align*}

\end_inset

by Fact 
\begin_inset CommandInset ref
LatexCommand ref
reference "fact31"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\begin_inset Formula $k$
\end_inset

-th element of the vector coefficient
\begin_inset Formula 
\[
\widehat{\beta}_{k}|X=\eta_{k}'\widehat{\beta}|X\sim N\left(\beta_{k},\gamma\eta_{k}'\left(X'X\right)^{-1}\eta_{k}\right)\sim N\left(\beta_{k},\gamma\left[\left(X'X\right)^{-1}\right]_{kk}\right),
\]

\end_inset

where 
\begin_inset Formula $\eta_{k}=\left(1\left\{ l=k\right\} \right)_{l=1,\ldots,K}$
\end_inset

 is the selector of the 
\begin_inset Formula $k$
\end_inset

-th element.
\end_layout

\begin_layout Standard
In reality, 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is an unknown parameter, and 
\begin_inset Formula 
\[
s^{2}=\widehat{e}'\widehat{e}/\left(n-K\right)=e'M_{X}e/\left(n-K\right)
\]

\end_inset

is an unbiased estimator of 
\begin_inset Formula $\gamma$
\end_inset

.
 (Because 
\begin_inset Formula 
\begin{align*}
E\left[s^{2}|X\right] & =\frac{1}{n-K}E\left[e'M_{X}e|X\right]=\frac{1}{n-K}\mathrm{trace}\left(E\left[e'M_{X}e|X\right]\right)\\
 & =\frac{1}{n-K}\mathrm{trace}\left(E\left[M_{X}ee'|X\right]\right)=\frac{1}{n-K}\mathrm{trace}\left(M_{X}E\left[ee'|X\right]\right)\\
 & =\frac{1}{n-K}\mathrm{trace}\left(M_{X}\gamma I_{n}\right)=\frac{\gamma}{n-K}\mathrm{trace}\left(M_{X}\right)=\gamma
\end{align*}

\end_inset

where we use the property of trace 
\begin_inset Formula $\mathrm{trace}\left(AB\right)=\mathrm{trace}\left(BA\right)$
\end_inset

.)
\end_layout

\begin_layout Standard
Under the null hypothesis 
\begin_inset Formula $H_{0}:\beta_{k}=\beta_{k}^{*}$
\end_inset

, where 
\begin_inset Formula $\beta_{k}^{*}$
\end_inset

 is the hypothesized value we want to test.
 We can construct a 
\begin_inset Formula $t$
\end_inset

-statistic 
\begin_inset Formula 
\[
T_{k}=\frac{\widehat{\beta}_{k}-\beta_{k}^{*}}{\sqrt{s^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}},
\]

\end_inset

which is 
\emph on
infeasible
\emph default
 is that sense that it can be directly computed from the data because there
 is no unknown object in this statistic.
 When the hypothesis is true, 
\begin_inset Formula $\beta_{k}=\beta_{k}^{*}$
\end_inset

 and thus 
\begin_inset Formula 
\begin{align}
T_{k} & =\frac{\widehat{\beta}_{k}-\beta_{k}}{\sqrt{s^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}\nonumber \\
 & =\frac{\widehat{\beta}_{k}-\beta_{k}}{\sqrt{\sigma^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}\cdot\frac{\sqrt{\sigma^{2}}}{\sqrt{s^{2}}}\nonumber \\
 & =\frac{\left(\widehat{\beta}_{k}-\beta_{0,k}\right)/\sqrt{\sigma^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}{\sqrt{\frac{e'}{\sigma}M_{X}\frac{e}{\sigma}/\left(n-K\right)}},\label{eq:t-stat}
\end{align}

\end_inset

where we introduce the population quantity 
\begin_inset Formula $\sigma^{2}$
\end_inset

 into the second equality to help derive the distribution of the numerator
 and the denominator of the last expression.
 The numerator 
\begin_inset Formula 
\[
\left(\widehat{\beta}_{k}-\beta_{k}\right)/\sqrt{\sigma^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}\sim N\left(0,1\right),
\]

\end_inset

 and the denominator 
\begin_inset Formula $\sqrt{\frac{e'}{\sigma}M_{X}\frac{e}{\sigma}/\left(n-K\right)}$
\end_inset

 follows 
\begin_inset Formula $\sqrt{\frac{1}{n-K}\chi^{2}\left(n-K\right)}$
\end_inset

.
 Moreover, because 
\begin_inset Formula 
\begin{align*}
\begin{bmatrix}\widehat{\beta}-\beta\\
\widehat{e}
\end{bmatrix} & =\begin{bmatrix}\left(X'X\right)^{-1}X'e\\
M_{X}e
\end{bmatrix}=\begin{bmatrix}\left(X'X\right)^{-1}X'\\
M_{X}
\end{bmatrix}e\\
 & \sim\begin{bmatrix}\left(X'X\right)^{-1}X'\\
M_{X}
\end{bmatrix}\cdot N\left(0,\gamma I_{n}\right)\sim N\left(0,\gamma\begin{bmatrix}\left(X'X\right)^{-1} & 0\\
0 & M_{X}
\end{bmatrix}\right)
\end{align*}

\end_inset

are jointly normal with zero off-diagonal blocks, 
\begin_inset Formula $\left(\widehat{\beta}-\beta\right)$
\end_inset

 and 
\begin_inset Formula $\widehat{e}$
\end_inset

 are statistically independent.
 (This claim is true, although the covariance matrix of the 
\begin_inset Formula $\widehat{e}$
\end_inset

 is singular.) Given that 
\begin_inset Formula $X$
\end_inset

 is viewed as if non-random, the numerator and the denominator of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:t-stat"
plural "false"
caps "false"
noprefix "false"

\end_inset

) are statistically independent as well is a function since the former is
 a function of 
\begin_inset Formula $\left(\widehat{\beta}-\beta\right)$
\end_inset

 and latter is a function of 
\begin_inset Formula $\widehat{e}$
\end_inset

.
 (Alternatively, the statistically independent can be verified by Basu's
 theorem, See Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Basu's-Theorem"
plural "false"
caps "false"
noprefix "false"

\end_inset

.) As a result, we conclude 
\begin_inset Formula $T_{k}\sim t\left(n-K\right)$
\end_inset

 by Fact 
\begin_inset CommandInset ref
LatexCommand ref
reference "fact32"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 This finite sample distribution allows us to conduct statistical inference.
\end_layout

\begin_layout Section
Mean and Variance
\begin_inset CommandInset label
LatexCommand label
name "mean-and-variance"

\end_inset


\end_layout

\begin_layout Standard
Now we relax the normality assumption and statistical independence.
 Instead, we represent the regression model as 
\begin_inset Formula $Y=X\beta+e$
\end_inset

 and 
\begin_inset Formula 
\begin{align*}
E[e|X] & =0_{n}\\
\mathrm{var}\left[e|X\right] & =E\left[ee'|X\right]=\sigma^{2}I_{n}.
\end{align*}

\end_inset

where the first condition is the 
\emph on
mean independence
\emph default
 assumption, and the second condition is the 
\emph on
homoskedasticity
\emph default
 assumption.
 These assumptions are about the first and second 
\emph on
moments
\emph default
 of 
\begin_inset Formula $e_{i}$
\end_inset

 conditional on 
\begin_inset Formula $x_{i}$
\end_inset

.
 Unlike the normality assumption, they do not restrict the distribution
 of 
\begin_inset Formula $e_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Unbiasedness: 
\begin_inset Formula 
\begin{align*}
E\left[\widehat{\beta}|X\right] & =E\left[\left(X'X\right)^{-1}XY|X\right]=E\left[\left(X'X\right)^{-1}X\left(X'\beta+e\right)|X\right]\\
 & =\beta+\left(X'X\right)^{-1}XE\left[e|X\right]=\beta.
\end{align*}

\end_inset

By the law of iterated expectations, the unconditional expectation 
\begin_inset Formula $E\left[\widehat{\beta}\right]=E\left[E\left[\widehat{\beta}|X\right]\right]=\beta.$
\end_inset

 Unbiasedness does not rely on homoskedasticity.
\end_layout

\begin_layout Itemize
Variance: 
\begin_inset Formula 
\[
\begin{aligned}\mathrm{var}\left[\widehat{\beta}|X\right] & =E\left[\left(\widehat{\beta}-E\widehat{\beta}\right)\left(\widehat{\beta}-E\widehat{\beta}\right)'|X\right]\\
 & =E\left[\left(\widehat{\beta}-\beta\right)\left(\widehat{\beta}-\beta\right)'|X\right]\\
 & =E\left[\left(X'X\right)^{-1}X'ee'X\left(X'X\right)^{-1}|X\right]\\
 & =\left(X'X\right)^{-1}X'E\left[ee'|X\right]X\left(X'X\right)^{-1}
\end{aligned}
\]

\end_inset

where the second equality holds as
\end_layout

\begin_layout Itemize
Under the assumption of homoskedasticity, it can be simplified as
\begin_inset Formula 
\[
\begin{aligned}\mathrm{var}\left[\widehat{\beta}|X\right] & =\left(X'X\right)^{-1}X'\left(\sigma^{2}I_{n}\right)X\left(X'X\right)^{-1}\\
 & =\sigma^{2}\left(X'X\right)^{-1}X'I_{n}X\left(X'X\right)^{-1}\\
 & =\sigma^{2}\left(X'X\right)^{-1}.
\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Example
(Heteroskedasticity) If 
\begin_inset Formula $e_{i}=x_{i}u_{i}$
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 is a scalar random variable, 
\begin_inset Formula $u_{i}$
\end_inset

 is statistically independent of 
\begin_inset Formula $x_{i}$
\end_inset

, 
\begin_inset Formula $E\left[u_{i}\right]=0$
\end_inset

 and 
\begin_inset Formula $E\left[u_{i}^{2}\right]=\sigma_{u}^{2}$
\end_inset

.
 Then 
\begin_inset Formula $E\left[e_{i}|x_{i}\right]=E\left[x_{i}u_{i}|x_{i}\right]=x_{i}E\left[u_{i}|x_{i}\right]=0$
\end_inset

 but 
\begin_inset Formula $E\left[e_{i}^{2}|x_{i}\right]=E\left[x_{i}^{2}u_{i}^{2}|x_{i}\right]=x_{i}^{2}E\left[u_{i}^{2}|x_{i}\right]=\sigma_{u}^{2}x_{i}^{2}$
\end_inset

 is a function of 
\begin_inset Formula $x_{i}$
\end_inset

.
 We say 
\begin_inset Formula $e_{i}^{2}$
\end_inset

 is a heteroskedastic error.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Plain Layout
Notice that the above derivation only applies when 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 are independent across 
\begin_inset Formula $i$
\end_inset

.
 In time series the conditional zero mean condition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:e_mean_zero"
plural "false"
caps "false"
noprefix "false"

\end_inset

) has to be strengthened to 
\begin_inset Formula $E\left[e_{i}|x_{1},\ldots,x_{n}\right]=0$
\end_inset

 for unbiasedness.
\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<hsk, dev='my_pdf', fig.ext='pdf'>>=
\end_layout

\begin_layout Plain Layout

n = 100; X = rnorm(n)
\end_layout

\begin_layout Plain Layout

u = rnorm(n)
\end_layout

\begin_layout Plain Layout

plot( y = u, x = X, col = 
\begin_inset Quotes eld
\end_inset

blue
\begin_inset Quotes erd
\end_inset

, ylab = 
\begin_inset Quotes eld
\end_inset

e
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout

e = X * rnorm(n)
\end_layout

\begin_layout Plain Layout

points( y = e, x = X, col = 
\begin_inset Quotes eld
\end_inset

red
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Example
It is important to notice that independently and identically distributed
 sample (iid) 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 does not imply homoskedasticity.
 Homoskedasticity or heteroskedasticity is about the relationship between
 
\begin_inset Formula $\left(x_{i},e_{i}=y_{i}-\beta x\right)$
\end_inset

 within an observation, whereas iid is about the relationship between 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 and 
\begin_inset Formula $\left(y_{j},x_{j}\right)$
\end_inset

 for 
\begin_inset Formula $i\neq j$
\end_inset

 across observations.
\end_layout

\begin_layout Section
Gauss-Markov Theorem
\end_layout

\begin_layout Standard
Gauss-Markov theorem is concerned about the optimality of OLS.
 It justifies OLS as the efficient estimator among all linear unbiased ones.
 
\emph on
Efficient
\emph default
 here means that it enjoys the smallest variance in a family of estimators.
\end_layout

\begin_layout Standard
We have shown that OLS is unbiased in that 
\begin_inset Formula $E\left[\widehat{\beta}\right]=\beta$
\end_inset

.
 There are numerous linearly unbiased estimators.
 For example, 
\begin_inset Formula $\left(Z'X\right)^{-1}Z'y$
\end_inset

 for 
\begin_inset Formula $z_{i}=x_{i}^{2}$
\end_inset

 is unbiased because 
\begin_inset Formula $E\left[\left(Z'X\right)^{-1}Z'y\right]=E\left[\left(Z'X\right)^{-1}Z'\left(X\beta+e\right)\right]=\beta$
\end_inset

.
 We cannot say OLS is better than those other unbiased estimators because
 they are all unbiased — they are equally good at this aspect.
 We move to the second order property of variance: an estimator is better
 if its variance is smaller.
\end_layout

\begin_layout Fact
For two generic random vectors 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 of the same size, we say 
\begin_inset Formula $X$
\end_inset

's variance is smaller or equal to 
\begin_inset Formula $Y$
\end_inset

's variance if 
\begin_inset Formula $\left(\Omega_{Y}-\Omega_{X}\right)$
\end_inset

 is a positive semi-definite matrix.
 The comparison is defined this way because for any non-zero constant vector
 
\begin_inset Formula $c$
\end_inset

, the variance of the linear combination of 
\begin_inset Formula $X$
\end_inset


\begin_inset Formula 
\[
\mathrm{var}\left(c'X\right)=c'\Omega_{X}c\leq c'\Omega_{Y}c=\mathrm{var}\left(c'Y\right)
\]

\end_inset

is no bigger than the same linear combination of 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\tilde{\beta}=A'y$
\end_inset

 be a generic linear estimator, where 
\begin_inset Formula $A$
\end_inset

 is any 
\begin_inset Formula $n\times K$
\end_inset

 functions of 
\begin_inset Formula $X$
\end_inset

.
 As 
\begin_inset Formula 
\[
E\left[A'y|X\right]=E\left[A'\left(X\beta+e\right)|X\right]=A'X\beta.
\]

\end_inset

So the linearity and unbiasedness of 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 implies 
\begin_inset Formula $A'X=I_{n}$
\end_inset

.
 Moreover, the variance 
\begin_inset Formula 
\[
\mbox{var}\left(A'y|X\right)=E\left[\left(A'y-\beta\right)\left(A'y-\beta\right)'|X\right]=E\left[A'ee'A|X\right]=\sigma^{2}A'A.
\]

\end_inset

Let 
\begin_inset Formula $C=A-X\left(X'X\right)^{-1}.$
\end_inset

 
\begin_inset Formula 
\[
\begin{aligned}A'A-\left(X'X\right)^{-1} & =\left(C+X\left(X'X\right)^{-1}\right)'\left(C+X\left(X'X\right)^{-1}\right)-\left(X'X\right)^{-1}\\
 & =C'C+\left(X'X\right)^{-1}X'C+C'X\left(X'X\right)^{-1}\\
 & =C'C,
\end{aligned}
\]

\end_inset

where the last equality follows as 
\begin_inset Formula 
\[
\left(X'X\right)^{-1}X'C=\left(X'X\right)^{-1}X'\left(A-X\left(X'X\right)^{-1}\right)=\left(X'X\right)^{-1}-\left(X'X\right)^{-1}=0.
\]

\end_inset

Therefore 
\begin_inset Formula $A'A-\left(X'X\right)^{-1}$
\end_inset

 is a positive semi-definite matrix.
 The variance of any 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 is no smaller than the OLS estimator 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 The above derivation shows OLS achieves the smallest variance among all
 linear unbiased estimators.
\end_layout

\begin_layout Standard
Homoskedasticity is a restrictive assumption.
 Under homoskedasticity, 
\begin_inset Formula $\mathrm{var}\left[\widehat{\beta}\right]=\sigma^{2}\left(X'X\right)^{-1}$
\end_inset

.
 Popular estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is the sample mean of the residuals 
\begin_inset Formula $\widehat{\sigma}^{2}=\frac{1}{n}\widehat{e}'\widehat{e}$
\end_inset

 or the unbiased one 
\begin_inset Formula $s^{2}=\frac{1}{n-K}\widehat{e}'\widehat{e}$
\end_inset

.
 Under heteroskedasticity, Gauss-Markov theorem does not apply.
\end_layout

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
The exact distribution under the normality assumption of the error term
 is the classical statistical results.
 The Gauss Markov theorem holds under two crucial assumptions: linear CEF
 and homoskedasticity.
 
\end_layout

\begin_layout Standard

\series bold
Historical notes
\series default
: MLE was promulgated and popularized by Ronald Fisher (1890–1962).
 He was a major contributor of the frequentist approach which dominates
 mathematical statistics today, and he sharply criticized the Bayesian approach.
 Fisher collected the iris flower dataset of 150 observations in his biological
 study in 1936, which can be displayed in R by typing 
\family typewriter
iris
\family default
.
 Fisher invented the many concepts in classical mathematical statistics,
 such as sufficient statistic, ancillary statistic, completeness, and exponentia
l family, etc.
\end_layout

\begin_layout Standard

\series bold
Further reading
\series default
: 
\begin_inset CommandInset citation
LatexCommand citet
key "phillips1983exact"
literal "false"

\end_inset

 offered a comprehensive treatment of exact small sample theory in econometrics.
 After that, theoretical studies in econometrics swiftly shifted to large
 sample theory, which we will introduce in the next chapter.
\end_layout

\begin_layout Section
Appendix 
\end_layout

\begin_layout Subsection
Joint Normal Distribution
\end_layout

\begin_layout Standard
It is arguable that normal distribution is the most frequently encountered
 distribution in statistical inference, as it is the asymptotic distribution
 of many popular estimators.
 Moreover, it boasts some unique features that facilitates the calculation
 of objects of interest.
 This note summaries a few of them.
\end_layout

\begin_layout Standard
An 
\begin_inset Formula $n\times1$
\end_inset

 random vector 
\begin_inset Formula $Y$
\end_inset

 follows a joint normal distribution 
\begin_inset Formula $N\left(\mu,\Sigma\right)$
\end_inset

, where 
\begin_inset Formula $\mu$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector and 
\begin_inset Formula $\Sigma$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 symmetric positive definite matrix.
 The probability density function is
\begin_inset Formula 
\[
f_{y}\left(y\right)=\left(2\pi\right)^{-n/2}\left(\mathrm{det}\left(\Sigma\right)\right)^{-1/2}\exp\left(-\frac{1}{2}\left(y-\mu\right)'\Sigma^{-1}\left(y-\mu\right)\right)
\]

\end_inset

where 
\begin_inset Formula $\mathrm{det}\left(\cdot\right)$
\end_inset

 is the determinant of a matrix.
 The moment generating function is
\begin_inset Formula 
\[
M_{y}\left(t\right)=\exp\left(t'\mu+\frac{1}{2}t'\Sigma t\right).
\]

\end_inset


\end_layout

\begin_layout Standard
We will discuss the relationship between two components of a random vector.
 To fix notation,
\begin_inset Formula 
\[
Y=\left(\begin{array}{c}
Y_{1}\\
Y_{2}
\end{array}\right)\sim N\left(\left(\begin{array}{c}
\mu_{1}\\
\mu_{2}
\end{array}\right),\left(\begin{array}{cc}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)\right)
\]

\end_inset

where 
\begin_inset Formula $Y_{1}$
\end_inset

 is an 
\begin_inset Formula $m\times1$
\end_inset

 vector, and 
\begin_inset Formula $Y_{2}$
\end_inset

 is an 
\begin_inset Formula $\left(n-m\right)\times1$
\end_inset

 vector.
 
\begin_inset Formula $\mu_{1}$
\end_inset

 and 
\begin_inset Formula $\mu_{2}$
\end_inset

 are the corresponding mean vectors, and 
\begin_inset Formula $\Sigma_{ij}$
\end_inset

, 
\begin_inset Formula $j=1,2$
\end_inset

 are the corresponding variance and covariance matrices.
 From now on, we always maintain the assumption that 
\begin_inset Formula $Y=\left(Y_{1}',Y_{2}'\right)'$
\end_inset

 is jointly normal.
\end_layout

\begin_layout Standard
Fact 
\begin_inset CommandInset ref
LatexCommand ref
reference "fact31"
plural "false"
caps "false"
noprefix "false"

\end_inset

 immediately implies a convenient feature of the normal distribution.
 Generally speaking, if we are given a joint pdf of two random variables
 and intend to find the marginal distribution of one random variables, we
 need to integrate out the other variable from the joint pdf.
 However, if the variables are jointly normal, the information of the other
 random variable is irrelevant to the marginal distribution of the random
 variable of interest.
 We only need to know the partial information of the part of interest, say
 the mean 
\begin_inset Formula $\mu_{1}$
\end_inset

 and the variance 
\begin_inset Formula $\Sigma_{11}$
\end_inset

 to decide the marginal distribution of 
\begin_inset Formula $Y_{1}$
\end_inset

.
\end_layout

\begin_layout Fact
\begin_inset CommandInset label
LatexCommand label
name "fact:marginal"

\end_inset

The marginal distribution 
\begin_inset Formula $Y_{1}\sim N\left(\mu_{1},\Sigma_{11}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
This result is very convenient if we are interested in some component if
 an estimator, but not the entire vector of the estimator.
 For example, the OLS estimator of the linear regression model 
\begin_inset Formula $y_{i}=x_{i}'\beta+e_{i}$
\end_inset

, under the classical assumption of (i) random sample; (ii) independence
 of 
\begin_inset Formula $z_{i}$
\end_inset

 and 
\begin_inset Formula $e_{i}$
\end_inset

; (iii) 
\begin_inset Formula $e_{i}\sim N\left(0,\gamma\right)$
\end_inset

 is
\begin_inset Formula 
\[
\widehat{\beta}=\left(X'X\right)^{-1}X'y,
\]

\end_inset

and the finite sample exact distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is 
\begin_inset Formula 
\[
\left(\widehat{\beta}-\beta\right)|X\sim N\left(0,\gamma\left(X'X\right)^{-1}\right)
\]

\end_inset

If we are interested in the inference of only the 
\begin_inset Formula $j$
\end_inset

-th component of 
\begin_inset Formula $\beta_{0}^{\left(j\right)}$
\end_inset

, then from Fact 
\begin_inset CommandInset ref
LatexCommand ref
reference "fact:marginal"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset Formula 
\[
\left(\widehat{\beta}_{k}-\beta_{k}\right)/\left(X'X\right)_{kk}^{-1}\sim N\left(0,\gamma\right)
\]

\end_inset

where 
\begin_inset Formula $\left[\left(X'X\right)^{-1}\right]_{kk}$
\end_inset

 is the 
\begin_inset Formula $k$
\end_inset

-th diagonal element of 
\begin_inset Formula $\left(X'X\right)^{-1}$
\end_inset

.
 The marginal distribution is independent of the other components.
 This saves us from integrating out the other components, which could be
 troublesome if the dimension of the vector is high.
\end_layout

\begin_layout Standard
Generally, zero covariance of two random variables only indicates that they
 are uncorrelated, whereas full statistical independence is a much stronger
 requirement.
 However, if 
\begin_inset Formula $Y_{1}$
\end_inset

 and 
\begin_inset Formula $Y_{2}$
\end_inset

 are jointly normal, then zero covariance is equivalent to full independence.
\end_layout

\begin_layout Fact
If 
\begin_inset Formula $\Sigma_{12}=0$
\end_inset

, then 
\begin_inset Formula $Y_{1}$
\end_inset

 and 
\begin_inset Formula $Y_{2}$
\end_inset

 are independent.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Fact
If 
\begin_inset Formula $\Sigma$
\end_inset

 is invertible, then 
\begin_inset Formula $Y'\Sigma^{-1}Y\sim\chi^{2}\left(\mathrm{rank}\left(\Sigma\right)\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The last result, which is useful in linear regression, is that if 
\begin_inset Formula $Y_{1}$
\end_inset

 and 
\begin_inset Formula $Y_{2}$
\end_inset

 are jointly normal, the conditional distribution of 
\begin_inset Formula $Y_{1}$
\end_inset

 on 
\begin_inset Formula $Y_{2}$
\end_inset

 is still jointly normal, with the mean and variance specified as in the
 following fact.
 
\end_layout

\begin_layout Fact
\begin_inset Formula $Y_{1}|Y_{2}\sim N\left(\mu_{1}+\Sigma_{12}\Sigma_{22}^{-1}\left(Y_{2}-\mu_{2}\right),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Basu's Theorem* 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Basu's-Theorem"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $Y=\left(y_{1},\ldots,y_{n}\right)$
\end_inset

 consists of 
\begin_inset Formula $n$
\end_inset

 iid observations.
 We say 
\begin_inset Formula $T\left(Y\right)$
\end_inset

 is a
\emph on
 sufficient statistic
\emph default
 for a parameter 
\begin_inset Formula $\theta$
\end_inset

 if the conditional probability 
\begin_inset Formula $f\left(Y|T\left(Y\right)\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
 We say 
\begin_inset Formula $S\left(Y\right)$
\end_inset

 is an 
\emph on
ancillary statistic
\emph default
 for 
\begin_inset Formula $\theta$
\end_inset

 if its distribution does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Basu's theorem
\emph default
 says that a 
\emph on
complete
\emph default
 sufficient statistic is statistically independent from any ancillary statistic.
\end_layout

\begin_layout Standard
Sufficient statistic is closely related to the exponential family in classical
 mathematical statistics.
 A parametric distribution indexed by 
\begin_inset Formula $\theta$
\end_inset

 is a member of the 
\emph on
exponential family
\emph default
 is its PDF can be written as 
\begin_inset Formula 
\[
f\left(Y|\theta\right)=h\left(Y\right)g\left(\theta\right)\exp\left(\eta\left(\theta\right)'T\left(Y\right)\right),
\]

\end_inset

 where 
\begin_inset Formula $g\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\eta\left(\theta\right)$
\end_inset

 are functions depend, only on 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $h\left(Y\right)$
\end_inset

 and 
\begin_inset Formula $T\left(Y\right)$
\end_inset

 are functions depend only on 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Example
(Univariate Gaussian location model.) For a normal distribution 
\begin_inset Formula $y_{i}\sim N\left(\mu,\gamma\right)$
\end_inset

 with known 
\begin_inset Formula $\gamma$
\end_inset

 and unknown 
\begin_inset Formula $\mu$
\end_inset

, the sample mean 
\begin_inset Formula $\bar{y}$
\end_inset

 is the sufficient statistic and the sample standard deviation 
\begin_inset Formula $s^{2}$
\end_inset

 is an ancillary statistic.
\end_layout

\begin_layout Example
We first verify that the sample mean 
\begin_inset Formula $\bar{y}=n^{-1}\sum_{i=1}^{n}y_{i}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\mu$
\end_inset

.
 Notice that the joint density of 
\begin_inset Formula $Y$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
f\left(Y\right) & =\left(2\pi\gamma\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right)\\
 & =\left(2\pi\gamma\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(\left(y_{i}-\bar{y}\right)+\left(\bar{y}-\mu\right)\right)^{2}\right)\\
 & =\left(2\pi\gamma\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(\left(y_{i}-\bar{y}\right)^{2}+2\left(y_{i}-\bar{y}\right)\left(\bar{y}-\mu\right)+\left(\bar{y}-\mu\right)^{2}\right)\right)\\
 & =\left(2\pi\gamma\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}\right)\exp\left(-\frac{n}{2\gamma}\left(\bar{y}-\mu\right)^{2}\right).
\end{align*}

\end_inset

Because 
\begin_inset Formula $\bar{y}\sim N\left(\mu,\gamma/n\right),$
\end_inset

 the marginal density is 
\begin_inset Formula 
\[
f\left(\bar{y}\right)=\left(2\pi\gamma/n\right)^{-1/2}\exp\left(-\frac{n}{2\gamma}\left(\bar{y}-\mu\right)^{2}\right).
\]

\end_inset

For 
\begin_inset Formula $\bar{y}$
\end_inset

 is a statistic of 
\begin_inset Formula $Y$
\end_inset

, we have 
\begin_inset Formula $f\left(Y,\bar{y}\right)=f\left(Y\right)$
\end_inset

.
 The conditional density is 
\begin_inset Formula 
\[
f\left(Y|\bar{y}\right)=\frac{f\left(Y,\bar{y}\right)}{f\left(\bar{y}\right)}=\frac{f\left(Y\right)}{f\left(\bar{y}\right)}=\sqrt{n}\left(2\pi\gamma\right)^{-\frac{n-1}{2}}\exp\left(-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}\right)
\]

\end_inset

is independent of 
\begin_inset Formula $\mu$
\end_inset

, and thus 
\begin_inset Formula $\bar{y}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\mu$
\end_inset

.
 In the meantime, the sample standard deviation 
\begin_inset Formula $s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)$
\end_inset

 is an 
\emph on
ancillary statistic
\emph default
 for 
\begin_inset Formula $\mu$
\end_inset

 , because the distribution of 
\begin_inset Formula $s^{2}$
\end_inset

 does not depend on 
\begin_inset Formula $\mu.$
\end_inset


\end_layout

\begin_layout Standard
The normal distribution with known 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and unknown 
\begin_inset Formula $\mu$
\end_inset

 belongs to the exponential family in view of the decomposition
\begin_inset Formula 
\begin{align*}
f(Y) & =\left(2\pi\gamma\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right)\\
 & =\underbrace{\exp\left(-\sum_{i=1}^{n}\frac{y_{i}^{2}}{2\gamma}\right)}_{h\left(Y\right)}\cdot\underbrace{\left(2\pi\gamma\right)^{-\frac{n}{2}}\exp\left(-\frac{n}{2\gamma}\mu^{2}\right)}_{g\left(\theta\right)}\cdot\underbrace{\exp\left(\frac{\mu}{2\gamma}n\bar{y}\right)}_{\exp\left(\eta\left(\theta\right)'T\left(Y\right)\right)}.
\end{align*}

\end_inset

The exponential family is a class of distributions with the special functional
 form which is convenient for deriving sufficient statistics as well as
 other desirable properties in classical mathematical statistics.
\end_layout

\begin_layout Example
(Conditional Gaussian location model.) If 
\begin_inset Formula $y_{i}\sim N\left(x_{i}\beta,\gamma\right)$
\end_inset

 with known 
\begin_inset Formula $\gamma$
\end_inset

 and unknown 
\begin_inset Formula $\beta$
\end_inset

, We verify that the sample mean 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\beta$
\end_inset

.
 Notice that the joint density of 
\begin_inset Formula $Y$
\end_inset

 given 
\begin_inset Formula $X$
\end_inset

 is
\begin_inset Formula 
\begin{align*}
f\left(Y|X\right) & =\left(2\pi\gamma\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right)\\
 & =\left(2\pi\gamma\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\gamma}\left(Y-X\widehat{\beta}\right)'\left(Y-X\widehat{\beta}\right)\right)\exp\left(-\frac{1}{2\gamma}\left(\widehat{\beta}-\beta\right)'X'X\left(\widehat{\beta}-\beta\right)\right).
\end{align*}

\end_inset

Because 
\begin_inset Formula $\widehat{\beta}\sim N\left(\beta,\gamma\left(X'X\right)^{-1}\right),$
\end_inset

 the marginal density is 
\begin_inset Formula 
\[
f\left(\widehat{\beta}|X\right)=\left(2\pi\gamma\right)^{-\frac{K}{2}}\left(\mathrm{det}\left(\left(X'X\right)^{-1}\right)\right)^{-1/2}\exp\left(-\frac{1}{2\gamma}\left(\widehat{\beta}-\beta\right)'X'X\left(\widehat{\beta}-\beta\right)\right).
\]

\end_inset

The conditional density is 
\begin_inset Formula 
\begin{align*}
f\left(Y|\widehat{\beta},X\right) & =\frac{f\left(Y|X\right)}{f\left(\widehat{\beta}|X\right)}\\
 & =\left(2\pi\gamma\right)^{-\frac{n-K}{2}}\left(\mathrm{det}\left(\left(X'X\right)^{-1}\right)\right)^{-1/2}\exp\left(-\frac{1}{2\gamma}\left(Y-X\widehat{\beta}\right)'\left(Y-X\widehat{\beta}\right)\right)
\end{align*}

\end_inset

is independent of 
\begin_inset Formula $\beta$
\end_inset

, and thus 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Standard
In the meantime, the sample standard deviation 
\begin_inset Formula $s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(y_{i}-x_{i}\widehat{\beta}\right)$
\end_inset

 is an 
\emph on
ancillary statistic
\emph default
 for 
\begin_inset Formula $\beta$
\end_inset

 , because the distribution of 
\begin_inset Formula $s^{2}$
\end_inset

 does not depend on 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 Oct 10.}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ref_teaching"
options "chicagoa"

\end_inset


\end_layout

\end_body
\end_document
