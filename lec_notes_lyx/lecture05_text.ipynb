{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Asymptotic Theory\n",
    "\n",
    "Our universe, though enormous, consists of fewer than $10^{82}$ atoms,\n",
    "which is a finite number. However, mathematical ideas are not bounded by\n",
    "secular realities. Asymptotic theory is about behaviors of statistics\n",
    "when the sample size is arbitrarily large up to infinity. It is a set of\n",
    "approximation techniques to simplify complicated finite-sample analysis.\n",
    "Asymptotic theory is the cornerstone of modern econometrics. It sheds\n",
    "lights on estimation and inference procedures under much more general\n",
    "conditions than what are covered by exact finite sample theory.\n",
    "\n",
    "Nevertheless, we always have at hand a finite sample, and mostly it is\n",
    "difficult to increase the sample size in reality. Asymptotic theory\n",
    "rarely answers “how large is large”, and we must be cautious about the\n",
    "treacherous landscape of *asymptopia*. In the era of big data, albeit\n",
    "the sheer size of data balloons dramatically, we build more\n",
    "sophisticated models to better capture heterogeneity in the data. Large\n",
    "sample is a relative notion to the complexity of the model and\n",
    "underlying (in)dependence structure of the data.\n",
    "\n",
    "Both the classical parametric approach, which is based on hard-to-verify\n",
    "parametric assumptions, and the asymptotic approach, which is predicated\n",
    "on imaginary infinite sequences, deviate from the reality. Which\n",
    "approach is more constructive can only be judged case by case. The\n",
    "prevalence of asymptotic theory is its mathematical amenability and\n",
    "generality. The law of evolution elevates asymptotic theory to the\n",
    "throne of mathematical statistics of our time.\n",
    "\n",
    "## Modes of Convergence\n",
    "\n",
    "We first review what is *convergence* for a non-random sequence, which\n",
    "you learned in high school. Let $z_{1},z_{2},\\ldots$ be an infinite\n",
    "sequence of non-random variables.\n",
    "\n",
    "Convergence of this non-random sequence means that for any\n",
    "$\\varepsilon>0$, there exists an $N\\left(\\varepsilon\\right)$ such that\n",
    "for all $n>N\\left(\\varepsilon\\right)$, we have\n",
    "$\\left|z_{n}-z\\right|<\\varepsilon$. We say $z$ is the limit of $z_{n}$,\n",
    "and write $z_{n}\\to z$ or $\\lim_{n\\to\\infty}z_{n}=z$.\n",
    "\n",
    "Instead of a deterministic sequence, we are interested in the\n",
    "convergence of a sequence of random variables. Since a random variable\n",
    "is “random” thanks to the induced probability measure by the measurable\n",
    "function, we must be clear what *convergence* means. Several modes of\n",
    "convergence are widely used.\n",
    "\n",
    "We say a sequence of random variables $\\left(z_{n}\\right)$ converges in\n",
    "probability to $z$, where $z$ can be either a random variable or a\n",
    "non-random constant, if for any $\\varepsilon>0$, the probability\n",
    "$P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|<\\varepsilon\\right\\} \\to1$\n",
    "(or equivalently\n",
    "$P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|\\geq\\varepsilon\\right\\} \\to0$)\n",
    "as $n\\to\\infty$. We can write $z_{n}\\stackrel{p}{\\to}z$ or\n",
    "$\\mathrm{plim}_{n\\to\\infty}z_{n}=z$.\n",
    "\n",
    "A sequence of random variables $\\left(z_{n}\\right)$ converges in\n",
    "squared-mean to $z$, where $z$ can be either a random variable or a\n",
    "non-random constant, if $E\\left[\\left(z_{n}-z\\right)^{2}\\right]\\to0.$ It\n",
    "is denoted as $z_{n}\\stackrel{m.s.}{\\to}z$.\n",
    "\n",
    "In these definitions either\n",
    "$P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|>\\varepsilon\\right\\}$\n",
    "or $E\\left[\\left(z_{n}-z\\right)^{2}\\right]$ is a non-random quantity,\n",
    "and it converges to 0 as a non-random sequence.\n",
    "\n",
    "Squared-mean convergence is stronger than convergence in probability.\n",
    "That is, $z_{n}\\stackrel{m.s.}{\\to}z$ implies $z_{n}\\stackrel{p}{\\to}z$\n",
    "but the converse is untrue. Here is an example.\n",
    "\n",
    "<span id=\"eg:in_p_in_ms\"\n",
    "label=\"eg:in_p_in_ms\">\\[eg:in_p\\_in_ms\\]</span>$(z_{n})$ is a sequence\n",
    "of binary random variables: $z_{n}=\\sqrt{n}$ with probability $1/n$, and\n",
    "$z_{n}=0$ with probability $1-1/n$. Then $z_{n}\\stackrel{p}{\\to}0$ but\n",
    "$z_{n}\\stackrel{m.s.}{\\nrightarrow}0$. To verify these claims, notice\n",
    "that for any $\\varepsilon>0$, we have\n",
    "$P\\left(\\omega:\\left|z_{n}\\left(\\omega\\right)-0\\right|<\\varepsilon\\right)=P\\left(\\omega:z_{n}\\left(\\omega\\right)=0\\right)=1-1/n\\rightarrow1$\n",
    "and thereby $z_{n}\\stackrel{p}{\\to}0$. On the other hand,\n",
    "$E\\left[\\left(z_{n}-0\\right)^{2}\\right]=n\\cdot1/n+0\\cdot(1-1/n)=1\\nrightarrow0,$\n",
    "so $z_{n}\\stackrel{m.s.}{\\nrightarrow}0$.\n",
    "\n",
    "Example\n",
    "<a href=\"#eg:in_p_in_ms\" data-reference-type=\"ref\" data-reference=\"eg:in_p_in_ms\">[eg:in_p_in_ms]</a>\n",
    "highlights the difference between the two modes of convergence.\n",
    "Convergence in probability does not count what happens on a subset in\n",
    "the sample space of small probability. Squared-mean convergence deals\n",
    "with the average over the entire probability space. If a random variable\n",
    "can take a wild value, with small probability though, it may blow away\n",
    "the squared-mean convergence. On the contrary, such irregularity does\n",
    "not undermine convergence in probability.\n",
    "\n",
    "Both convergence in probability and squared-mean convergence are about\n",
    "convergence of random variables to a target random variable or constant.\n",
    "That is, the distribution of $z_{n}-z$ is concentrated around 0 as\n",
    "$n\\to\\infty$. Instead, *convergence in distribution* is about the\n",
    "convergence of CDF, but not the random variable. Let\n",
    "$F_{z_{n}}\\left(\\cdot\\right)$ be the CDF of $z_{n}$ and\n",
    "$F_{z}\\left(\\cdot\\right)$ be the CDF of $z$.\n",
    "\n",
    "We say a sequence of random variables $\\left(z_{n}\\right)$ converges in\n",
    "distribution to a random variable $z$ if\n",
    "$F_{z_{n}}\\left(a\\right)\\to F_{z}\\left(a\\right)$ as $n\\to\\infty$ at each\n",
    "point $a\\in\\mathbb{R}$ such that where $F_{z}\\left(\\cdot\\right)$ is\n",
    "continuous. We write $z_{n}\\stackrel{d}{\\to}z$.\n",
    "\n",
    "Convergence in distribution is the weakest mode. If\n",
    "$z_{n}\\stackrel{p}{\\to}z$, then $z_{n}\\stackrel{d}{\\to}z$. The converse\n",
    "is not true in general, unless $z$ is a non-random constant (A constant\n",
    "$z$ can be viewed as a degenerate random variables, with a corresponding\n",
    "“CDF” $F_{z}\\left(\\cdot\\right)=1\\left\\{ \\cdot\\geq z\\right\\}$.\n",
    "\n",
    "Let $x\\sim N\\left(0,1\\right)$. If $z_{n}=x+1/n$, then\n",
    "$z_{n}\\stackrel{p}{\\to}x$ and of course $z_{n}\\stackrel{d}{\\to}x$.\n",
    "However, if $z_{n}=-x+1/n$, or $z_{n}=y+1/n$ where\n",
    "$y\\sim N\\left(0,1\\right)$ is independent of $x$, then\n",
    "$z_{n}\\stackrel{d}{\\to}x$ but $z_{n}\\stackrel{p}{\\nrightarrow}x$.\n",
    "\n",
    "$(z_{n})$ is a sequence of binary random variables: $z_{n}=n$ with\n",
    "probability $1/\\sqrt{n}$, and $z_{n}=0$ with probability $1-1/\\sqrt{n}$.\n",
    "Then $z_{n}\\stackrel{d}{\\to}z=0.$ Because\n",
    "$$F_{z_{n}}\\left(a\\right)=\\begin{cases}\n",
    "0 & a<0\\\\\n",
    "1-1/\\sqrt{n} & 0\\leq a\\leq n\\\\\n",
    "1 & a\\geq n\n",
    "\\end{cases}.$$\n",
    "$F_{z}\\left(a\\right)=\\begin{cases} 0, & a<0\\\\ 1 & a\\geq0 \\end{cases}$.\n",
    "It is easy to verify that $F_{z_{n}}\\left(a\\right)$ converges to\n",
    "$F_{z}\\left(a\\right)$ *pointwisely* on each point in\n",
    "$\\left(-\\infty,0\\right)\\cup\\left(0,+\\infty\\right)$, where\n",
    "$F_{z}\\left(a\\right)$ is continuous.\n",
    "\n",
    "So far we have talked about convergence of scalar variables. These three\n",
    "modes of converges can be easily generalized to random vectors. In\n",
    "particular, the *Cramer-Wold device* collapses a random vector into a\n",
    "random vector via arbitrary linear combination. We say a sequence of\n",
    "$K$-dimensional random vectors $\\left(z_{n}\\right)$ converge in\n",
    "distribution to $z$ if $\\lambda'z_{n}\\stackrel{d}{\\to}\\lambda'z$ for any\n",
    "$\\lambda\\in\\mathbb{R}^{K}$ and $\\left\\Vert \\lambda\\right\\Vert _{2}=1.$\n",
    "\n",
    "## Law of Large Numbers\n",
    "\n",
    "(Weak) law of large numbers (LLN) is a collection of statements about\n",
    "convergence in probability of the sample average to its population\n",
    "counterpart. The basic form of LLN is:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}(z_{i}-E[z_{i}])\\stackrel{p}{\\to}0$$ as\n",
    "$n\\to\\infty$. Various versions of LLN work under different assumptions\n",
    "about some features and/or dependence of the underlying random\n",
    "variables.\n",
    "\n",
    "### Cherbyshev LLN\n",
    "\n",
    "We illustrate LLN by the simple example of Chebyshev LLN, which can be\n",
    "proved by elementary calculation. It utilizes the *Chebyshev\n",
    "inequality*.\n",
    "\n",
    "-   *Chebyshev inequality*: If a random variable $x$ has a finite second\n",
    "    moment $E\\left[x^{2}\\right]<\\infty$, then we have\n",
    "    $P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} \\leq E\\left[x^{2}\\right]/\\varepsilon^{2}$\n",
    "    for any constant $\\varepsilon>0$.\n",
    "\n",
    "Show that if $r_{2}\\geq r_{1}\\geq1$, then\n",
    "$E\\left[\\left|x\\right|^{r_{2}}\\right]<\\infty$ implies\n",
    "$E\\left[\\left|x\\right|^{r_{1}}\\right]<\\infty.$ (Hint: use Holder’s\n",
    "inequality.)\n",
    "\n",
    "The Chebyshev inequality is a special case of the *Markov inequality*.\n",
    "\n",
    "-   *Markov inequality*: If a random variable $x$ has a finite $r$-th\n",
    "    absolute moment $E\\left[\\left|x\\right|^{r}\\right]<\\infty$ for some\n",
    "    $r\\ge1$, then we have\n",
    "    $P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} \\leq E\\left[\\left|x\\right|^{r}\\right]/\\varepsilon^{r}$\n",
    "    any constant $\\varepsilon>0$.\n",
    "\n",
    "It is easy to verify the Markov inequality.\n",
    "$$\\begin{aligned}E\\left[\\left|x\\right|^{r}\\right] & =\\int_{\\left|x\\right|>\\varepsilon}\\left|x\\right|^{r}dF_{X}+\\int_{\\left|x\\right|\\leq\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\\n",
    " & \\geq\\int_{\\left|x\\right|>\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\\n",
    " & \\geq\\varepsilon^{r}\\int_{\\left|x\\right|>\\varepsilon}dF_{X}=\\varepsilon^{r}P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} .\n",
    "\\end{aligned}$$ Rearrange the above inequality and we obtain the Markov\n",
    "inequality.\n",
    "\n",
    "Let the *partial sum* $S_{n}=\\sum_{i=1}^{n}x_{i}$, where\n",
    "$\\mu_{i}=E\\left[x_{i}\\right]$ and\n",
    "$\\sigma_{i}^{2}=\\mathrm{var}\\left[x_{i}\\right]$. We apply the Chebyshev\n",
    "inequality to the sample mean\n",
    "$z_{n}=\\overline{x}-\\bar{\\mu}=n^{-1}\\left(S_{n}-E\\left[S_{n}\\right]\\right)$.\n",
    "$$\\begin{aligned}\n",
    "P\\left\\{ \\left|z_{n}\\right|\\geq\\varepsilon\\right\\}  & =P\\left\\{ n^{-1}\\left|S_{n}-E\\left[S_{n}\\right]\\right|\\geq\\varepsilon\\right\\} \\nonumber \\\\\n",
    " & \\leq E\\left[\\left(n^{-1}\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)\\right)^{2}\\right]/\\varepsilon^{2}\\nonumber \\\\\n",
    " & =\\left(n\\varepsilon\\right)^{-2}\\left\\{ E\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)^{2}\\right]+\\sum_{i=1}^{n}\\sum_{j\\neq i}E\\left[\\left(x_{i}-\\mu_{i}\\right)\\left(x_{j}-\\mu_{j}\\right)\\right]\\right\\} \\nonumber \\\\\n",
    " & =\\left(n\\varepsilon\\right)^{-2}\\left\\{ \\sum_{i=1}^{n}\\mathrm{var}\\left(x_{i}\\right)+\\sum_{i=1}^{n}\\sum_{j\\neq i}\\mathrm{cov}\\left(x_{i},x_{j}\\right)\\right\\} .\\label{eq:cheby_mean}\\end{aligned}$$\n",
    "Convergence in probability holds if the right-hand side shrinks to 0 as\n",
    "$n\\to\\infty$. For example, If $x_{1},\\ldots,x_{n}$ are iid with\n",
    "$\\mathrm{var}\\left(x_{1}\\right)=\\sigma^{2}$, then the RHS of\n",
    "(<a href=\"#eq:cheby_mean\" data-reference-type=\"ref\" data-reference=\"eq:cheby_mean\">[eq:cheby_mean]</a>)\n",
    "is\n",
    "$\\left(n\\varepsilon\\right)^{-2}\\left(n\\sigma^{2}\\right)=o\\left(n^{-1}\\right)\\to0$.\n",
    "This result gives the Chebyshev LLN:\n",
    "\n",
    "-   Chebyshev LLN: If $\\left(z_{1},\\ldots,z_{n}\\right)$ is a sample of\n",
    "    iid observations, $E\\left[z_{1}\\right]=\\mu$ , and\n",
    "    $\\sigma^{2}=\\mathrm{var}\\left[z_{1}\\right]<\\infty$ exists, then\n",
    "    $\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\stackrel{p}{\\to}\\mu.$\n",
    "\n",
    "The convergence in probability can be indeed maintained under much more\n",
    "general conditions than under iid case. The random variables in the\n",
    "sample do not have to be identically distributed, and they do not have\n",
    "to be independent either.\n",
    "\n",
    "Consider an inid (independent but non-identically distributed) sample\n",
    "$\\left(x_{1},\\ldots,x_{n}\\right)$ with $E\\left[x_{i}\\right]=0$ and\n",
    "$\\mathrm{var}\\left[x_{i}\\right]=\\sqrt{n}c$ for some constant $c>0$. Use\n",
    "the Chebyshev inequality to show that\n",
    "$n^{-1}\\sum_{i=1}^{n}x_{i}\\stackrel{p}{\\to}0$.\n",
    "\n",
    "Consider the time series moving average model\n",
    "$x_{i}=\\varepsilon_{i}+\\theta\\varepsilon_{i-1}$ for $i=1,\\ldots,n$,\n",
    "where $\\left|\\theta\\right|<1$, $E\\left[\\varepsilon_{i}\\right]=0$,\n",
    "$\\mathrm{var}\\left[\\varepsilon_{i}\\right]=\\sigma^{2}$, and\n",
    "$\\left(\\varepsilon_{i}\\right)_{i=0}^{n}$ iid. Use the Chebyshev\n",
    "inequality to show that $n^{-1}\\sum_{i=1}^{n}x_{i}\\stackrel{p}{\\to}0$.\n",
    "\n",
    "Another useful LLN is the *Kolmogorov LLN*. Since its derivation\n",
    "requires more advanced knowledge of probability theory, we state the\n",
    "result without proof.\n",
    "\n",
    "-   Kolmogorov LLN: If $\\left(z_{1},\\ldots,z_{n}\\right)$ is a sample of\n",
    "    iid observations and $E\\left[z_{1}\\right]=\\mu$ exists, then\n",
    "    $\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\stackrel{p}{\\to}\\mu$.\n",
    "\n",
    "Compared with the Chebyshev LLN, the Kolmogorov LLN only requires the\n",
    "existence of the population mean, but not any higher moments. On the\n",
    "other hand, iid is essential for the Kolmogorov LLN.\n",
    "\n",
    "Consider three distributions: standard normal $N\\left(0,1\\right)$,\n",
    "$t\\left(2\\right)$ (zero mean, infinite variance), and the Cauchy\n",
    "distribution (no moments exist). We plot paths of the sample average\n",
    "with $n=2^{1},2^{2},\\ldots,2^{20}$. We will see that the sample averages\n",
    "of $N\\left(0,1\\right)$ and $t\\left(2\\right)$ converge, but that of the\n",
    "Cauchy distribution does not.\n",
    "\n",
    "**knitrout**\n",
    "\n",
    "## Central Limit Theorem\n",
    "\n",
    "The central limit theorem (CLT) is a collection of probability results\n",
    "about the convergence in distribution to a stable distribution. The\n",
    "limiting distribution is usually the Gaussian distribution. The basic\n",
    "form of the CLT is:\n",
    "\n",
    "-   *Under some conditions to be spelled out*, the sample average of\n",
    "    *zero-mean* random variables $\\left(z_{1},\\ldots,z_{n}\\right)$\n",
    "    multiplied by $\\sqrt{n}$ satisfies\n",
    "    $$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}z_{i}\\stackrel{d}{\\to}N\\left(0,\\sigma^{2}\\right)$$\n",
    "    as $n\\to\\infty$.\n",
    "\n",
    "Various versions of CLT work under different assumptions about the\n",
    "random variables. *Lindeberg-Levy CLT* is the simplest CLT.\n",
    "\n",
    "-   If the sample $\\left(x_{1},\\ldots,x_{n}\\right)$ is iid,\n",
    "    $E\\left[x_{1}\\right]=0$ and\n",
    "    $\\mathrm{var}\\left[x_{1}\\right]=\\sigma^{2}<\\infty$, then\n",
    "    $\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}\\stackrel{d}{\\to}N\\left(0,\\sigma^{2}\\right)$.\n",
    "\n",
    "Lindeberg-Levy CLT can be proved by the *moment generating function*.\n",
    "For any random variable $x$, the function\n",
    "$M_{x}\\left(t\\right)=E\\left[\\exp\\left(xt\\right)\\right]$ is called its\n",
    "the *moment generating function* (MGF) if it exists. MGF fully describes\n",
    "a distribution, just like PDF or CDF. For example, the MGF of\n",
    "$N\\left(\\mu,\\sigma^{2}\\right)$ is\n",
    "$\\exp\\left(\\mu t+\\frac{1}{2}\\sigma^{2}t^{2}\\right)$.\n",
    "\n",
    "If $E\\left[\\left|x\\right|^{k}\\right]<\\infty$ for a positive integer $k$,\n",
    "then\n",
    "$$M_{X}\\left(t\\right)=1+tE\\left[X\\right]+\\frac{t^{2}}{2}E\\left[X^{2}\\right]+\\ldots\\frac{t}{k!}E\\left[X^{k}\\right]+O\\left(t^{k+1}\\right).$$\n",
    "Under the assumption of Lindeberg-Levy CLT,\n",
    "$$M_{\\frac{X_{i}}{\\sqrt{n}}}\\left(t\\right)=1+\\frac{t^{2}}{2n}\\sigma^{2}+O\\left(\\frac{t^{3}}{n^{3/2}}\\right)$$\n",
    "for all $i$, and by independence we have\n",
    "$$\\begin{aligned}M_{\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}}\\left(t\\right) & =\\prod_{i=1}^{n}M_{\\frac{X_{i}}{\\sqrt{n}}}\\left(t\\right)=\\left(1+\\frac{t^{2}}{2n}\\sigma^{2}+O\\left(\\frac{t^{3}}{n^{3/2}}\\right)\\right)^{n}\\\\\n",
    " & \\to\\exp\\left(\\frac{\\sigma^{2}}{2}t^{2}\\right),\n",
    "\\end{aligned}$$ where the limit is exactly the characteristic function\n",
    "of $N\\left(0,\\sigma^{2}\\right)$.\n",
    "\n",
    "This proof with MGF is simple and elementary. Its drawback is that not\n",
    "all distributions have a well-defined MGF. A more general proof can be\n",
    "carried out by replacing MGF with the *characteristic function*\n",
    "$\\varphi_{x}\\left(t\\right)=E\\left[\\exp\\left(\\mathrm{i}xt\\right)\\right]$,\n",
    "where “$\\mathrm{i}$” is the imaginary number. The characteristic\n",
    "function is the *Fourier transform* of the probability measure and it\n",
    "always exists. Such a proof will require background knowledge of Fourier\n",
    "transform and inverse transform, which we do not pursuit here.\n",
    "\n",
    "-   Lindeberg-Feller CLT: $\\left(x_{i}\\right)_{i=1}^{n}$ is inid. If the\n",
    "    *Lindeberg condition* is satisfied (for any fixed $\\varepsilon>0$,\n",
    "    $\\frac{1}{s_{n}^{2}}\\sum_{i=1}^{n}E\\left[x_{i}^{2}\\cdot\\boldsymbol{1}\\left\\{ \\left|x_{i}\\right|\\geq\\varepsilon s_{n}\\right\\} \\right]\\to0$\n",
    "    where $s_{n}=\\sqrt{\\sum_{i=1}^{n}\\sigma_{i}^{2}}$), then we have\n",
    "    $$\\frac{\\sum_{i=1}^{n}x_{i}}{s_{n}}\\stackrel{d}{\\to}N\\left(0,1\\right).$$\n",
    "\n",
    "-   Lyapunov CLT: $\\left(x_{i}\\right)_{i=1}^{n}$ is inid. If\n",
    "    $\\max_{i\\leq n}E\\left[\\left|x_{i}\\right|^{3}\\right]<C<\\infty,$ then\n",
    "    we have\n",
    "    $$\\frac{\\sum_{i=1}^{n}x_{i}}{s_{n}}\\stackrel{d}{\\to}N\\left(0,1\\right).$$\n",
    "\n",
    "This is a simulated example.\n",
    "\n",
    "\\[knitrout\\]\n",
    "\n",
    "## Tools for Transformations\n",
    "\n",
    "In their original forms, LLN deals with the sample mean, and CLT handles\n",
    "the scaled (by $\\sqrt{n}$) and/or standardized (by standard deviation)\n",
    "sample mean. However, most of the econometric estimators of interest are\n",
    "functions of sample means. For example, in the OLS estimator\n",
    "$$\\widehat{\\beta}=\\left(\\frac{1}{n}\\sum_{i}x_{i}x_{i}'\\right)^{-1}\\frac{1}{n}\\sum_{i}x_{i}y_{i}$$\n",
    "involves matrix inverse and the matrix-vector multiplication. We need\n",
    "tools to handle transformations.\n",
    "\n",
    "-   Continuous mapping theorem 1: If $x_{n}\\stackrel{p}{\\to}a$ and\n",
    "    $f\\left(\\cdot\\right)$ is continuous at $a$, then\n",
    "    $f\\left(x_{n}\\right)\\stackrel{p}{\\to}f\\left(a\\right)$.\n",
    "\n",
    "-   Continuous mapping theorem 2: If $x_{n}\\stackrel{d}{\\to}x$ and\n",
    "    $f\\left(\\cdot\\right)$ is continuous almost surely on the support of\n",
    "    $x$, then $f\\left(x_{n}\\right)\\stackrel{d}{\\to}f\\left(x\\right)$.\n",
    "\n",
    "-   Slutsky’s theorem: If $x_{n}\\stackrel{d}{\\to}x$ and\n",
    "    $y_{n}\\stackrel{p}{\\to}a$, then\n",
    "\n",
    "    -   $x_{n}+y_{n}\\stackrel{d}{\\to}x+a$\n",
    "\n",
    "    -   $x_{n}y_{n}\\stackrel{d}{\\to}ax$\n",
    "\n",
    "    -   $x_{n}/y_{n}\\stackrel{d}{\\to}x/a$ if $a\\neq0$.\n",
    "\n",
    "Slutsky’s theorem consists of special cases of the continuous mapping\n",
    "theorem 2. Only because the addition, multiplication and division are\n",
    "encountered so frequently in practice, we list it as a separate theorem.\n",
    "\n",
    "-   Delta method: if\n",
    "    $\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\Omega\\right)$,\n",
    "    and $f\\left(\\cdot\\right)$ is continuously differentiable at\n",
    "    $\\theta_{0}$ (meaning\n",
    "    $\\frac{\\partial}{\\partial\\theta}f\\left(\\cdot\\right)$ is continuous\n",
    "    at $\\theta_{0}$), then we have\n",
    "    $$\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\to}N\\left(0,\\frac{\\partial f}{\\partial\\theta'}\\left(\\theta_{0}\\right)\\Omega\\left(\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)\\right)'\\right).$$\n",
    "\n",
    "Take a Taylor expansion of $f\\left(\\widehat{\\theta}\\right)$ around\n",
    "$f\\left(\\theta_{0}\\right)$:\n",
    "$$f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)=\\frac{\\partial f\\left(\\dot{\\theta}\\right)}{\\partial\\theta'}\\left(\\widehat{\\theta}-\\theta_{0}\\right),$$\n",
    "where $\\dot{\\theta}$ lies on the line segment between $\\widehat{\\theta}$\n",
    "and $\\theta_{0}$. Multiply $\\sqrt{n}$ on both sides,\n",
    "$$\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)=\\frac{\\partial f\\left(\\dot{\\theta}\\right)}{\\partial\\theta'}\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right).$$\n",
    "Because $\\widehat{\\theta}\\stackrel{p}{\\to}\\theta_{0}$ implies\n",
    "$\\dot{\\theta}\\stackrel{p}{\\to}\\theta_{0}$ and\n",
    "$\\frac{\\partial}{\\partial\\theta'}f\\left(\\cdot\\right)$ is continuous at\n",
    "$\\theta_{0}$, we have\n",
    "$\\frac{\\partial}{\\partial\\theta'}f\\left(\\dot{\\theta}\\right)\\stackrel{p}{\\to}\\frac{\\partial f\\left(\\theta_{0}\\right)}{\\partial\\theta'}$\n",
    "by the continuous mapping theorem 1. In view of\n",
    "$\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\Omega\\right)$,\n",
    "Slutsky’s Theorem implies\n",
    "$$\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\to}\\frac{\\partial f\\left(\\theta_{0}\\right)}{\\partial\\theta'}N\\left(0,\\Omega\\right)$$\n",
    "and the conclusion follows.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Asymptotic theory is a topic with vast breadth and depth. In this\n",
    "chapter we only scratch the very surface of it. We will discuss in the\n",
    "next chapter how to apply the asymptotic tools we learned here to the\n",
    "OLS estimator.\n",
    "\n",
    "**Historical notes**: Before 1980s, most econometricians did not have a\n",
    "good training in mathematical rigor to master asymptotic theory. A few\n",
    "prominent young (at that time) econometricians came to the field and\n",
    "changed the situation, among them were Halbert White (UCSD), Peter\n",
    "C.B. Phillips (Yale) and Peter Robinson (LSE), to name a few.\n",
    "\n",
    "**Further reading**: Halbert White (1950-2012) wrote an accessible book\n",
    "\\[@white2014asymptotic first edition 1984\\] to introduce asymptotics to\n",
    "econometricians. This book remains popular among researchers and\n",
    "graduate students in economics. @davidson1994stochastic is a longer and\n",
    "more self-contained monograph."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
