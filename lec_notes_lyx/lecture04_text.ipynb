{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares: Finite Sample Theory\n",
    "\n",
    "We continue with properties of OLS. We will show that OLS coincides with\n",
    "the maximum likelihood estimator if the error term follows a normal\n",
    "distribution. We derive its finite-sample exact distribution which can\n",
    "be used for statistical inference. The Gauss-Markov theorem justifies\n",
    "the optimality of OLS under the classical assumptions.\n",
    "\n",
    "Suppose the data is generated from a parametric model. Statistical\n",
    "estimation looks for the unknown parameter from the observed data. A\n",
    "*principle* is an ideology about a proper way of estimation. Over the\n",
    "history of statistics, only a few principles are widely accepted. Among\n",
    "them Maximum Likelihood is the most important and fundamental. The\n",
    "maximum likelihood principle entails that the unknown parameter being\n",
    "found as the maximizer of the log-likelihood function.\n",
    "\n",
    "## Maximum Likelihood\n",
    "\n",
    "In this chapter, we first give an introduction of the maximum likelihood\n",
    "estimation. Consider a random sample of\n",
    "$Z=\\left(z_{1},z_{2},\\ldots,z_{n}\\right)$ drawn from a parametric\n",
    "distribution with density $f_{z}\\left(z_{i};\\theta\\right)$, where\n",
    "$z_{i}$ is either a scalar random variable or a random vector. A\n",
    "parametric distribution is completely characterized by a\n",
    "finite-dimensional parameter $\\theta$. We know that $\\theta$ belongs to\n",
    "a parameter space $\\Theta$. We use the data to estimate $\\theta$.\n",
    "\n",
    "The log-likelihood of observing the entire sample $Z$ is\n",
    "$$L_{n}\\left(\\theta;Z\\right):=\\log\\left(\\prod_{i=1}^{n}f_{z}\\left(z_{i};\\theta\\right)\\right)=\\sum_{i=1}^{n}\\log f_{z}\\left(z_{i};\\theta\\right).\\label{eq:raw_likelihood}$$\n",
    "In reality the sample $Z$ is given and for each $\\theta\\in\\Theta$ we can\n",
    "evaluate $L_{n}\\left(\\theta;Z\\right)$. The maximum likelihood estimator\n",
    "is\n",
    "$$\\widehat{\\theta}_{MLE}:=\\arg\\max_{\\theta\\in\\Theta}L_{n}\\left(\\theta;Z\\right).$$\n",
    "Why maximizing the log-likelihood function is desirable? An intuitive\n",
    "explanation is that $\\widehat{\\theta}_{MLE}$ makes observing $Z$ the\n",
    "“most likely” in the entire parametric space.\n",
    "\n",
    "A more formal justification requires an explicitly defined distance.\n",
    "Suppose that the true parameter value that generates the data is\n",
    "$\\theta_{0}$, so that the true distribution is\n",
    "$f_{z}\\left(z_{i};\\theta_{0}\\right)$. Any generic point\n",
    "$\\theta\\in\\Theta$ produces $f_{z}\\left(z_{i};\\theta\\right)$. To measure\n",
    "their difference, we introduce the *Kullback-Leibler divergence*, or the\n",
    "Kullback-Leibler distance, defined as the logarithms of the expected\n",
    "log-likelihood ratio $$\\begin{aligned}\n",
    "D_{f}\\left(\\theta_{0}\\Vert\\theta\\right) & =D\\left(f_{z}\\left(z_{i};\\theta_{0}\\right)\\Vert f_{z}\\left(z_{i};\\theta\\right)\\right):=E_{\\theta_{0}}\\left[\\log\\frac{f_{z}\\left(z_{i};\\theta_{0}\\right)}{f_{z}\\left(z_{i};\\theta\\right)}\\right]\\\\\n",
    " & =E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{i};\\theta_{0}\\right)\\right]-E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{i};\\theta\\right)\\right].\\end{aligned}$$\n",
    "We call it a “distance” because it is non-negative, although it is not\n",
    "symmetric in that\n",
    "$D_{f}\\left(\\theta_{1}\\Vert\\theta_{2}\\right)\\neq D_{f}\\left(\\theta_{2}\\Vert\\theta_{1}\\right)$\n",
    "and it does not satisfy the triangle inequality. To see\n",
    "$D_{f}\\left(\\theta_{0}\\Vert\\theta\\right)$ is non-negative, notice that\n",
    "$-\\log\\left(\\cdot\\right)$ is strictly convex and then by Jensen’s\n",
    "inequality $$\\begin{aligned}\n",
    "E_{\\theta_{0}}\\left[\\log\\frac{f_{z}\\left(z_{i};\\theta_{0}\\right)}{f_{z}\\left(z_{i};\\theta\\right)}\\right] & =E_{\\theta_{0}}\\left[-\\log\\frac{f_{z}\\left(z_{i};\\theta\\right)}{f_{z}\\left(z_{i};\\theta_{0}\\right)}\\right]\\geq-\\log\\left(E_{\\theta_{0}}\\left[\\frac{f_{z}\\left(z_{i};\\theta\\right)}{f_{z}\\left(z_{i};\\theta_{0}\\right)}\\right]\\right)\\\\\n",
    " & =-\\log\\left(\\int\\frac{f_{z}\\left(z_{i};\\theta\\right)}{f_{z}\\left(z_{i};\\theta_{0}\\right)}f_{z}\\left(z_{i};\\theta_{0}\\right)dz_{i}\\right)=-\\log\\left(\\int f_{z}\\left(z_{i};\\theta\\right)dz_{i}\\right)\\\\\n",
    " & =-\\log1=0,\\end{aligned}$$ where\n",
    "$\\int f_{z}\\left(z_{i};\\theta\\right)dz_{i}=1$ for any pdf. The equality\n",
    "holds if and only if\n",
    "$f_{z}\\left(z_{i};\\theta\\right)=f_{z}\\left(z_{i};\\theta_{0}\\right)$\n",
    "almost everywhere. Furthermore, if there is a one-to-one mapping between\n",
    "$\\theta$ and $f_{z}\\left(z_{i};\\theta\\right)$ on $\\Theta$\n",
    "(identification), then\n",
    "$\\theta_{0}=\\arg\\min_{\\theta\\in\\Theta}D_{f}\\left(\\theta_{0}\\Vert\\theta\\right)$\n",
    "is the unique solution.\n",
    "\n",
    "In information theory,\n",
    "$-E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{i};\\theta_{0}\\right)\\right]$ is\n",
    "the *entropy* of the continuous distribution of\n",
    "$f_{z}\\left(z_{i};\\theta_{0}\\right)$. Entropy measures the uncertainty\n",
    "of a random variable; the larger is the value, the more chaotic is the\n",
    "random variable. The Kullback-Leibler distance is the *relative entropy*\n",
    "between the distribution $f_{z}\\left(z_{i};\\theta_{0}\\right)$ and\n",
    "$f_{z}\\left(z_{i};\\theta\\right)$. It measures the inefficiency of\n",
    "assuming that the distribution is $f_{z}\\left(z_{i};\\theta\\right)$ when\n",
    "the true distribution is indeed $f_{z}\\left(z_{i};\\theta_{0}\\right)$.\n",
    "\\[@cover2006elements p.19\\]\n",
    "\n",
    "Consider the Gaussian location model $z_{i}\\sim N\\left(\\mu,1\\right)$,\n",
    "where $\\mu$ is the unknown parameter to be estimated. The likelihood of\n",
    "observing $z_{i}$ is\n",
    "$f_{z}\\left(z_{i};\\mu\\right)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(z_{i}-\\mu\\right)^{2}\\right)$.\n",
    "The likelihood of observing the sample $Z$ is\n",
    "$$f_{Z}\\left(Z;\\mu\\right)=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(z_{i}-\\mu\\right)^{2}\\right)$$\n",
    "and the log-likelihood is\n",
    "$$L_{n}\\left(\\mu;Z\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}\\sum_{i=1}^{n}\\left(z_{i}-\\mu\\right)^{2}.$$\n",
    "The (averaged) log-likelihood function for the $n$ observations is\n",
    "$$\\begin{aligned}\n",
    "\\ell_{n}\\left(\\mu\\right) & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2n}\\sum_{i=1}^{n}\\left(z_{i}-\\mu\\right)^{2}.\\end{aligned}$$\n",
    "We work with the averaged log-likelihood $\\ell_{n}$, instead of the\n",
    "(raw) log-likelihood $L_{n}$, to make it directly comparable with the\n",
    "expected log density $$\\begin{aligned}\n",
    "E_{\\mu_{0}}\\left[\\log f_{z}\\left(z;\\mu\\right)\\right] & =E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]\\\\\n",
    " & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(z_{i}-\\mu\\right)^{2}\\right]\\\\\n",
    " & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(\\left(z_{i}-\\mu_{0}\\right)+\\left(\\mu_{0}-\\mu\\right)\\right)^{2}\\right]\\\\\n",
    " & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(z_{i}-\\mu_{0}\\right)^{2}\\right]-E_{\\mu_{0}}\\left[z_{i}-\\mu_{0}\\right]\\left(\\mu_{0}-\\mu\\right)-\\frac{1}{2}\\left(\\mu_{0}-\\mu\\right)^{2}\\\\\n",
    " & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}-\\frac{1}{2}\\left(\\mu-\\mu_{0}\\right)^{2}.\\end{aligned}$$\n",
    "where the first equality holds because of random sampling. Obviously,\n",
    "$\\ell_{n}\\left(\\mu\\right)$ is maximized at\n",
    "$\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}$ while\n",
    "$E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]$ is maximized at\n",
    "$\\mu=\\mu_{0}$. The Kullback-Leibler divergence in this example is\n",
    "$$D\\left(\\mu_{0}\\Vert\\mu\\right)=E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu_{0}\\right)\\right]-E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]=\\frac{1}{2}\\left(\\mu-\\mu_{0}\\right)^{2},$$\n",
    "where\n",
    "$-E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu_{0}\\right)\\right]=\\frac{1}{2}\\left(\\log\\left(2\\pi\\right)+1\\right)$\n",
    "is the entropy of the normal distribution with unit variance.\n",
    "\n",
    "We use the following code to demonstrate the population log-likelihood\n",
    "$E\\left[\\ell_{n}\\left(\\mu\\right)\\right]$ when $\\mu_{0}=2$ (solid line)\n",
    "and the 3 sample realizations when $n=4$ (dashed lines).\n",
    "\n",
    "\\*\\*there is a knitr\\*\\* part\n",
    "\n",
    "## Likelihood Estimation for Regression\n",
    "\n",
    "Notation: $y_{i}$ is a scalar, and\n",
    "$x_{i}=\\left(x_{i1},\\ldots,x_{iK}\\right)'$ is a $K\\times1$ vector. $Y$\n",
    "is an $n\\times1$ vector, and $X$ is an $n\\times K$ matrix.\n",
    "\n",
    "In this chapter we employ the classical statistical framework under\n",
    "restrictive distributional assumption\n",
    "$$y_{i}|x_{i}\\sim N\\left(x_{i}'\\beta,\\gamma\\right),\\label{eq:normal_yx}$$\n",
    "where $\\gamma=\\sigma^{2}$ to ease the differentiation. This assumption\n",
    "is equivalent to\n",
    "$e_{i}|x_{i}=\\left(y_{i}-x_{i}'\\beta\\right)|x_{i}\\sim N\\left(0,\\gamma\\right)$.\n",
    "Because the distribution of $e_{i}$ is invariant to $x_{i}$, the error\n",
    "term $e_{i}\\sim N\\left(0,\\gamma\\right)$ and is statistically independent\n",
    "of $x_{i}$. This is a very strong assumption.\n",
    "\n",
    "The likelihood of observing a pair $\\left(y_{i},x_{i}\\right)$ is\n",
    "$$\\begin{aligned}\n",
    "f_{yx}\\left(y_{i},x_{i}\\right) & =f_{y|x}\\left(y_{i}|x_{i}\\right)f_{x}\\left(x\\right)\\\\\n",
    " & =\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{i}-x_{i}'\\beta\\right)^{2}\\right)\\times f_{x}\\left(x\\right),\\end{aligned}$$\n",
    "where $f_{yx}$ is the joint pdf, $f_{y|x}$ is the conditional pdf and\n",
    "$f_{x}$ is the marginal pdf of $x$, and the second equality holds under\n",
    "(<a href=\"#eq:normal_yx\" data-reference-type=\"ref\" data-reference=\"eq:normal_yx\">[eq:normal_yx]</a>).\n",
    "The likelihood of the random sample $\\left(y_{i},x_{i}\\right)_{i=1}^{n}$\n",
    "is $$\\begin{aligned}\n",
    "\\prod_{i=1}^{n}f_{yx}\\left(y_{i},x_{i}\\right) & =\\prod_{i=1}^{n}f_{y|x}\\left(y_{i}|x_{i}\\right)f_{x}\\left(x\\right)\\\\\n",
    " & =\\prod_{i=1}^{n}f_{y|x}\\left(y_{i}|x_{i}\\right)\\times\\prod_{i=1}^{n}f_{x}\\left(x\\right)\\\\\n",
    " & =\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{i}-x_{i}'\\beta\\right)^{2}\\right)\\times\\prod_{i=1}^{n}f_{x}\\left(x\\right).\\end{aligned}$$\n",
    "The parameters of interest $\\left(\\beta,\\gamma\\right)$ are irrelevant to\n",
    "the second term $\\prod_{i=1}^{n}f_{x}\\left(x\\right)$ for they appear\n",
    "only in the *conditional likelihood*\n",
    "$$\\prod_{i=1}^{n}f_{y|x}\\left(y_{i}|x_{i}\\right)=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{i}-x_{i}'\\beta\\right)^{2}\\right).$$\n",
    "We focus on the conditional likelihood. To facilitate derivation, we\n",
    "work with the (averaged) conditional log-likelihood function\n",
    "$$\\ell_{n}\\left(\\beta,\\gamma\\right)=-\\frac{1}{2}\\log2\\pi-\\frac{1}{2}\\log\\gamma-\\frac{1}{2n\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'\\beta\\right)^{2},$$\n",
    "for $\\log\\left(\\cdot\\right)$ is a monotonic transformation that does not\n",
    "change the maximizer. The maximum likelihood estimator\n",
    "$\\widehat{\\beta}_{MLE}$ can be found using the FOC: $$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial\\beta}\\ell_{n}\\left(\\beta,\\gamma\\right) & =\\frac{1}{n\\gamma}\\sum_{i=1}^{n}x_{i}\\left(y_{i}-x_{i}'\\beta\\right)=0\\\\\n",
    "\\frac{\\partial}{\\partial\\gamma}\\ell_{n}\\left(\\beta,\\gamma\\right) & =-\\frac{1}{2\\gamma}+\\frac{1}{2n\\gamma^{2}}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'\\beta\\right)^{2}=0.\\end{aligned}$$\n",
    "Rearranging the above equations in matrix form: $$\\begin{aligned}\n",
    "X'X\\beta & =X'Y\\\\\n",
    "\\gamma & =\\frac{1}{n}\\left(Y-X\\beta\\right)'\\left(Y-X\\beta\\right).\\end{aligned}$$\n",
    "We solve $$\\begin{aligned}\n",
    "\\widehat{\\beta}_{MLE} & =(X'X)^{-1}X'Y\\\\\n",
    "\\widehat{\\gamma}_{\\mathrm{MLE}} & =\\frac{1}{n}\\left(Y-X\\widehat{\\beta}_{MLE}\\right)'\\left(Y-X\\widehat{\\beta}_{MLE}\\right)=\\widehat{e}'\\widehat{e}/n\\end{aligned}$$\n",
    "when $X'X$ is invertible. The MLE of the slope coefficient\n",
    "$\\widehat{\\beta}_{MLE}$ coincides with the OLS estimator, and\n",
    "$\\widehat{e}$ is exactly the OLS residual.\n",
    "\n",
    "## Finite Sample Distribution\n",
    "\n",
    "We can show the finite-sample exact distribution of $\\widehat{\\beta}$\n",
    "assuming the error term follows a Gaussian distribution. *Finite sample\n",
    "distribution* means that the distribution holds for any $n$; it is in\n",
    "contrast to *asymptotic distribution*, which is a large sample\n",
    "approximation to the finite sample distribution. We first review some\n",
    "properties of a generic jointly normal random vector.\n",
    "\n",
    "<span id=\"fact31\" label=\"fact31\">\\[fact31\\]</span> Let\n",
    "$z\\sim N\\left(\\mu,\\Omega\\right)$ be an $l\\times1$ random vector with a\n",
    "positive definite variance-covariance matrix $\\Omega$. Let $A$ be an\n",
    "$m\\times l$ non-random matrix where $m\\leq l$. Then\n",
    "$Az\\sim N\\left(A\\mu,A\\Omega A'\\right)$.\n",
    "\n",
    "<span id=\"fact32\" label=\"fact32\">\\[fact32\\]</span>If\n",
    "$z\\sim N\\left(0,1\\right)$, $w\\sim\\chi^{2}\\left(d\\right)$ and $z$ and $w$\n",
    "are independent. Then $\\frac{z}{\\sqrt{w/d}}\\sim t\\left(d\\right)$.\n",
    "\n",
    "The OLS estimator\n",
    "$$\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'Y=\\left(X'X\\right)^{-1}X'\\left(X'\\beta+e\\right)=\\beta+\\left(X'X\\right)^{-1}X'e,$$\n",
    "and its conditional distribution can be written as $$\\begin{aligned}\n",
    "\\widehat{\\beta}|X & =\\beta+\\left(X'X\\right)^{-1}X'e|X\\\\\n",
    " & \\sim\\beta+\\left(X'X\\right)^{-1}X'\\cdot N\\left(0_{n},\\gamma I_{n}\\right)\\\\\n",
    " & \\sim N\\left(\\beta,\\gamma\\left(X'X\\right)^{-1}X'X\\left(X'X\\right)^{-1}\\right)\\sim N\\left(\\beta,\\gamma\\left(X'X\\right)^{-1}\\right)\\end{aligned}$$\n",
    "by Fact\n",
    "<a href=\"#fact31\" data-reference-type=\"ref\" data-reference=\"fact31\">[fact31]</a>.\n",
    "The $k$-th element of the vector coefficient\n",
    "$$\\widehat{\\beta}_{k}|X=\\eta_{k}'\\widehat{\\beta}|X\\sim N\\left(\\beta_{k},\\gamma\\eta_{k}'\\left(X'X\\right)^{-1}\\eta_{k}\\right)\\sim N\\left(\\beta_{k},\\gamma\\left[\\left(X'X\\right)^{-1}\\right]_{kk}\\right),$$\n",
    "where $\\eta_{k}=\\left(1\\left\\{ l=k\\right\\} \\right)_{l=1,\\ldots,K}$ is\n",
    "the selector of the $k$-th element.\n",
    "\n",
    "In reality, $\\sigma^{2}$ is an unknown parameter, and\n",
    "$$s^{2}=\\widehat{e}'\\widehat{e}/\\left(n-K\\right)=e'M_{X}e/\\left(n-K\\right)$$\n",
    "is an unbiased estimator of $\\gamma$. (Because $$\\begin{aligned}\n",
    "E\\left[s^{2}|X\\right] & =\\frac{1}{n-K}E\\left[e'M_{X}e|X\\right]=\\frac{1}{n-K}\\mathrm{trace}\\left(E\\left[e'M_{X}e|X\\right]\\right)\\\\\n",
    " & =\\frac{1}{n-K}\\mathrm{trace}\\left(E\\left[M_{X}ee'|X\\right]\\right)=\\frac{1}{n-K}\\mathrm{trace}\\left(M_{X}E\\left[ee'|X\\right]\\right)\\\\\n",
    " & =\\frac{1}{n-K}\\mathrm{trace}\\left(M_{X}\\gamma I_{n}\\right)=\\frac{\\gamma}{n-K}\\mathrm{trace}\\left(M_{X}\\right)=\\gamma\\end{aligned}$$\n",
    "where we use the property of trace\n",
    "$\\mathrm{trace}\\left(AB\\right)=\\mathrm{trace}\\left(BA\\right)$.)\n",
    "\n",
    "Under the null hypothesis $H_{0}:\\beta_{k}=\\beta_{k}^{*}$, where\n",
    "$\\beta_{k}^{*}$ is the hypothesized value we want to test. We can\n",
    "construct a $t$-statistic\n",
    "$$T_{k}=\\frac{\\widehat{\\beta}_{k}-\\beta_{k}^{*}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}},$$\n",
    "which is *infeasible* is that sense that it can be directly computed\n",
    "from the data because there is no unknown object in this statistic. When\n",
    "the hypothesis is true, $\\beta_{k}=\\beta_{k}^{*}$ and thus\n",
    "$$\\begin{aligned}\n",
    "T_{k} & =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}\\nonumber \\\\\n",
    " & =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}\\cdot\\frac{\\sqrt{\\sigma^{2}}}{\\sqrt{s^{2}}}\\nonumber \\\\\n",
    " & =\\frac{\\left(\\widehat{\\beta}_{k}-\\beta_{0,k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}{\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}},\\label{eq:t-stat}\\end{aligned}$$\n",
    "where we introduce the population quantity $\\sigma^{2}$ into the second\n",
    "equality to help derive the distribution of the numerator and the\n",
    "denominator of the last expression. The numerator\n",
    "$$\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}\\sim N\\left(0,1\\right),$$\n",
    "and the denominator\n",
    "$\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}$ follows\n",
    "$\\sqrt{\\frac{1}{n-K}\\chi^{2}\\left(n-K\\right)}$. Moreover, because\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix}\\widehat{\\beta}-\\beta\\\\\n",
    "\\widehat{e}\n",
    "\\end{bmatrix} & =\\begin{bmatrix}\\left(X'X\\right)^{-1}X'e\\\\\n",
    "M_{X}e\n",
    "\\end{bmatrix}=\\begin{bmatrix}\\left(X'X\\right)^{-1}X'\\\\\n",
    "M_{X}\n",
    "\\end{bmatrix}e\\\\\n",
    " & \\sim\\begin{bmatrix}\\left(X'X\\right)^{-1}X'\\\\\n",
    "M_{X}\n",
    "\\end{bmatrix}\\cdot N\\left(0,\\gamma I_{n}\\right)\\sim N\\left(0,\\gamma\\begin{bmatrix}\\left(X'X\\right)^{-1} & 0\\\\\n",
    "0 & M_{X}\n",
    "\\end{bmatrix}\\right)\\end{aligned}$$ are jointly normal with zero\n",
    "off-diagonal blocks, $\\left(\\widehat{\\beta}-\\beta\\right)$ and\n",
    "$\\widehat{e}$ are statistically independent. (This claim is true,\n",
    "although the covariance matrix of the $\\widehat{e}$ is singular.) Given\n",
    "that $X$ is viewed as if non-random, the numerator and the denominator\n",
    "of\n",
    "(<a href=\"#eq:t-stat\" data-reference-type=\"ref\" data-reference=\"eq:t-stat\">[eq:t-stat]</a>)\n",
    "are statistically independent as well is a function since the former is\n",
    "a function of $\\left(\\widehat{\\beta}-\\beta\\right)$ and latter is a\n",
    "function of $\\widehat{e}$. (Alternatively, the statistically independent\n",
    "can be verified by Basu’s theorem, See Appendix\n",
    "<a href=\"#subsec:Basu&#39;s-Theorem\" data-reference-type=\"ref\" data-reference=\"subsec:Basu&#39;s-Theorem\">[subsec:Basu's-Theorem]</a>.)\n",
    "As a result, we conclude $T_{k}\\sim t\\left(n-K\\right)$ by Fact\n",
    "<a href=\"#fact32\" data-reference-type=\"ref\" data-reference=\"fact32\">[fact32]</a>.\n",
    "This finite sample distribution allows us to conduct statistical\n",
    "inference.\n",
    "\n",
    "## Mean and Variance<span id=\"mean-and-variance\" label=\"mean-and-variance\">\\[mean-and-variance\\]</span>\n",
    "\n",
    "Now we relax the normality assumption and statistical independence.\n",
    "Instead, we represent the regression model as $Y=X\\beta+e$ and\n",
    "$$\\begin{aligned}\n",
    "E[e|X] & =0_{n}\\\\\n",
    "\\mathrm{var}\\left[e|X\\right] & =E\\left[ee'|X\\right]=\\sigma^{2}I_{n}.\\end{aligned}$$\n",
    "where the first condition is the *mean independence* assumption, and the\n",
    "second condition is the *homoskedasticity* assumption. These assumptions\n",
    "are about the first and second *moments* of $e_{i}$ conditional on\n",
    "$x_{i}$. Unlike the normality assumption, they do not restrict the\n",
    "distribution of $e_{i}$.\n",
    "\n",
    "-   Unbiasedness: $$\\begin{aligned}\n",
    "    E\\left[\\widehat{\\beta}|X\\right] & =E\\left[\\left(X'X\\right)^{-1}XY|X\\right]=E\\left[\\left(X'X\\right)^{-1}X\\left(X'\\beta+e\\right)|X\\right]\\\\\n",
    "     & =\\beta+\\left(X'X\\right)^{-1}XE\\left[e|X\\right]=\\beta.\\end{aligned}$$\n",
    "    By the law of iterated expectations, the unconditional expectation\n",
    "    $E\\left[\\widehat{\\beta}\\right]=E\\left[E\\left[\\widehat{\\beta}|X\\right]\\right]=\\beta.$\n",
    "    Unbiasedness does not rely on homoskedasticity.\n",
    "\n",
    "-   Variance:\n",
    "    $$\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] & =E\\left[\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)'|X\\right]\\\\\n",
    "     & =E\\left[\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\widehat{\\beta}-\\beta\\right)'|X\\right]\\\\\n",
    "     & =E\\left[\\left(X'X\\right)^{-1}X'ee'X\\left(X'X\\right)^{-1}|X\\right]\\\\\n",
    "     & =\\left(X'X\\right)^{-1}X'E\\left[ee'|X\\right]X\\left(X'X\\right)^{-1}\n",
    "    \\end{aligned}$$ where the second equality holds as\n",
    "\n",
    "-   Under the assumption of homoskedasticity, it can be simplified as\n",
    "    $$\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] & =\\left(X'X\\right)^{-1}X'\\left(\\sigma^{2}I_{n}\\right)X\\left(X'X\\right)^{-1}\\\\\n",
    "     & =\\sigma^{2}\\left(X'X\\right)^{-1}X'I_{n}X\\left(X'X\\right)^{-1}\\\\\n",
    "     & =\\sigma^{2}\\left(X'X\\right)^{-1}.\n",
    "    \\end{aligned}$$\n",
    "\n",
    "(Heteroskedasticity) If $e_{i}=x_{i}u_{i}$, where $x_{i}$ is a scalar\n",
    "random variable, $u_{i}$ is statistically independent of $x_{i}$,\n",
    "$E\\left[u_{i}\\right]=0$ and $E\\left[u_{i}^{2}\\right]=\\sigma_{u}^{2}$.\n",
    "Then\n",
    "$E\\left[e_{i}|x_{i}\\right]=E\\left[x_{i}u_{i}|x_{i}\\right]=x_{i}E\\left[u_{i}|x_{i}\\right]=0$\n",
    "but\n",
    "$E\\left[e_{i}^{2}|x_{i}\\right]=E\\left[x_{i}^{2}u_{i}^{2}|x_{i}\\right]=x_{i}^{2}E\\left[u_{i}^{2}|x_{i}\\right]=\\sigma_{u}^{2}x_{i}^{2}$\n",
    "is a function of $x_{i}$. We say $e_{i}^{2}$ is a heteroskedastic error.\n",
    "\n",
    "\\*\\*knitr\\*\\*\n",
    "\n",
    "It is important to notice that independently and identically distributed\n",
    "sample (iid) $\\left(y_{i},x_{i}\\right)$ does not imply homoskedasticity.\n",
    "Homoskedasticity or heteroskedasticity is about the relationship between\n",
    "$\\left(x_{i},e_{i}=y_{i}-\\beta x\\right)$ within an observation, whereas\n",
    "iid is about the relationship between $\\left(y_{i},x_{i}\\right)$ and\n",
    "$\\left(y_{j},x_{j}\\right)$ for $i\\neq j$ across observations.\n",
    "\n",
    "## Gauss-Markov Theorem\n",
    "\n",
    "Gauss-Markov theorem is concerned about the optimality of OLS. It\n",
    "justifies OLS as the efficient estimator among all linear unbiased ones.\n",
    "*Efficient* here means that it enjoys the smallest variance in a family\n",
    "of estimators.\n",
    "\n",
    "We have shown that OLS is unbiased in that\n",
    "$E\\left[\\widehat{\\beta}\\right]=\\beta$. There are numerous linearly\n",
    "unbiased estimators. For example, $\\left(Z'X\\right)^{-1}Z'y$ for\n",
    "$z_{i}=x_{i}^{2}$ is unbiased because\n",
    "$E\\left[\\left(Z'X\\right)^{-1}Z'y\\right]=E\\left[\\left(Z'X\\right)^{-1}Z'\\left(X\\beta+e\\right)\\right]=\\beta$.\n",
    "We cannot say OLS is better than those other unbiased estimators because\n",
    "they are all unbiased — they are equally good at this aspect. We move to\n",
    "the second order property of variance: an estimator is better if its\n",
    "variance is smaller.\n",
    "\n",
    "For two generic random vectors $X$ and $Y$ of the same size, we say\n",
    "$X$’s variance is smaller or equal to $Y$’s variance if\n",
    "$\\left(\\Omega_{Y}-\\Omega_{X}\\right)$ is a positive semi-definite matrix.\n",
    "The comparison is defined this way because for any non-zero constant\n",
    "vector $c$, the variance of the linear combination of $X$\n",
    "$$\\mathrm{var}\\left(c'X\\right)=c'\\Omega_{X}c\\leq c'\\Omega_{Y}c=\\mathrm{var}\\left(c'Y\\right)$$\n",
    "is no bigger than the same linear combination of $Y$.\n",
    "\n",
    "Let $\\tilde{\\beta}=A'y$ be a generic linear estimator, where $A$ is any\n",
    "$n\\times K$ functions of $X$. As\n",
    "$$E\\left[A'y|X\\right]=E\\left[A'\\left(X\\beta+e\\right)|X\\right]=A'X\\beta.$$\n",
    "So the linearity and unbiasedness of $\\tilde{\\beta}$ implies\n",
    "$A'X=I_{n}$. Moreover, the variance\n",
    "$$\\mbox{var}\\left(A'y|X\\right)=E\\left[\\left(A'y-\\beta\\right)\\left(A'y-\\beta\\right)'|X\\right]=E\\left[A'ee'A|X\\right]=\\sigma^{2}A'A.$$\n",
    "Let $C=A-X\\left(X'X\\right)^{-1}.$\n",
    "$$\\begin{aligned}A'A-\\left(X'X\\right)^{-1} & =\\left(C+X\\left(X'X\\right)^{-1}\\right)'\\left(C+X\\left(X'X\\right)^{-1}\\right)-\\left(X'X\\right)^{-1}\\\\\n",
    " & =C'C+\\left(X'X\\right)^{-1}X'C+C'X\\left(X'X\\right)^{-1}\\\\\n",
    " & =C'C,\n",
    "\\end{aligned}$$ where the last equality follows as\n",
    "$$\\left(X'X\\right)^{-1}X'C=\\left(X'X\\right)^{-1}X'\\left(A-X\\left(X'X\\right)^{-1}\\right)=\\left(X'X\\right)^{-1}-\\left(X'X\\right)^{-1}=0.$$\n",
    "Therefore $A'A-\\left(X'X\\right)^{-1}$ is a positive semi-definite\n",
    "matrix. The variance of any $\\tilde{\\beta}$ is no smaller than the OLS\n",
    "estimator $\\widehat{\\beta}$. The above derivation shows OLS achieves the\n",
    "smallest variance among all linear unbiased estimators.\n",
    "\n",
    "Homoskedasticity is a restrictive assumption. Under homoskedasticity,\n",
    "$\\mathrm{var}\\left[\\widehat{\\beta}\\right]=\\sigma^{2}\\left(X'X\\right)^{-1}$.\n",
    "Popular estimator of $\\sigma^{2}$ is the sample mean of the residuals\n",
    "$\\widehat{\\sigma}^{2}=\\frac{1}{n}\\widehat{e}'\\widehat{e}$ or the\n",
    "unbiased one $s^{2}=\\frac{1}{n-K}\\widehat{e}'\\widehat{e}$. Under\n",
    "heteroskedasticity, Gauss-Markov theorem does not apply.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The exact distribution under the normality assumption of the error term\n",
    "is the classical statistical results. The Gauss Markov theorem holds\n",
    "under two crucial assumptions: linear CEF and homoskedasticity.\n",
    "\n",
    "**Historical notes**: MLE was promulgated and popularized by Ronald\n",
    "Fisher (1890–1962). He was a major contributor of the frequentist\n",
    "approach which dominates mathematical statistics today, and he sharply\n",
    "criticized the Bayesian approach. Fisher collected the iris flower\n",
    "dataset of 150 observations in his biological study in 1936, which can\n",
    "be displayed in R by typing `iris`. Fisher invented the many concepts in\n",
    "classical mathematical statistics, such as sufficient statistic,\n",
    "ancillary statistic, completeness, and exponential family, etc.\n",
    "\n",
    "**Further reading**: @phillips1983exact offered a comprehensive\n",
    "treatment of exact small sample theory in econometrics. After that,\n",
    "theoretical studies in econometrics swiftly shifted to large sample\n",
    "theory, which we will introduce in the next chapter.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### Joint Normal Distribution\n",
    "\n",
    "It is arguable that normal distribution is the most frequently\n",
    "encountered distribution in statistical inference, as it is the\n",
    "asymptotic distribution of many popular estimators. Moreover, it boasts\n",
    "some unique features that facilitates the calculation of objects of\n",
    "interest. This note summaries a few of them.\n",
    "\n",
    "An $n\\times1$ random vector $Y$ follows a joint normal distribution\n",
    "$N\\left(\\mu,\\Sigma\\right)$, where $\\mu$ is an $n\\times1$ vector and\n",
    "$\\Sigma$ is an $n\\times n$ symmetric positive definite matrix. The\n",
    "probability density function is\n",
    "$$f_{y}\\left(y\\right)=\\left(2\\pi\\right)^{-n/2}\\left(\\mathrm{det}\\left(\\Sigma\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2}\\left(y-\\mu\\right)'\\Sigma^{-1}\\left(y-\\mu\\right)\\right)$$\n",
    "where $\\mathrm{det}\\left(\\cdot\\right)$ is the determinant of a matrix.\n",
    "The moment generating function is\n",
    "$$M_{y}\\left(t\\right)=\\exp\\left(t'\\mu+\\frac{1}{2}t'\\Sigma t\\right).$$\n",
    "\n",
    "We will discuss the relationship between two components of a random\n",
    "vector. To fix notation, $$Y=\\left(\\begin{array}{c}\n",
    "Y_{1}\\\\\n",
    "Y_{2}\n",
    "\\end{array}\\right)\\sim N\\left(\\left(\\begin{array}{c}\n",
    "\\mu_{1}\\\\\n",
    "\\mu_{2}\n",
    "\\end{array}\\right),\\left(\\begin{array}{cc}\n",
    "\\Sigma_{11} & \\Sigma_{12}\\\\\n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{array}\\right)\\right)$$ where $Y_{1}$ is an $m\\times1$ vector, and\n",
    "$Y_{2}$ is an $\\left(n-m\\right)\\times1$ vector. $\\mu_{1}$ and $\\mu_{2}$\n",
    "are the corresponding mean vectors, and $\\Sigma_{ij}$, $j=1,2$ are the\n",
    "corresponding variance and covariance matrices. From now on, we always\n",
    "maintain the assumption that $Y=\\left(Y_{1}',Y_{2}'\\right)'$ is jointly\n",
    "normal.\n",
    "\n",
    "Fact\n",
    "<a href=\"#fact31\" data-reference-type=\"ref\" data-reference=\"fact31\">[fact31]</a>\n",
    "immediately implies a convenient feature of the normal distribution.\n",
    "Generally speaking, if we are given a joint pdf of two random variables\n",
    "and intend to find the marginal distribution of one random variables, we\n",
    "need to integrate out the other variable from the joint pdf. However, if\n",
    "the variables are jointly normal, the information of the other random\n",
    "variable is irrelevant to the marginal distribution of the random\n",
    "variable of interest. We only need to know the partial information of\n",
    "the part of interest, say the mean $\\mu_{1}$ and the variance\n",
    "$\\Sigma_{11}$ to decide the marginal distribution of $Y_{1}$.\n",
    "\n",
    "<span id=\"fact:marginal\"\n",
    "label=\"fact:marginal\">\\[fact:marginal\\]</span>The marginal distribution\n",
    "$Y_{1}\\sim N\\left(\\mu_{1},\\Sigma_{11}\\right)$.\n",
    "\n",
    "This result is very convenient if we are interested in some component if\n",
    "an estimator, but not the entire vector of the estimator. For example,\n",
    "the OLS estimator of the linear regression model\n",
    "$y_{i}=x_{i}'\\beta+e_{i}$, under the classical assumption of (i) random\n",
    "sample; (ii) independence of $z_{i}$ and $e_{i}$; (iii)\n",
    "$e_{i}\\sim N\\left(0,\\gamma\\right)$ is\n",
    "$$\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y,$$ and the finite sample\n",
    "exact distribution of $\\widehat{\\beta}$ is\n",
    "$$\\left(\\widehat{\\beta}-\\beta\\right)|X\\sim N\\left(0,\\gamma\\left(X'X\\right)^{-1}\\right)$$\n",
    "If we are interested in the inference of only the $j$-th component of\n",
    "$\\beta_{0}^{\\left(j\\right)}$, then from Fact\n",
    "<a href=\"#fact:marginal\" data-reference-type=\"ref\" data-reference=\"fact:marginal\">[fact:marginal]</a>,\n",
    "$$\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\left(X'X\\right)_{kk}^{-1}\\sim N\\left(0,\\gamma\\right)$$\n",
    "where $\\left[\\left(X'X\\right)^{-1}\\right]_{kk}$ is the $k$-th diagonal\n",
    "element of $\\left(X'X\\right)^{-1}$. The marginal distribution is\n",
    "independent of the other components. This saves us from integrating out\n",
    "the other components, which could be troublesome if the dimension of the\n",
    "vector is high.\n",
    "\n",
    "Generally, zero covariance of two random variables only indicates that\n",
    "they are uncorrelated, whereas full statistical independence is a much\n",
    "stronger requirement. However, if $Y_{1}$ and $Y_{2}$ are jointly\n",
    "normal, then zero covariance is equivalent to full independence.\n",
    "\n",
    "If $\\Sigma_{12}=0$, then $Y_{1}$ and $Y_{2}$ are independent.\n",
    "\n",
    "If $\\Sigma$ is invertible, then\n",
    "$Y'\\Sigma^{-1}Y\\sim\\chi^{2}\\left(\\mathrm{rank}\\left(\\Sigma\\right)\\right)$.\n",
    "\n",
    "The last result, which is useful in linear regression, is that if\n",
    "$Y_{1}$ and $Y_{2}$ are jointly normal, the conditional distribution of\n",
    "$Y_{1}$ on $Y_{2}$ is still jointly normal, with the mean and variance\n",
    "specified as in the following fact.\n",
    "\n",
    "$Y_{1}|Y_{2}\\sim N\\left(\\mu_{1}+\\Sigma_{12}\\Sigma_{22}^{-1}\\left(Y_{2}-\\mu_{2}\\right),\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\right)$.\n",
    "\n",
    "### Basu’s Theorem\\* \\[\\[subsec:Basu's-Theorem\\]\\]{\\#subsec:Basu’s-Theorem label=“subsec:Basu’s-Theorem”}\n",
    "\n",
    "$Y=\\left(y_{1},\\ldots,y_{n}\\right)$ consists of $n$ iid observations. We\n",
    "say $T\\left(Y\\right)$ is a *sufficient statistic* for a parameter\n",
    "$\\theta$ if the conditional probability\n",
    "$f\\left(Y|T\\left(Y\\right)\\right)$ does not depend on $\\theta$. We say\n",
    "$S\\left(Y\\right)$ is an *ancillary statistic* for $\\theta$ if its\n",
    "distribution does not depend on $\\theta$.\n",
    "\n",
    "*Basu’s theorem* says that a *complete* sufficient statistic is\n",
    "statistically independent from any ancillary statistic.\n",
    "\n",
    "Sufficient statistic is closely related to the exponential family in\n",
    "classical mathematical statistics. A parametric distribution indexed by\n",
    "$\\theta$ is a member of the *exponential family* is its PDF can be\n",
    "written as\n",
    "$$f\\left(Y|\\theta\\right)=h\\left(Y\\right)g\\left(\\theta\\right)\\exp\\left(\\eta\\left(\\theta\\right)'T\\left(Y\\right)\\right),$$\n",
    "where $g\\left(\\theta\\right)$ and $\\eta\\left(\\theta\\right)$ are functions\n",
    "depend, only on $\\theta$ and $h\\left(Y\\right)$ and $T\\left(Y\\right)$ are\n",
    "functions depend only on $Y$.\n",
    "\n",
    "(Univariate Gaussian location model.) For a normal distribution\n",
    "$y_{i}\\sim N\\left(\\mu,\\gamma\\right)$ with known $\\gamma$ and unknown\n",
    "$\\mu$, the sample mean $\\bar{y}$ is the sufficient statistic and the\n",
    "sample standard deviation $s^{2}$ is an ancillary statistic.\n",
    "\n",
    "We first verify that the sample mean $\\bar{y}=n^{-1}\\sum_{i=1}^{n}y_{i}$\n",
    "is a sufficient statistic for $\\mu$. Notice that the joint density of\n",
    "$Y$ is $$\\begin{aligned}\n",
    "f\\left(Y\\right) & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\\right)\\\\\n",
    " & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(\\left(y_{i}-\\bar{y}\\right)+\\left(\\bar{y}-\\mu\\right)\\right)^{2}\\right)\\\\\n",
    " & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(\\left(y_{i}-\\bar{y}\\right)^{2}+2\\left(y_{i}-\\bar{y}\\right)\\left(\\bar{y}-\\mu\\right)+\\left(\\bar{y}-\\mu\\right)^{2}\\right)\\right)\\\\\n",
    " & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\\right)\\exp\\left(-\\frac{n}{2\\gamma}\\left(\\bar{y}-\\mu\\right)^{2}\\right).\\end{aligned}$$\n",
    "Because $\\bar{y}\\sim N\\left(\\mu,\\gamma/n\\right),$ the marginal density\n",
    "is\n",
    "$$f\\left(\\bar{y}\\right)=\\left(2\\pi\\gamma/n\\right)^{-1/2}\\exp\\left(-\\frac{n}{2\\gamma}\\left(\\bar{y}-\\mu\\right)^{2}\\right).$$\n",
    "For $\\bar{y}$ is a statistic of $Y$, we have\n",
    "$f\\left(Y,\\bar{y}\\right)=f\\left(Y\\right)$. The conditional density is\n",
    "$$f\\left(Y|\\bar{y}\\right)=\\frac{f\\left(Y,\\bar{y}\\right)}{f\\left(\\bar{y}\\right)}=\\frac{f\\left(Y\\right)}{f\\left(\\bar{y}\\right)}=\\sqrt{n}\\left(2\\pi\\gamma\\right)^{-\\frac{n-1}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\\right)$$\n",
    "is independent of $\\mu$, and thus $\\bar{y}$ is a sufficient statistic\n",
    "for $\\mu$. In the meantime, the sample standard deviation\n",
    "$s^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)$ is an\n",
    "*ancillary statistic* for $\\mu$ , because the distribution of $s^{2}$\n",
    "does not depend on $\\mu.$\n",
    "\n",
    "The normal distribution with known $\\sigma^{2}$ and unknown $\\mu$\n",
    "belongs to the exponential family in view of the decomposition\n",
    "$$\\begin{aligned}\n",
    "f(Y) & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\\right)\\\\\n",
    " & =\\underbrace{\\exp\\left(-\\sum_{i=1}^{n}\\frac{y_{i}^{2}}{2\\gamma}\\right)}_{h\\left(Y\\right)}\\cdot\\underbrace{\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{n}{2\\gamma}\\mu^{2}\\right)}_{g\\left(\\theta\\right)}\\cdot\\underbrace{\\exp\\left(\\frac{\\mu}{2\\gamma}n\\bar{y}\\right)}_{\\exp\\left(\\eta\\left(\\theta\\right)'T\\left(Y\\right)\\right)}.\\end{aligned}$$\n",
    "The exponential family is a class of distributions with the special\n",
    "functional form which is convenient for deriving sufficient statistics\n",
    "as well as other desirable properties in classical mathematical\n",
    "statistics.\n",
    "\n",
    "(Conditional Gaussian location model.) If\n",
    "$y_{i}\\sim N\\left(x_{i}\\beta,\\gamma\\right)$ with known $\\gamma$ and\n",
    "unknown $\\beta$, We verify that the sample mean $\\widehat{\\beta}$ is a\n",
    "sufficient statistic for $\\beta$. Notice that the joint density of $Y$\n",
    "given $X$ is $$\\begin{aligned}\n",
    "f\\left(Y|X\\right) & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\\right)\\\\\n",
    " & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(Y-X\\widehat{\\beta}\\right)'\\left(Y-X\\widehat{\\beta}\\right)\\right)\\exp\\left(-\\frac{1}{2\\gamma}\\left(\\widehat{\\beta}-\\beta\\right)'X'X\\left(\\widehat{\\beta}-\\beta\\right)\\right).\\end{aligned}$$\n",
    "Because\n",
    "$\\widehat{\\beta}\\sim N\\left(\\beta,\\gamma\\left(X'X\\right)^{-1}\\right),$\n",
    "the marginal density is\n",
    "$$f\\left(\\widehat{\\beta}|X\\right)=\\left(2\\pi\\gamma\\right)^{-\\frac{K}{2}}\\left(\\mathrm{det}\\left(\\left(X'X\\right)^{-1}\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2\\gamma}\\left(\\widehat{\\beta}-\\beta\\right)'X'X\\left(\\widehat{\\beta}-\\beta\\right)\\right).$$\n",
    "The conditional density is $$\\begin{aligned}\n",
    "f\\left(Y|\\widehat{\\beta},X\\right) & =\\frac{f\\left(Y|X\\right)}{f\\left(\\widehat{\\beta}|X\\right)}\\\\\n",
    " & =\\left(2\\pi\\gamma\\right)^{-\\frac{n-K}{2}}\\left(\\mathrm{det}\\left(\\left(X'X\\right)^{-1}\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2\\gamma}\\left(Y-X\\widehat{\\beta}\\right)'\\left(Y-X\\widehat{\\beta}\\right)\\right)\\end{aligned}$$\n",
    "is independent of $\\beta$, and thus $\\widehat{\\beta}$ is a sufficient\n",
    "statistic for $\\beta$.\n",
    "\n",
    "In the meantime, the sample standard deviation\n",
    "$s^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}\\widehat{\\beta}\\right)$\n",
    "is an *ancillary statistic* for $\\beta$ , because the distribution of\n",
    "$s^{2}$ does not depend on $\\beta.$\n",
    "\n",
    "`Zhentao Shi. Oct 10.`"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
