#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass report
\begin_preamble
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "urlcolor=urlcolor,linkcolor=linkcolor,citecolor=citecolor,"
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{chapter}{2}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Least Squares: Linear Algebra
\end_layout

\begin_layout Standard
Notation: 
\begin_inset Formula $y_{i}$
\end_inset

 is a scalar, and 
\begin_inset Formula $x_{i}=\left(x_{i1},\ldots,x_{iK}\right)'$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 vector.
 
\begin_inset Formula $Y=\left(y_{1},\ldots,y_{n}\right)'$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector, and 
\begin_inset Formula 
\[
X=\left[\begin{array}{c}
x_{1}'\\
x_{2}'\\
\vdots\\
x_{n}'
\end{array}\right]=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1K}\\
x_{21} & x_{22} & \cdots & x_{2K}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{22} & \cdots & x_{nK}
\end{array}\right]
\]

\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 matrix.
 
\begin_inset Formula $I_{n}$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 identity matrix.
\end_layout

\begin_layout Standard
Ordinary least squares (OLS) is the most basic estimation technique in econometr
ics.
 It is simple and transparent.
 Understanding it thoroughly paves the way to study more sophisticated linear
 estimators.
 Moreover, many nonlinear estimators resemble the behavior of linear estimators
 in a neighborhood of the true value.
 In this lecture, we learn a series of facts from the linear algebra operation.
\end_layout

\begin_layout Standard
To manipulate Leopold Kronecker's famous saying 
\begin_inset Quotes eld
\end_inset

God made the integers; all else is the works of man
\begin_inset Quotes erd
\end_inset

, I would say 
\begin_inset Quotes eld
\end_inset

Gauss made OLS; all else is the works of applied researchers.
\begin_inset Quotes erd
\end_inset

 Popularity of OLS goes far beyond our dismal science.
 But be aware that OLS is a pure statistical or supervised machine learning
 method which reveals correlation instead of causality.
 Rather, economic theory hypothesizes causality while data are collected
 to test the theory or quantify the effect.
\end_layout

\begin_layout Section
Estimator
\end_layout

\begin_layout Standard
As we have learned from the linear project model, the projection coefficient
 
\begin_inset Formula $\beta$
\end_inset

 in the regression 
\begin_inset Formula 
\[
\begin{aligned}y & =x'\beta+e\end{aligned}
\]

\end_inset

can be written as 
\begin_inset Formula 
\begin{equation}
\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right].\label{eq:pop_OLS}
\end{equation}

\end_inset

 We draw a pair of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 from the joint distribution, and we mark it as 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 for 
\begin_inset Formula $i=1,\ldots,n$
\end_inset

 repeated experiments.
 We possess a 
\emph on
sample
\emph default
 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

.
\end_layout

\begin_layout Remark
Is 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 random or deterministic? Before we make the observation, they are treated
 as random variables whose realized values are uncertain.
 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 is treated as random when we talk about statistical properties — statistical
 properties of a fixed number is meaningless.
 After we make the observation, they become deterministic values which cannot
 vary anymore.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
In reality, we have at hand fixed numbers (more recently, words, photos,
 audio clips, video clips, etc., which can all be represented in digital
 formats with 0 and 1) to feed into a computational operation, and the operation
 will return one or some numbers.
 All statistical interpretation about these numbers are drawn from the probabili
stic thought experiments.
 A 
\emph on
thought experiment
\emph default
 is an academic jargon for a 
\emph on
story
\emph default
 in plain language.
 Under the axiomatic approach of probability theory, such stories are mathematic
al consistent and coherent.
 But mathematics is a tautological system, not science.
 The scientific value of a probability model depends on how close it is
 to the 
\emph on
truth
\emph default
 or implications of the truth.
 In this course, we suppose that the data are generated from some mechanism,
 which is taken as the truth.
 In the linear regression model for example, the joint distribution of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 is the truth, while we are interested in the linear projection coefficient
 
\begin_inset Formula $\beta$
\end_inset

, which is an implication of the truth as in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pop_OLS"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Probabilists suppose there is a dragon and try to tell the dragon's behaviors.
 Statisticians observe many snakes on earth, and try to tell what a dragon
 looks like.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The sample mean is a natural estimator of the population mean.
 Replace the population mean 
\begin_inset Formula $E\left[\cdot\right]$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pop_OLS"
plural "false"
caps "false"
noprefix "false"

\end_inset

) by the sample mean 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\cdot$
\end_inset

, and the resulting estimator is 
\begin_inset Formula 
\begin{align*}
\widehat{\beta} & =\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}\\
 & =\left(\frac{X'X}{n}\right)^{-1}\frac{X'y}{n}=\left(X'X\right)^{-1}X'y
\end{align*}

\end_inset

if 
\begin_inset Formula $X'X$
\end_inset

 is invertible.
 This is one way to motivate the OLS estimator.
 
\end_layout

\begin_layout Standard
Alternatively, we can derive the OLS estimator from minimizing the sum of
 squared residuals 
\begin_inset Formula $\sum_{i=1}^{n}\left(y_{i}-x_{i}'b\right)^{2}$
\end_inset

, or equivalently 
\begin_inset Formula 
\[
Q\left(b\right)=\frac{1}{2n}\sum_{i=1}^{n}\left(y_{i}-x_{i}'b\right)^{2}=\frac{1}{2n}\left(Y-Xb\right)'\left(Y-Xb\right)=\frac{1}{2n}\left\Vert Y-Xb\right\Vert ^{2},
\]

\end_inset

where the factor 
\begin_inset Formula $\frac{1}{2n}$
\end_inset

 is nonrandom and does not change the minimizer, and 
\begin_inset Formula $\left\Vert \cdot\right\Vert $
\end_inset

 is the Euclidean norm of a vector.
 Solve the first-order condition 
\begin_inset Formula 
\[
\frac{\partial}{\partial b}Q\left(b\right)=\left[\begin{array}{c}
\partial Q\left(b\right)/\partial b_{1}\\
\partial Q\left(b\right)/\partial b_{2}\\
\vdots\\
\partial Q\left(b\right)/\partial b_{K}
\end{array}\right]=-\frac{1}{n}X'\left(Y-Xb\right)=0.
\]

\end_inset

This necessary condition for optimality gives exactly the same 
\begin_inset Formula $\widehat{\beta}=\left(X'X\right)^{-1}X'y$
\end_inset

.
 Moreover, the second-order condition 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial b\partial b'}Q\left(b\right)=\left[\begin{array}{cccc}
\frac{\partial^{2}}{\partial b_{1}^{2}}Q\left(b\right) & \frac{\partial^{2}}{\partial b_{2}\partial b_{2}}Q\left(b\right) & \cdots & \frac{\partial^{2}}{\partial b_{K}\partial b_{1}}Q\left(b\right)\\
\frac{\partial^{2}}{\partial b_{1}\partial b_{2}}Q\left(b\right) & \frac{\partial^{2}}{\partial b_{2}^{2}}Q\left(b\right) & \cdots & \frac{\partial^{2}}{\partial b_{K}\partial b_{2}}Q\left(b\right)\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\partial^{2}}{\partial b_{1}\partial b_{K}}Q\left(b\right) & \frac{\partial^{2}}{\partial b_{2}\partial b_{K}}Q\left(b\right) & \cdots & \frac{\partial^{2}}{\partial b_{K}^{2}}Q\left(b\right)
\end{array}\right]=\frac{1}{n}X'X
\]

\end_inset

shows that 
\begin_inset Formula $Q\left(b\right)$
\end_inset

 is convex in 
\begin_inset Formula $b$
\end_inset

 due to the positive semi-definite matrix 
\begin_inset Formula $X'X/n$
\end_inset

.
 (The function 
\begin_inset Formula $Q\left(b\right)$
\end_inset

 is strictly convex in 
\begin_inset Formula $b$
\end_inset

 if 
\begin_inset Formula $X'X/n$
\end_inset

 is positive definite.)
\end_layout

\begin_layout Remark
In the derivation of OLS we presume that the 
\begin_inset Formula $K$
\end_inset

 columns in 
\begin_inset Formula $X$
\end_inset

 are 
\emph on
linearly independent
\emph default
, which means there is no 
\begin_inset Formula $K\times1$
\end_inset

 vector 
\begin_inset Formula $b$
\end_inset

 such that 
\begin_inset Formula $b\neq0_{K}$
\end_inset

 and 
\begin_inset Formula $Xb=0_{n}$
\end_inset

.
 Linear independence of the columns implies 
\begin_inset Formula $n\geq K$
\end_inset

 and the invertibility of 
\begin_inset Formula $X'X/n$
\end_inset

.
 Linear independence is violated when some regressors are 
\emph on
perfectly collinear
\emph default
, for example when we use dummy variables to indicate categorical variables
 and put all these categories into the regression.
 Modern econometrics software automatically detects and reports perfect
 collinearity.
 What is treacherous is 
\emph on
nearly collinear
\emph default
, meaning that the minimal eigenvalue of 
\begin_inset Formula $X'X/n$
\end_inset

 is close to 0, though not exactly equal to 0.
 We will talk about the consequence of near collinearity in the chapter
 of asymptotic theory.
\end_layout

\begin_layout Standard
Here are some definitions and properties of the OLS estimator.
\end_layout

\begin_layout Itemize
Fitted value: 
\begin_inset Formula $\widehat{Y}=X\widehat{\beta}$
\end_inset

.
\end_layout

\begin_layout Itemize
Projection matrix: 
\begin_inset Formula $P_{X}=X\left(X'X\right)^{-1}X$
\end_inset

; Residual maker matrix: 
\begin_inset Formula $M_{X}=I_{n}-P_{X}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $P_{X}X=X$
\end_inset

; 
\begin_inset Formula $X'P_{X}=X'$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $M_{X}X=0_{n\times K}$
\end_inset

; 
\begin_inset Formula $X'M_{X}=0_{K\times n}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $P_{X}M_{X}=M_{X}P_{X}=0_{n\times n}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $AA=A$
\end_inset

, we call it an 
\emph on
idempotent
\emph default
 matrix.
 Both 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $M_{X}$
\end_inset

 are idempotent.
 All eigenvalues of an idempotent matrix must be either 1 or 0.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathrm{rank}\left(P_{X}\right)=K$
\end_inset

, and 
\begin_inset Formula $\mathrm{rank}\left(M_{X}\right)=n-K$
\end_inset

 (See the Appendix of this chapter).
\end_layout

\begin_layout Itemize
Residual: 
\begin_inset Formula $\widehat{e}=Y-\widehat{Y}=Y-X\widehat{\beta}=Y-X(X'X)^{-1}X'Y=(I_{n}-P_{X})Y=M_{X}Y=M_{X}\left(X\beta+e\right)=M_{X}e$
\end_inset

.
 Notice 
\begin_inset Formula $\widehat{e}$
\end_inset

 and 
\begin_inset Formula $e$
\end_inset

 are two different objects.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $X'\widehat{e}=X'M_{X}e=0_{K}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

 if 
\begin_inset Formula $x_{i}$
\end_inset

 contains a constant.
 
\end_layout

\begin_deeper
\begin_layout Standard
(Because 
\begin_inset Formula $X'\widehat{e}=\left[\begin{array}{cccc}
1 & 1 & \cdots & 1\\
\heartsuit & \heartsuit & \cdots & \heartsuit\\
\cdots & \cdots & \ddots & \vdots\\
\heartsuit & \heartsuit & \cdots & \heartsuit
\end{array}\right]\left[\begin{array}{c}
\widehat{e}_{1}\\
\widehat{e}_{2}\\
\vdots\\
\widehat{e}_{n}
\end{array}\right]=\left[\begin{array}{c}
0\\
0\\
\vdots\\
0
\end{array}\right]$
\end_inset

 and the the first row implies 
\begin_inset Formula $\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

.
 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $\heartsuit$
\end_inset


\begin_inset Quotes erd
\end_inset

 indicates the entries irrelevant to our purpose.)
\end_layout

\end_deeper
\begin_layout Standard
The operation of OLS bears a natural geometric interpretation.
 Notice 
\begin_inset Formula $\mathcal{X}=\left\{ Xb:b\in\mathbb{R}^{K}\right\} $
\end_inset

 is the linear space spanned by the 
\begin_inset Formula $K$
\end_inset

 columns of 
\begin_inset Formula $X=\left[X_{\cdot1},\ldots,X_{\cdot K}\right]$
\end_inset

, which is of 
\begin_inset Formula $K$
\end_inset

-dimension if the columns are linearly independent.
 The OLS estimator is the minimizer of 
\begin_inset Formula $\min_{b\in\mathbb{R}^{K}}\left\Vert Y-Xb\right\Vert $
\end_inset

 (Square the Euclidean norm or not does not change the minimizer because
 
\begin_inset Formula $a^{2}$
\end_inset

 is a monotonic transformation for 
\begin_inset Formula $a\geq0$
\end_inset

).
 In other words, 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 is the point in 
\begin_inset Formula $\mathcal{X}$
\end_inset

 such that it is the closest to the vector 
\begin_inset Formula $Y$
\end_inset

 in terms of the Euclidean norm.
\end_layout

\begin_layout Standard
The relationship 
\begin_inset Formula $Y=X\widehat{\beta}+\widehat{e}$
\end_inset

 decomposes 
\begin_inset Formula $Y$
\end_inset

 into two orthogonal vectors 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 and 
\begin_inset Formula $\widehat{e}$
\end_inset

 as 
\begin_inset Formula $\left\langle X\widehat{\beta},\widehat{e}\right\rangle =\widehat{\beta}X'\widehat{e}=0_{K}^{\prime}$
\end_inset

, where 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 is the 
\emph on
inner product
\emph default
 of two vectors.
 Therefore 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 is the 
\emph on
projection
\emph default
 of 
\begin_inset Formula $Y$
\end_inset

 onto 
\begin_inset Formula $\mathcal{X}$
\end_inset

, and 
\begin_inset Formula $\widehat{e}$
\end_inset

 is the corresponding 
\emph on
projection residuals.
 
\emph default
The Pythagorean theorem implies
\begin_inset Formula 
\[
\left\Vert Y\right\Vert ^{2}=\Vert X\widehat{\beta}\Vert^{2}+\left\Vert \widehat{e}\right\Vert ^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
definecolor{dtsfsf}{rgb}{0.8274509803921568,0.1843137254901961,0.1843137254901961}
 
\backslash
definecolor{wrwrwr}{rgb}{0.3803921568627451,0.3803921568627451,0.3803921568627451}
 
\backslash
definecolor{rvwvcq}{rgb}{0.08235294117647059,0.396078431372549,0.7529411764705882}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm,
 scale=0.5] 
\end_layout

\begin_layout Plain Layout


\backslash
clip(-20.36,-10.76) rectangle (3.16,11.16); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-14.94,-1.92) -- (-5.88,-1.8); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-14.94,-1.92) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.88,-1.8) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.86,5.1) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.86,5.1) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.86,5.1) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.86,5.1) -- (-5.86,5.1); 
\backslash
draw (-10.26,-2) node[anchor=north west] {$
\backslash
hat{Y} = X 
\backslash
hat{
\backslash
beta}$}; 
\backslash
draw (-10.56,2.94) node[anchor=north west] {$Y$}; 
\backslash
draw (-5.4,2.04) node[anchor=north west] {$
\backslash
hat{e}$}; 
\backslash
draw (-10.12,-3.12) node[anchor=north west] {projection}; 
\backslash
draw (-5.42,1.14) node[anchor=north west] {residual}; 
\backslash
draw [line width=2pt,color=dtsfsf,domain=-20.36:3.16] plot(
\backslash
x,{(-15.6024--0.12*
\backslash
x)/9.06}); 
\backslash
draw (-4.48,-1.72) node[anchor=north west] {$
\backslash
mathcal{X}$}; 
\backslash
begin{scriptsize} 
\backslash
draw [fill=rvwvcq] (-14.94,-1.92) circle (2.5pt); 
\backslash
draw [fill=rvwvcq] (-5.88,-1.8) circle (2.5pt); 
\backslash
draw [fill=rvwvcq] (-5.86,5.1) circle (2.5pt); 
\backslash
draw [fill=rvwvcq] (-5.86,5.1) circle (2.5pt); 
\backslash
end{scriptsize} 
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Example
Here is a simple simulated example to demonstrate the properties of OLS.
 Given 
\begin_inset Formula $\left(x_{1i},x_{2i},x_{3i},e_{i}\right)^{\prime}\sim N\left(0_{4},I_{4}\right)$
\end_inset

, the dependent variable 
\begin_inset Formula $y_{i}$
\end_inset

 is generated from 
\begin_inset Formula 
\[
y_{i}=0.5+2\cdot x_{1i}-1\cdot x_{2i}+e_{i}
\]

\end_inset

The researcher does not know 
\begin_inset Formula $x_{3i}$
\end_inset

 is redundant, and he regresses 
\begin_inset Formula $y_{i}$
\end_inset

 on 
\begin_inset Formula $\left(1,x_{1i},x_{2i},x_{3i}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

library(magrittr); set.seed(2020-9-23)
\end_layout

\begin_layout Plain Layout

n = 20 # sample size  
\end_layout

\begin_layout Plain Layout

K = 4  # number of paramters
\end_layout

\begin_layout Plain Layout

b0 = as.matrix( c(0.5, 2, -1, 0) ) # the true coefficient
\end_layout

\begin_layout Plain Layout

X = cbind(1, matrix( rnorm(n * (K-1)), nrow = n ) )  # the regressor matrix
 
\end_layout

\begin_layout Plain Layout

e = rnorm(n) # the error term
\end_layout

\begin_layout Plain Layout

Y = X %*% b0 + e # generate the dependent variable
\end_layout

\begin_layout Plain Layout

bhat = solve(t(X) %*% X, t(X) %*% Y ) %>% as.vector() %>% print()
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The estimated coefficient 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{ format(bhat, digits=3) }
\end_layout

\end_inset

).
 It is close to the true value, but not very accurate due to the small sample
 size.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

ehat = Y - X %*% bhat 
\end_layout

\begin_layout Plain Layout

as.vector( t(X) %*% ehat ) %>% print()
\end_layout

\begin_layout Plain Layout

MX = diag(n) - X %*% solve( crossprod(X) ) %*% t(X)
\end_layout

\begin_layout Plain Layout

data.frame(e = e, ehat = ehat, MXY = MX%*%Y, MXe = MX%*%e ) %>% head()
\end_layout

\begin_layout Plain Layout

cat("The mean of the residual is ", mean(ehat), ".
\backslash
n
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout

cat(
\begin_inset Quotes eld
\end_inset

The mean of the true error term is", mean(e), 
\begin_inset Quotes eld
\end_inset

.
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Section
Subvector
\end_layout

\begin_layout Standard
The Frish-Waugh-Lovell (FWL) theorem is an algebraic fact about the formula
 of a subvector of the OLS estimator.
 To derive the FWL theorem we need to use the inverse of partitioned matrix.
 For a positive definite symmetric matrix 
\begin_inset Formula $A=\begin{pmatrix}A_{11} & A_{12}\\
A_{12}' & A_{22}
\end{pmatrix}$
\end_inset

, the inverse can be written as 
\begin_inset Formula 
\[
A^{-1}=\begin{pmatrix}\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} & -\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1}A_{12}A_{22}^{-1}\\
-A_{22}^{-1}A_{12}'\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} & \left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\right)^{-1}
\end{pmatrix}.
\]

\end_inset

In our context of OLS estimator, let 
\begin_inset Formula $X=\left(\begin{array}{cc}
X_{1} & X_{2}\end{array}\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\begin{pmatrix}\widehat{\beta}_{1}\\
\widehat{\beta}_{2}
\end{pmatrix} & =\widehat{\beta}=(X'X)^{-1}X'Y\\
 & =\left(\begin{pmatrix}X_{1}'\\
X_{2}'
\end{pmatrix}\begin{pmatrix}X_{1} & X_{2}\end{pmatrix}\right)^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 & =\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\
X_{2}'X_{1} & X_{2}'X_{2}
\end{pmatrix}^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 & =\begin{pmatrix}\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1} & -\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}\\
\heartsuit & \heartsuit
\end{pmatrix}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The subvector
\begin_inset Formula 
\begin{align*}
\widehat{\beta}_{1} & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}X_{2}'Y\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'P_{X_{2}}Y\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\right)\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'M_{X_{2}}Y.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

 can be obtained by the following:
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $X_{2}$
\end_inset

, obtain the residual 
\begin_inset Formula $\tilde{Y}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $X_{1}$
\end_inset

 on 
\begin_inset Formula $X_{2}$
\end_inset

, obtain the residual 
\begin_inset Formula $\tilde{X}_{1}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $\tilde{Y}$
\end_inset

 on 
\begin_inset Formula $\tilde{X}_{1}$
\end_inset

, obtain OLS estimates 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

.
\end_layout

\begin_layout Standard
Similar derivation can also be carried out in the population linear projection.
 See Hansen (2020) [E] Chapter 2.22-23.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

X1 = X[,1:2];X2 = X[,3:4]
\end_layout

\begin_layout Plain Layout

PX2 = X2 %*% solve( t(X2) %*% X2) %*% t(X2) 
\end_layout

\begin_layout Plain Layout

MX2 = diag(rep(1,n)) - PX2
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

bhat1 <- (solve(t(X1)%*% MX2 %*% X1, t(X1) %*% MX2 %*% Y )) %>%
\end_layout

\begin_layout Plain Layout

  as.vector() %>% print()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

ehat1 = MX2 %*% Y - MX2 %*% X1 %*% bhat1  
\end_layout

\begin_layout Plain Layout

data.frame(ehat = ehat, ehat1 = ehat1) %>% head() %>% print()
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Section
Goodness of Fit
\end_layout

\begin_layout Standard
Consider the regression with the intercept 
\begin_inset Formula $Y=X_{1}\beta_{1}+\beta_{2}+e.$
\end_inset

 The OLS estimator gives 
\begin_inset Formula 
\begin{equation}
Y=\widehat{Y}+\widehat{e}=\left(X_{1}\widehat{\beta}_{1}+\widehat{\beta}_{2}\right)+\widehat{e}.\label{eq:decomp_1}
\end{equation}

\end_inset

Applying the FWL theorem with 
\begin_inset Formula $X_{2}=\iota$
\end_inset

, where 
\begin_inset Formula $\iota$
\end_inset

 (Greek letter, iota) is an 
\begin_inset Formula $n\times1$
\end_inset

 vector of 1's.
 Then 
\begin_inset Formula $M_{X_{2}}=M_{\iota}=I_{n}-\frac{1}{n}\iota\iota'$
\end_inset

.
 Notice 
\begin_inset Formula $M_{\iota}$
\end_inset

 is the 
\emph on
demeaner
\emph default
 in that 
\begin_inset Formula $M_{\iota}z=z-\bar{z}$
\end_inset

.
 It subtract the vector mean 
\begin_inset Formula $\bar{z}=\frac{1}{n}\sum_{i=1}^{n}z_{i}$
\end_inset

 from the original vector 
\begin_inset Formula $z$
\end_inset

.
 The above three-step procedure becomes
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $\iota$
\end_inset

, and the residual is 
\begin_inset Formula $M_{\iota}Y$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $X_{1}$
\end_inset

 on 
\begin_inset Formula $\iota$
\end_inset

, and the residual is 
\begin_inset Formula $M_{\iota}X_{1}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $M_{\iota}Y$
\end_inset

 on 
\begin_inset Formula $M_{\iota}X_{1}$
\end_inset

, and the OLS estimates is exactly the same as 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decomp_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
The last step gives the decomposition
\begin_inset Formula 
\begin{equation}
M_{\iota}Y=M_{\iota}X_{1}\widehat{\beta}_{1}+\tilde{e},\label{eq:decomp_2}
\end{equation}

\end_inset

and the Pythagorean theorem implies
\begin_inset Formula 
\[
\left\Vert M_{\iota}Y\right\Vert ^{2}=\Vert M_{\iota}X_{1}\widehat{\beta}_{1}\Vert^{2}+\left\Vert \widehat{e}\right\Vert ^{2}.
\]

\end_inset


\end_layout

\begin_layout Exercise
\begin_inset CommandInset label
LatexCommand label
name "ex:e_equiv"

\end_inset

 Show that 
\begin_inset Formula $\widehat{e}$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decomp_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is exactly the same as 
\begin_inset Formula $\tilde{e}$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decomp_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard

\emph on
R-squared
\emph default
 is a popular measure of goodness-of-fit in the linear regression.
 The (in-sample) R-squared
\begin_inset Formula 
\[
R^{2}=\frac{\Vert M_{\iota}X_{1}\widehat{\beta}_{1}\Vert^{2}}{\left\Vert M_{\iota}Y\right\Vert ^{2}}=1-\frac{\left\Vert \tilde{e}\right\Vert ^{2}}{\left\Vert M_{\iota}Y\right\Vert ^{2}}.
\]

\end_inset

is well defined only when a constant is included in the regressors.
\end_layout

\begin_layout Exercise
Show 
\begin_inset Formula 
\[
R^{2}=\frac{\widehat{Y}'M_{\iota}\widehat{Y}}{Y'M_{\iota}Y}=\frac{\sum_{i=1}^{n}\left(\widehat{y_{i}}-\overline{y}\right)^{2}}{\sum_{i=1}^{n}\left(y_{i}-\overline{y}\right)^{2}}
\]

\end_inset

 as in the decomposition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decomp_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 In other words, it is the ratio between the sample variance of 
\begin_inset Formula $\widehat{Y}$
\end_inset

 and the sample variance of 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
We can decompose 
\begin_inset Formula $Y=P_{X}Y+M_{X}Y=\widehat{Y}+\widehat{e}$
\end_inset

.
 The total variation is 
\begin_inset Formula 
\[
Y'M_{\iota}Y=\left(\widehat{Y}+\widehat{e}\right)'M_{\iota}\left(\widehat{Y}+\widehat{e}\right)=\widehat{Y}'M_{\iota}\widehat{Y}+2\widehat{Y}'M_{\iota}\widehat{e}+\widehat{e}'M_{\iota}\widehat{e}=\widehat{Y}'M_{\iota}\widehat{Y}+\widehat{e}'\widehat{e}
\]

\end_inset

where the last equality follows by 
\begin_inset Formula $M_{\iota}\widehat{e}=\widehat{e}$
\end_inset

 as 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

 (This property fails if the regression has no constant), and 
\begin_inset Formula $\widehat{Y}'\widehat{e}=Y'P_{X}M_{X}e=0$
\end_inset

.
 
\end_layout

\end_inset

The magnitude of R-squared varies in different contexts.
 In macro models with the lagged dependent variables, it is not unusually
 to observe R-squared larger than 90%.
 In cross sectional regressions it is often below 20%.
\end_layout

\begin_layout Exercise
Consider a short regression 
\begin_inset Quotes eld
\end_inset

regress 
\begin_inset Formula $y_{i}$
\end_inset

 on 
\begin_inset Formula $x_{1i}$
\end_inset


\begin_inset Quotes erd
\end_inset

 and a long regression 
\begin_inset Quotes eld
\end_inset

regress 
\begin_inset Formula $y_{i}$
\end_inset

 on 
\begin_inset Formula $\left(x_{1i},x_{2i}\right)$
\end_inset


\begin_inset Quotes erd
\end_inset

.
 Given the same dataset 
\begin_inset Formula $\left(Y,X_{1},X_{2}\right)$
\end_inset

, show that the R-squared from the short regression is no larger than that
 from the long regression.
 In other words, we can always (weakly) increase 
\begin_inset Formula $R^{2}$
\end_inset

 by adding more regressors.
\end_layout

\begin_layout Standard
Conventionally we consider the regressions when the number of regressors
 
\begin_inset Formula $K$
\end_inset

 is much smaller the sample size 
\begin_inset Formula $n$
\end_inset

.
 In the era of big data, it can happen that we have more potential regressors
 than the sample size.
\end_layout

\begin_layout Exercise
Show 
\begin_inset Formula $R^{2}=1$
\end_inset

 when 
\begin_inset Formula $K\geq n$
\end_inset

.
 (When 
\begin_inset Formula $K>n$
\end_inset

, the matrix 
\begin_inset Formula $X'X$
\end_inset

 must be rank deficient.
 We can generalize the definition OLS fitting as any vector that minimizes
 
\begin_inset Formula $\left\Vert Y-Xb\right\Vert ^{2}$
\end_inset

 though the minimizer is not unique.
\end_layout

\begin_layout Exercise
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

n = 5; K = 6; 
\end_layout

\begin_layout Plain Layout

Y = rnorm(n)
\end_layout

\begin_layout Plain Layout

X = matrix( rnorm(n*K), n)
\end_layout

\begin_layout Plain Layout

summary( lm(Y~X) )
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
With a new dataset 
\begin_inset Formula $\left(Y^{\mathrm{new}},X^{\mathrm{new}}\right)$
\end_inset

, the 
\emph on
out-of-sample
\emph default
 (OOS) R-squared is 
\begin_inset Formula 
\[
OOS\ R^{2}=\frac{\widehat{\beta}^{\prime}X^{\mathrm{new}\prime}M_{\iota}X^{\mathrm{new}}\widehat{\beta}}{Y^{\mathrm{new}\prime}M_{\iota}Y^{\mathrm{new}}}.
\]

\end_inset

OOS R-squred measures the goodness of fit in a new dataset given the coefficient
 estimated from the original data.
 In financial market shorter-term predictive models, a person may become
 a billion if he can systematically achieve 2% OOS R-squared.
\end_layout

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
The linear algebraic properties holds in finite sample no matter the data
 are taken as fixed numbers or random variables.
 The Gauss Markov theorem holds under two crucial assumptions: linear CEF
 and homoskedasticity.
 
\end_layout

\begin_layout Standard

\series bold
Historical notes
\series default
: Carl Friedrich Gauss (1777–1855) claimed he had come up with the operation
 of OLS in 1795.
 With only three data points at hand, Gauss successfully applied his method
 to predict the location of the dwarf planet Ceres in 1801.
 While Gauss did not publish the work on OLS until 1809, Adrien-Marie Legendre
 (1752–1833) presented this method in 1805.
 Today people tend to attribute OLS to Gauss, assuming that a giant like
 Gauss had no need to tell a lie to steal Legendre's discovery.
\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $A$
\end_inset

 be any 
\begin_inset Formula $n\times K$
\end_inset

 generic real matrix.
 
\emph on
Singular value decomposition
\emph default
 (SVD) factorizes 
\begin_inset Formula $A=USV'$
\end_inset

, where 
\begin_inset Formula $U$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 real unitary matrix (A real unitary matrix is invertible and 
\begin_inset Formula $U'U=UU'=I$
\end_inset

, which implies 
\begin_inset Formula $U^{-1}=U'$
\end_inset

), 
\begin_inset Formula $S=\begin{bmatrix}S_{1}\\
0_{\left(n-K\right)\times K}
\end{bmatrix}$
\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 rectangular diagonal matrix with 
\begin_inset Formula $S_{1}$
\end_inset

 a 
\begin_inset Formula $K\times K$
\end_inset

 diagonal matrix of non-negative real elements (called 
\emph on
singular values
\emph default
), and 
\begin_inset Formula $V$
\end_inset

 is a 
\begin_inset Formula $K\times K$
\end_inset

 real unitary matrix.
\end_layout

\begin_layout Standard
We apply SVD to the projection matrix 
\begin_inset Formula $P_{X}=X\left(X'X\right)^{-1}X$
\end_inset

, where 
\begin_inset Formula $X$
\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 data matrix with 
\begin_inset Formula $K$
\end_inset

 linearly independent columns.
 Substitute 
\begin_inset Formula $X=USV'$
\end_inset

 into 
\begin_inset Formula $P_{X}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
P_{X} & =USV'\left(VS'U'USV'\right)^{-1}VS'U'=USV'\left(VS'SV'\right)^{-1}VS'U'\\
 & =USV'V^{\prime-1}\left(S'S\right)^{-1}V^{-1}VS'U'=US\left(S'S\right)^{-1}S'U'\\
 & =U\begin{bmatrix}S_{1}\\
0
\end{bmatrix}S_{1}^{-1}S_{1}^{-1}\begin{bmatrix}S_{1} & 0\end{bmatrix}U'=U\begin{bmatrix}I_{K} & 0_{K\times(n-K)}\\
0_{(n-K)\times K} & 0_{(n-K)\times(n-K)}
\end{bmatrix}U'\\
 & =U\,\mathrm{diag}\left(\iota_{K},0_{n-K}\right)U'.
\end{align*}

\end_inset

 All real symmetric matrices are diagonalizable, and the the last expression
 is the diagonalization of 
\begin_inset Formula $P_{X}$
\end_inset

.
 The projection matrix 
\begin_inset Formula $P_{X}$
\end_inset

 has 
\begin_inset Formula $K$
\end_inset

 repeated eigenvalues of 1 and 
\begin_inset Formula $\left(n-K\right)$
\end_inset

 repeated eigenvalues of 0, and obviously 
\begin_inset Formula $\mathrm{rank}(P_{X})=K$
\end_inset

.
 
\end_layout

\begin_layout Standard
Two generic square matrices 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are 
\emph on
similar
\emph default
 if there exists an invertible matrix 
\begin_inset Formula $Q$
\end_inset

 such that 
\begin_inset Formula $A=Q^{-1}BQ$
\end_inset

.
 By this definition, 
\begin_inset Formula $P_{X}$
\end_inset

 is similar to the diagonal matrix 
\begin_inset Formula $\mathrm{diag}\left(\iota_{K},0_{n-K}\right)$
\end_inset

, and 
\begin_inset Formula $M_{X}=I_{n}-P_{X}$
\end_inset

 is similar to 
\begin_inset Formula $\mathrm{diag}\left(0_{K},\iota_{n-K}\right)$
\end_inset

 because 
\begin_inset Formula 
\begin{align*}
U'\,M_{X}U & =U'\,\left(I_{n}-P_{X}\right)U=U'U-U'\,P_{X}U\\
 & =I_{n}-\mathrm{diag}\left(\iota_{K},0_{n-K}\right)=\mathrm{diag}\left(0_{K},\iota_{n-K}\right).
\end{align*}

\end_inset

It implies that 
\begin_inset Formula $\mathrm{rank}(M_{X})=n-K$
\end_inset

.
\end_layout

\begin_layout Standard
Both 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $M_{X}$
\end_inset

 are symmetric idempotent matrices.
 For a general idempotent matrices 
\begin_inset Formula $C$
\end_inset

 which does not have to be symmetric,
\end_layout

\begin_layout Itemize
\begin_inset Formula $C$
\end_inset

 is diagonalizable (See 
\begin_inset CommandInset citation
LatexCommand cite
after "p.148"
key "horn2012matrix"
literal "false"

\end_inset

).
\end_layout

\begin_layout Standard
This fact immediately implies that
\end_layout

\begin_layout Itemize
All eigenvalues of 
\begin_inset Formula $C$
\end_inset

 are either 0 and 1;
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathrm{rank}\left(C\right)=\mathrm{trace}\left(C\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 Oct 4.}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ref_teaching"
options "chicagoa"

\end_inset


\end_layout

\end_body
\end_document
