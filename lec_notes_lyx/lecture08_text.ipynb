{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "Notation: $\\mathbf{X}$ denotes a random variable or random vector.\n",
    "$\\mathbf{x}$ is its realization.\n",
    "\n",
    "A *hypothesis* is a statement about the parameter space $\\Theta$.\n",
    "Hypothesis testing checks whether the data support a *null hypothesis*\n",
    "$\\Theta_{0}$, which is a subset of $\\Theta$ of interest. Ideally the\n",
    "null hypothesis should be suggested by scientific theory. The\n",
    "*alternative hypothesis* $\\Theta_{1}=\\Theta\\backslash\\Theta_{0}$ is the\n",
    "complement of $\\Theta_{0}$. Based on the observed evidence, hypothesis\n",
    "testing decides to accept or reject the null hypothesis. If the null\n",
    "hypothesis is rejected by the data, it implies that from the statistical\n",
    "perspective the data is incompatible with the proposed scientific\n",
    "theory.\n",
    "\n",
    "In this chapter, we will first introduce the idea and practice of\n",
    "hypothesis testing and the related confidence interval. While we mainly\n",
    "focus on the frequentist interpretation of hypothesis, we briefly\n",
    "discuss the Bayesian approach to statistical decision. As an application\n",
    "of the testing procedures to the linear regression model, we elaborate\n",
    "how to test a linear or nonlinear hypothesis of the slope coefficients\n",
    "based on the unrestricted or restricted OLS estimators.\n",
    "\n",
    "## Testing\n",
    "\n",
    "### Decision Rule and Errors\n",
    "\n",
    "If $\\Theta_{0}$ is a singleton, we call it a *simple hypothesis*;\n",
    "otherwise we call it a *composite hypothesis*. For example, if the\n",
    "parameter space $\\Theta=\\mathbb{R}$, then $\\Theta_{0}=\\left\\{ 0\\right\\}$\n",
    "(or equivalently $\\theta_{0}=0$) is a simple hypothesis, whereas\n",
    "$\\Theta_{0}=(-\\infty,0]$ (or equivalently $\\theta_{0}\\leq0$) is a\n",
    "composite hypothesis.\n",
    "\n",
    "A *test function* is a mapping\n",
    "$$\\phi:\\mathcal{X}^{n}\\mapsto\\left\\{ 0,1\\right\\} ,$$ where $\\mathcal{X}$\n",
    "is the sample space. The null hypothesis is accepted if\n",
    "$\\phi\\left(\\mathbf{x}\\right)=0$, or rejected if\n",
    "$\\phi\\left(\\mathbf{x}\\right)=1$. We call the set\n",
    "$A_{\\phi}=\\left\\{ \\mathbf{x}\\in\\mathcal{X}^{n}:\\phi_{\\theta}\\left(\\mathbf{x}\\right)=0\\right\\}$\n",
    "the *acceptance region*, and its complement\n",
    "$R_{\\phi}=\\left\\{ \\mathbf{x}\\in\\mathcal{X}^{n}:\\phi\\left(\\mathbf{x}\\right)=1\\right\\}$\n",
    "the *rejection region.*\n",
    "\n",
    "The *power function* of a test $\\phi$ is\n",
    "$$\\beta\\left(\\theta\\right)=P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=1\\right\\} =E_{\\theta}\\left[\\phi\\left(\\mathbf{X}\\right)\\right].$$\n",
    "The power function measures the probability that the test function\n",
    "rejects the null when the data is generated under the true parameter\n",
    "$\\theta$, reflected in $P_{\\theta}$ and $E_{\\theta}$.\n",
    "\n",
    "The *power* of a test for some $\\theta\\in\\Theta_{1}$ is the value of\n",
    "$\\beta\\left(\\theta\\right)$. The *size* of the test is\n",
    "$\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right).$ Notice that the\n",
    "definition of power depends on a $\\theta$ in the alternative hypothesis\n",
    "$\\Theta_{1}$, whereas that of size is independent of $\\theta$ due to the\n",
    "supremum over the set of null $\\Theta_{0}$. The *level* of a test is any\n",
    "value $\\alpha\\in\\left(0,1\\right)$ such that\n",
    "$\\alpha\\geq\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right)$, which is\n",
    "often used when it is difficult to attain the exact supremum. A test of\n",
    "size $\\alpha$ is also of level $\\alpha$ or bigger; while a test of level\n",
    "$\\alpha$ must have size smaller or equal to $\\alpha$.\n",
    "\n",
    "The concept of *level* is useful if we do not have sufficient\n",
    "information to derive the exact size of a test. If\n",
    "$\\left(X_{1i},X_{2i}\\right)_{i=1}^{n}$ are randomly drawn from some\n",
    "unknown joint distribution, but we know the marginal distribution is\n",
    "$X_{ji}\\sim N\\left(\\theta_{j},1\\right)$, for $j=1,2$. In order to test\n",
    "the joint hypothesis $\\theta_{1}=\\theta_{2}=0$, we can construct a test\n",
    "function\n",
    "$$\\phi_{\\theta_{1}=\\theta_{2}=0}\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)=1\\left\\{ \\left\\{ \\sqrt{n}\\left|\\overline{X}_{1}\\right|\\geq z_{1-\\alpha/4}\\right\\} \\cup\\left\\{ \\sqrt{n}\\left|\\overline{X}_{2}\\right|\\geq z_{1-\\alpha/4}\\right\\} \\right\\} ,$$\n",
    "where $z_{1-\\alpha/4}$ is the $\\left(1-\\alpha/4\\right)$-th quantile of\n",
    "the standard normal distribution. The level of this test is\n",
    "$$\\begin{aligned}P\\left(\\phi_{\\theta_{1}=\\theta_{2}=0}\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)\\right) & \\leq P\\left(\\sqrt{n}\\left|\\overline{X}_{1}\\right|\\geq z_{1-\\alpha/4}\\right)+P\\left(\\sqrt{n}\\left|\\overline{X}_{2}\\right|\\geq z_{1-\\alpha/4}\\right)\\\\\n",
    " & =\\alpha/2+\\alpha/2=\\alpha.\n",
    "\\end{aligned}$$ where the inequality follows by the *Bonferroni\n",
    "inequality*\n",
    "$$P\\left(A\\cup B\\right)\\leq P\\left(A\\right)+P\\left(B\\right).$$ (The\n",
    "seemingly trivial Bonferroni inequality is useful in many proofs of\n",
    "probability results.) Therefore, the level of\n",
    "$\\phi\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)$ is $\\alpha$, but the\n",
    "exact size is unknown without the knowledge of the joint distribution.\n",
    "(Even if we know the correlation of $X_{1i}$ and $X_{2i}$, putting two\n",
    "marginally normal distributions together does not make a jointly normal\n",
    "vector in general.)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "                       accept $H_{0}$     reject $H_{0}$\n",
    "      $H_{0}$ true    correct decision     Type I error\n",
    "      $H_{0}$ false    Type II error     correct decision\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    ": <span id=\"tab:Decisions-and-States\"\n",
    "label=\"tab:Decisions-and-States\">\\[tab:Decisions-and-States\\]</span>\n",
    "Actions, States and Consequences\n",
    "\n",
    "-   The *probability of committing Type I error* is\n",
    "    $\\beta\\left(\\theta\\right)$ for some $\\theta\\in\\Theta_{0}$.\n",
    "\n",
    "-   The *probability of committing Type II error* is\n",
    "    $1-\\beta\\left(\\theta\\right)$ for some $\\theta\\in\\Theta_{1}$.\n",
    "\n",
    "The philosophy on hypothesis testing has been debated for centuries. At\n",
    "present the prevailing framework in statistics textbooks is the\n",
    "*frequentist perspective*. A frequentist views the parameter as a fixed\n",
    "constant. They keep a conservative attitude about the Type I error: Only\n",
    "if overwhelming evidence is demonstrated shall a researcher reject the\n",
    "null. Under the principle of protecting the null hypothesis, a desirable\n",
    "test should have a small level. Conventionally we take $\\alpha=0.01,$\n",
    "0.05 or 0.1. We say a test is *unbiased* if\n",
    "$\\beta\\left(\\theta\\right)>\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right)$\n",
    "for all $\\theta\\in\\Theta_{1}$. There can be many tests of correct size.\n",
    "\n",
    "A trivial test function\n",
    "$\\phi(\\mathbf{x})=1\\left\\{ 0\\leq U\\leq\\alpha\\right\\}$ for all\n",
    "$\\theta\\in\\Theta$, where $U$ is a random variable from a uniform\n",
    "distribution on $\\left[0,1\\right]$, has correct size $\\alpha$ but no\n",
    "non-trivial power at the alternative. On the other extreme, the trivial\n",
    "test function $\\phi\\left(\\mathbf{x}\\right)=1$ for all $\\mathbf{x}$\n",
    "enjoys the biggest power but suffers incorrect size.\n",
    "\n",
    "Usually, we design a test by proposing a test statistic\n",
    "$T_{n}:\\mathcal{X}^{n}\\mapsto\\mathbb{R}^{+}$ and a critical value\n",
    "$c_{1-\\alpha}$. Given $T_{n}$ and $c_{1-\\alpha}$, we write the test\n",
    "function as\n",
    "$$\\phi\\left(\\mathbf{X}\\right)=1\\left\\{ T_{n}\\left(\\mathbf{X}\\right)>c_{1-\\alpha}\\right\\} .$$\n",
    "To ensure such a $\\phi\\left(\\mathbf{x}\\right)$ has correct size, we need\n",
    "to figure out the distribution of $T_{n}$ under the null hypothesis\n",
    "(called the *null distribution*), and choose a critical value\n",
    "$c_{1-\\alpha}$ according to the null distribution and the desirable size\n",
    "or level $\\alpha$.\n",
    "\n",
    "Another commonly used indicator in hypothesis testing is $p$-value:\n",
    "$$\\sup_{\\theta\\in\\Theta_{0}}P_{\\theta}\\left\\{ T_{n}\\left(\\mathbf{x}\\right)\\leq T_{n}\\left(\\mathbf{X}\\right)\\right\\} .$$\n",
    "In the above expression, $T_{n}\\left(\\mathbf{x}\\right)$ is the realized\n",
    "value of the test statistic $T_{n}$, while\n",
    "$T_{n}\\left(\\mathbf{X}\\right)$ is the random variable generated by\n",
    "$\\mathbf{X}$ under the null $\\theta\\in\\Theta_{0}$. The interpretation of\n",
    "the $p$-value is tricky. $p$-value is the probability that we observe\n",
    "$T_{n}(\\mathbf{X})$ being greater than the realized $T_{n}(\\mathbf{x})$\n",
    "if the null hypothesis is true.\n",
    "\n",
    "$p$-value is *not* the probability that the null hypothesis is true.\n",
    "Under the frequentist perspective, the null hypothesis is either true or\n",
    "false, with certainty. The randomness of a test comes only from\n",
    "sampling, not from the hypothesis. $p$-value measures whether the\n",
    "dataset is compatible with the null hypothesis. $p$-value is closely\n",
    "related to the corresponding test. When $p$-value is smaller than the\n",
    "specified test size $\\alpha$, the test rejects the null.\n",
    "\n",
    "So far we have been talking about hypothesis testing in finite sample.\n",
    "The discussion and terminologies can be carried over to the asymptotic\n",
    "world when $n\\to\\infty$. If we denote the power function as\n",
    "$\\beta_{n}\\left(\\theta\\right)$, in which we make its dependence on the\n",
    "sample size $n$ explicit, the test is of asymptotic size $\\alpha$ if\n",
    "$\\limsup_{n\\to\\infty}\\beta_{n}\\left(\\theta\\right)\\leq\\alpha$ for all\n",
    "$\\theta\\in\\Theta_{0}$. A test is *consistent* if\n",
    "$\\beta_{n}\\left(\\theta\\right)\\to1$ for every $\\theta\\in\\Theta_{1}$.\n",
    "\n",
    "### Optimality\n",
    "\n",
    "Just as there may be multiple valid estimators for a task of estimation,\n",
    "there may be multiple tests for a task of hypothesis testing. For a\n",
    "class of tests of the same level $\\alpha$ under the null\n",
    "$\\Psi_{\\alpha}=\\left\\{ \\phi:\\sup_{\\theta\\in\\Theta_{0}}\\beta_{\\phi}\\left(\\theta\\right)\\leq\\alpha\\right\\}$\n",
    "where we put a subscript $\\phi$ in $\\beta_{\\phi}\\left(\\theta\\right)$ to\n",
    "distinguish the power for different tests, it is natural to prefer a\n",
    "test $\\phi^{*}$ that exhibits higher power than all other tests under\n",
    "consideration at each point of the alternative hypothesis in that\n",
    "$$\\beta_{\\phi^{*}}\\left(\\theta\\right)\\geq\\beta_{\\phi}\\left(\\theta\\right)$$\n",
    "for every $\\theta\\in\\Theta_{1}$ and every $\\phi\\in\\Psi_{\\alpha}$. If\n",
    "such a test $\\phi^{*}\\in\\Psi_{\\alpha}$ exists, we call it the *uniformly\n",
    "most powerful test.*\n",
    "\n",
    "Suppose a random sample of size 6 is generated from\n",
    "$$\\left(X_{1},\\ldots,X_{6}\\right)\\sim\\text{iid.}N\\left(\\theta,1\\right),$$\n",
    "where $\\theta$ is unknown. We want to infer the population mean of the\n",
    "normal distribution. The null hypothesis is $H_{0}$: $\\theta\\leq0$ and\n",
    "the alternative is $H_{1}$: $\\theta>0$. All tests in\n",
    "$$\\Psi=\\left\\{ 1\\left\\{ \\bar{X}\\geq c/\\sqrt{6}\\right\\} :c\\geq1.64\\right\\}$$\n",
    "has the correct level. Since $\\bar{X}=N\\left(\\theta,1/6\\right)$, the\n",
    "power function for those in $\\Psi$ is $$\\begin{aligned}\n",
    "\\beta_{\\phi}\\left(\\theta\\right) & =P\\left(\\bar{X}\\geq\\frac{c}{\\sqrt{6}}\\right)=P\\left(\\frac{\\bar{X}-\\theta}{1/\\sqrt{6}}\\geq\\frac{\\frac{c}{\\sqrt{6}}-\\theta}{1/\\sqrt{6}}\\right)\\\\\n",
    " & =P\\left(N\\geq c-\\sqrt{6}\\theta\\right)=1-\\Phi\\left(c-\\sqrt{6}\\theta\\right)\\end{aligned}$$\n",
    "where $N=\\frac{\\bar{X}-\\theta}{1/\\sqrt{6}}$ follows the standard normal,\n",
    "and $\\Phi$ is the cdf of the standard normal. It is clear that\n",
    "$\\beta_{\\phi}\\left(\\theta\\right)$ is monotonically decreasing in $c$.\n",
    "Thus the test function\n",
    "$$\\phi_{\\theta=0}\\left(\\mathbf{X}\\right)=1\\left\\{ \\bar{X}\\geq1.64/\\sqrt{6}\\right\\}$$\n",
    "is the most powerful test in $\\Psi$, as $c=1.64$ is the lower bound that\n",
    "$\\Psi_{\\alpha}$ allows in order to keep the level $\\alpha$.\n",
    "\n",
    "### Likelihood-Ratio Test and Wilks’ theorem\n",
    "\n",
    "When estimators are not available in closed-forms, the likelihood-ratio\n",
    "test (LRT) serves as a very general testing statistic under the\n",
    "likelihood principle. Let\n",
    "$\\ell_{n}\\left(\\theta\\right)=n^{-1}\\sum_{i}\\log f\\left(x_{i};\\theta\\right)$\n",
    "be the average sample log-likelihood, and\n",
    "$\\widehat{\\theta}=\\arg\\max_{\\theta\\in\\Theta}\\ell_{n}\\left(\\theta\\right)$\n",
    "is the maximum likelihood estimator (MLE). Take a Taylor expansion of\n",
    "$\\ell_{n}\\left(\\theta_{0}\\right)$ around\n",
    "$\\ell_{n}\\left(\\widehat{\\theta}\\right)$: $$\\begin{aligned}\n",
    "\\ell_{n}\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\widehat{\\theta}\\right) & =\\frac{\\partial\\ell_{n}}{\\partial\\theta}\\left(\\widehat{\\theta}\\right)'\\left(\\theta_{0}-\\widehat{\\theta}\\right)+\\frac{1}{2}\\left(\\theta_{0}-\\widehat{\\theta}\\right)'\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\theta_{0}-\\widehat{\\theta}\\right)+O\\left(\\left\\Vert \\widehat{\\theta}-\\theta_{0}\\right\\Vert _{2}^{3}\\right)\\\\\n",
    " & =\\frac{1}{2}\\left(\\widehat{\\theta}-\\theta_{0}\\right)'\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right)+O\\left(\\left\\Vert \\widehat{\\theta}-\\theta_{0}\\right\\Vert _{2}^{3}\\right)\\\\\n",
    " & =\\frac{1}{2}\\left(\\widehat{\\theta}-\\theta_{0}\\right)'\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right)+o_{p}\\left(1\\right)\\end{aligned}$$\n",
    "by that\n",
    "$\\frac{\\partial\\ell_{n}}{\\partial\\theta}\\left(\\widehat{\\theta}\\right)=0$\n",
    "due to the first order condition of optimality. Define\n",
    "$L_{n}\\left(\\theta\\right):=\\sum_{i}\\log f\\left(x_{i};\\theta\\right)$, and\n",
    "the *likelihood-ratio statistic* as\n",
    "$$\\mathcal{LR}:=2\\left(L_{n}\\left(\\widehat{\\theta}\\right)-L_{n}\\left(\\theta_{0}\\right)\\right)=2n\\left(\\ell_{n}\\left(\\widehat{\\theta}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)\\right).$$\n",
    "Obviously $\\mathcal{LR}\\geq0$ because $\\widehat{\\theta}$ maximizes\n",
    "$\\ell_{n}\\left(\\theta\\right)$. Multiply $-2n$ to the two sides of the\n",
    "above Taylor expansion:\n",
    "$$\\mathcal{LR}=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)'\\left(-\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\dot{\\theta}\\right)\\right)\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)+o_{p}\\left(1\\right)$$\n",
    "Notice that when the model is correctly specified we have\n",
    "$$\\begin{aligned}\n",
    "-\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\theta_{0}\\right) & \\stackrel{p}{\\to}-\\mathcal{H}\\left(\\theta_{0}\\right)=\\mathcal{I}\\left(\\theta_{0}\\right)\\\\\n",
    "\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) & \\stackrel{d}{\\to}N\\left(0,\\mathcal{I}^{-1}\\left(\\theta_{0}\\right)\\right)\\end{aligned}$$\n",
    "By Slutsky’s theorem:\n",
    "$$\\left(-\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\dot{\\theta}\\right)\\right)^{1/2}\\left[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\right]\\stackrel{d}{\\to}\\mathcal{I}^{1/2}\\left(\\theta_{0}\\right)\\times N\\left(0,\\mathcal{I}^{-1}\\left(\\theta_{0}\\right)\\right)\\sim N\\left(0,I_{k}\\right).$$\n",
    "and then $\\mathcal{LR}\\stackrel{d}{\\to}\\chi_{K}^{2}$ by the continuous\n",
    "mapping theorem.\n",
    "\n",
    "*Wilks’ theorem*, or *Wilks’ phenomenon* is referred to the fact that\n",
    "$\\mathcal{LR}\\stackrel{d}{\\to}\\chi^{2}\\left(K\\right)$ when the\n",
    "parametric model is correctly specified.\n",
    "\n",
    "### Score Test\n",
    "\n",
    "## Confidence Interval<span id=\"confidence-interval\" label=\"confidence-interval\">\\[confidence-interval\\]</span>\n",
    "\n",
    "An *interval estimate* is a function\n",
    "$C:\\mathcal{X}^{n}\\mapsto\\left\\{ \\Theta_{1}:\\Theta_{1}\\subseteq\\Theta\\right\\}$\n",
    "that maps a point in the sample space to a subset of the parameter\n",
    "space. The *coverage probability* of an *interval estimator*\n",
    "$C\\left(\\mathbf{X}\\right)$ is defined as\n",
    "$P_{\\theta}\\left(\\theta\\in C\\left(\\mathbf{X}\\right)\\right)$. When\n",
    "$\\theta$ is of one dimension, we usually call the interval estimator\n",
    "*confidence interval*. When $\\theta$ is of multiple dimensions, we call\n",
    "the it *confidence region* and it of course includes the one-dimensional\n",
    "$\\theta$ as a special case. The coverage probability is the frequency\n",
    "that the interval estimator captures the true parameter that generates\n",
    "the sample. From the frequentist perspective, the parameter is fixed\n",
    "while the confidence region is random. It is *not* the probability that\n",
    "$\\theta$ is inside the given confidence interval.\n",
    "\n",
    "Suppose a random sample of size 6 is generated from\n",
    "$\\left(X_{1},\\ldots,X_{6}\\right)\\sim\\text{iid }N\\left(\\theta,1\\right).$\n",
    "Find the coverage probability of the random interval is\n",
    "$\\left[\\bar{X}-1.96/\\sqrt{6},\\ \\bar{X}+1.96/\\sqrt{6}\\right].$\n",
    "\n",
    "Hypothesis testing and confidence region are closely related. Sometimes\n",
    "it is difficult to directly construct the confidence region, but easy to\n",
    "test a hypothesis. One way to construct confidence region is by\n",
    "*inverting a test*. Suppose $\\phi_{\\theta}$ is a test of size $\\alpha$.\n",
    "If $C\\left(\\mathbf{X}\\right)$ is constructed as\n",
    "$$C\\left(\\mathbf{X}\\right)=\\left\\{ \\theta\\in\\Theta:\\phi\\left(\\mathbf{X}\\right)=0\\right\\} .$$\n",
    "The coverage probability of the true data generating parameter $\\theta$\n",
    "is\n",
    "$$P_{\\theta}\\left\\{ \\theta\\in C\\left(\\mathbf{X}\\right)\\right\\} =P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=0\\right\\} =1-P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=1\\right\\} =1-\\beta\\left(\\theta\\right)\\geq1-\\alpha$$\n",
    "where the last inequality follows as\n",
    "$\\beta\\left(\\theta\\right)\\leq\\alpha$ for $\\theta\\in\\Theta_{0}$. If\n",
    "$\\Theta_{0}$ is a singleton, the equality holds.\n",
    "\n",
    "**knitr**\n",
    "\n",
    "## Bayesian Credible Set\n",
    "\n",
    "The Bayesian framework offers a coherent and natural language for\n",
    "statistical decision. However, the major criticism against Bayesian\n",
    "statistics is the arbitrariness of the choice of the prior.\n",
    "\n",
    "The Bayesian approach views both the data $\\mathbf{X}_{n}$ and the\n",
    "parameter $\\theta$ as random variables. Before she observes the data,\n",
    "she holds a *prior distribution* $\\pi$ about $\\theta$. After observing\n",
    "the data, she updates the prior distribution to a *posterior\n",
    "distribution* $p(\\theta|\\mathbf{X}_{n})$. The *Bayes Theorem* connects\n",
    "the prior and the posterior as\n",
    "$$p(\\theta|\\mathbf{X}_{n})\\propto f(\\mathbf{X}_{n}|\\theta)\\pi(\\theta)$$\n",
    "where $f(\\mathbf{X}_{n}|\\theta)$ is the likelihood function.\n",
    "\n",
    "Here is a classical example to illustrate the Bayesian approach to\n",
    "statistical inference. Suppose $\\mathbf{X}_{n}=(X_{1},\\ldots,X_{n})$ is\n",
    "an iid sample drawn from a normal distribution with unknown $\\theta$ and\n",
    "known $\\sigma$. If a researcher’s prior distribution\n",
    "$\\theta\\sim N(\\theta_{0},\\sigma_{0}^{2})$, her posterior distribution\n",
    "is, by some routine calculation, also a normal distribution\n",
    "$$p(\\theta|\\mathbf{x}_{n})\\sim N\\left(\\tilde{\\theta},\\tilde{\\sigma}^{2}\\right),$$\n",
    "where\n",
    "$\\tilde{\\theta}=\\frac{\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\theta_{0}+\\frac{n\\sigma_{0}^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\bar{x}$\n",
    "and\n",
    "$\\tilde{\\sigma}^{2}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}$.\n",
    "Thus the Bayesian credible set is\n",
    "$$\\left(\\tilde{\\theta}-z_{1-\\alpha/2}\\cdot\\tilde{\\sigma},\\ \\tilde{\\theta}+z_{1-\\alpha/2}\\cdot\\tilde{\\sigma}\\right).$$\n",
    "This posterior distribution depends on $\\theta_{0}$ and $\\sigma_{0}^{2}$\n",
    "from the prior. When the sample size is sufficiently large the posterior\n",
    "can be approximated by $N(\\bar{x},\\sigma^{2}/n)$, where the prior\n",
    "information is overwhelmed by the information accumulated from the data.\n",
    "\n",
    "In contrast, a frequentist will estimate\n",
    "$\\hat{\\theta}=\\bar{x}\\sim N(\\theta,\\sigma^{2}/n)$. Her confidence\n",
    "interval is\n",
    "$$\\left(\\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n},\\ \\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n}\\right).$$\n",
    "The Bayesian credible set and the frequentist confidence interval are\n",
    "different for finite $n$, but they coincide when $n\\to\\infty$.\n",
    "\n",
    "## Applications in OLS\n",
    "\n",
    "We will introduce three tests for a hypothesis of the linear regression\n",
    "coefficients, namely the Wald test, the Lagrangian multiplier (LM) test,\n",
    "and the likelihood ratio test. The Wald test is based on the\n",
    "unrestricted OLS estimator $\\widehat{\\beta}$. The LM test is based on\n",
    "the restricted estimator $\\tilde{\\beta}$. The LRT, as we have discussed,\n",
    "is based on the difference of the log-likelihood function evaluated at\n",
    "the unrestricted OLS estimator and that on the restricted estimator.\n",
    "\n",
    "Let $R$ be a $q\\times K$ constant matrix with $q\\leq K$ and\n",
    "$\\mbox{rank}\\left(R\\right)=q$. All linear restrictions about $\\beta$ can\n",
    "be written in the form of $R\\beta=r$, where $r$ is a $q\\times1$ constant\n",
    "vector.\n",
    "\n",
    "We want to simultaneously test $\\beta_{1}=1$ and $\\beta_{3}+\\beta_{4}=2$\n",
    "in the above example. The null hypothesis can be expressed in the\n",
    "general form $R\\beta=r$, where the restriction matrix $R$ is\n",
    "$$R=\\begin{pmatrix}1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 1 & 0\n",
    "\\end{pmatrix}$$ and $r=\\left(1,2\\right)'$.\n",
    "\n",
    "### Wald Test\n",
    "\n",
    "Suppose the OLS estimator $\\widehat{\\beta}$ is asymptotic normal, i.e.\n",
    "$$\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,\\Omega\\right)$$\n",
    "where $\\Omega$ is a $K\\times K$ positive definite covariance matrix and.\n",
    "Since\n",
    "$R\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,R\\Omega R'\\right)$,\n",
    "the quadratic form\n",
    "$$n\\left(\\widehat{\\beta}-\\beta\\right)'R'\\left(R\\Omega R'\\right)^{-1}R\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}\\chi_{q}^{2}.$$\n",
    "Now we intend to test the linear null hypothesis $R\\beta=r$. Under the\n",
    "null, the Wald statistic\n",
    "$$\\mathcal{W}=n\\left(R\\widehat{\\beta}-r\\right)'\\left(R\\widehat{\\Omega}R'\\right)^{-1}\\left(R\\widehat{\\beta}-r\\right)\\stackrel{d}{\\to}\\chi_{q}^{2}$$\n",
    "where $\\widehat{\\Omega}$ is a consistent estimator of $\\Omega$.\n",
    "\n",
    "(Single test) In a linear regression\n",
    "$$\\begin{aligned}y & =x_{i}'\\beta+e_{i}=\\sum_{k=1}^{5}\\beta_{k}x_{ik}+e_{i}.\\nonumber\\\\\n",
    "E\\left[e_{i}x_{i}\\right] & =\\mathbf{0}_{5},\\label{eq:example}\n",
    "\\end{aligned}$$ where $y$ is wage and\n",
    "$$x=\\left(\\mbox{edu},\\mbox{age},\\mbox{experience},\\mbox{experience}^{2},1\\right)'.$$\n",
    "To test whether *education* affects *wage*, we specify the null\n",
    "hypothesis $\\beta_{1}=0$. Let $R=\\left(1,0,0,0,0\\right)$ and $r=0$.\n",
    "$$\\sqrt{n}\\widehat{\\beta}_{1}=\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)=\\sqrt{n}R\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,R\\Omega R'\\right)\\sim N\\left(0,\\Omega_{11}\\right),\\label{eq:R11}$$\n",
    "where $\\Omega{}_{11}$ is the $\\left(1,1\\right)$ (scalar) element of\n",
    "$\\Omega$. Under\n",
    "$$H_{0}:R\\beta=\\left(1,0,0,0,0\\right)\\left(\\beta_{1},\\ldots,\\beta_{5}\\right)'=\\beta_{1}=0,$$\n",
    "we have\n",
    "$\\sqrt{n}R\\left(\\widehat{\\beta}-\\beta\\right)=\\sqrt{n}\\widehat{\\beta}_{1}\\stackrel{d}{\\to}N\\left(0,\\Omega_{11}\\right).$\n",
    "Therefore,\n",
    "$$\\sqrt{n}\\frac{\\widehat{\\beta}_{1}}{\\widehat{\\Omega}_{11}^{1/2}}=\\sqrt{\\frac{\\Omega_{11}}{\\widehat{\\Omega}_{11}}}\\sqrt{n}\\frac{\\widehat{\\beta}_{1}}{\\sqrt{\\Omega_{11}}}$$\n",
    "If $\\widehat{\\Omega}\\stackrel{p}{\\to}\\Omega$, then\n",
    "$\\left(\\Omega_{11}/\\widehat{\\Omega}_{11}\\right)^{1/2}\\stackrel{p}{\\to}1$\n",
    "by the continuous mapping theorem. As\n",
    "$\\sqrt{n}\\widehat{\\beta}_{1}/\\Omega_{11}^{1/2}\\stackrel{d}{\\to}N\\left(0,1\\right)$,\n",
    "we conclude\n",
    "$\\sqrt{n}\\widehat{\\beta}_{1}/\\widehat{\\Omega}_{11}^{1/2}\\stackrel{d}{\\to}N\\left(0,1\\right).$\n",
    "\n",
    "The above example is a test about a single coefficient, and the test\n",
    "statistic is essentially the square of the *t*-statistic, and the null\n",
    "distribution is the square of a standard normal.\n",
    "\n",
    "In order to test a nonlinear regression, we use the delta method.\n",
    "\n",
    "*(This is not a good example because it can be rewritten into a linear\n",
    "hypothesis.)* In the example of linear regression, the optimal\n",
    "experience level can be found by setting to zero the first order\n",
    "condition with respective to experience,\n",
    "$\\beta_{3}+2\\beta_{4}\\mbox{experience}^{*}=0$. We test the hypothesis\n",
    "that the optimal experience level is 20 years; in other words,\n",
    "$$\\mbox{experience}^{*}=-\\frac{\\beta_{3}}{2\\beta_{4}}=20.$$ This is a\n",
    "nonlinear hypothesis. If $q\\leq K$ where $q$ is the number of\n",
    "restrictions, we have\n",
    "$$n\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)'\\left(\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)\\Omega\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)'\\right)^{-1}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\to}\\chi_{q}^{2},$$\n",
    "where in this example, $\\theta=\\beta$,\n",
    "$f\\left(\\beta\\right)=-\\beta_{3}/\\left(2\\beta_{4}\\right)$. The gradient\n",
    "$$\\frac{\\partial f}{\\partial\\beta'}\\left(\\beta\\right)=\\left(0,0,-\\frac{1}{2\\beta_{4}},\\frac{\\beta_{3}}{2\\beta_{4}^{2}},0\\right)$$\n",
    "Since $\\widehat{\\beta}\\stackrel{p}{\\to}\\beta_{0}$, by the continuous\n",
    "mapping theorem, if $\\beta_{0,4}\\neq0$, we have\n",
    "$\\frac{\\partial}{\\partial\\beta}f\\left(\\widehat{\\beta}\\right)\\stackrel{p}{\\to}\\frac{\\partial}{\\partial\\beta}f\\left(\\beta_{0}\\right)$.\n",
    "Therefore, the (nonlinear) Wald test is\n",
    "$$\\mathcal{W}=n\\left(f\\left(\\widehat{\\beta}\\right)-20\\right)'\\left(\\frac{\\partial f}{\\partial\\beta'}\\left(\\widehat{\\beta}\\right)\\widehat{\\Omega}\\frac{\\partial f}{\\partial\\beta'}\\left(\\widehat{\\beta}\\right)\\right)^{-1}\\left(f\\left(\\widehat{\\beta}\\right)-20\\right)\\stackrel{d}{\\to}\\chi_{1}^{2}.$$\n",
    "This is a valid test with correct asymptotic size.\n",
    "\n",
    "However, we can equivalently state the null hypothesis as\n",
    "$\\beta_{3}+40\\beta_{4}=0$ and we can construct a Wald statistic\n",
    "accordingly. Asymptotically equivalent though, in general a linear\n",
    "hypothesis is preferred to a nonlinear one, due to the approximation\n",
    "error in the delta method under the null and more importantly the\n",
    "invalidity of the Taylor expansion under the alternative. It also\n",
    "highlights the problem of Wald test being *variant* to\n",
    "re-parametrization.\n",
    "\n",
    "### Lagrangian Multiplier Test\n",
    "\n",
    "The key difference between the Wald test and LM test is that the former\n",
    "is based on the unrestricted OLS estimator while the latter is based on\n",
    "the restricted OLS estimator. Estimate the constrained OLS estimator\n",
    "$$\\min_{\\beta}\\left(y-X\\beta\\right)'\\left(y-X\\beta\\right)\\mbox{ s.t. }R\\beta=r.$$\n",
    "We know that the restricted minimization problem can be converted into\n",
    "an unrestricted problem\n",
    "$$L\\left(\\beta,\\lambda\\right)=\\frac{1}{2n}\\left(y-X\\beta\\right)'\\left(y-X\\beta\\right)+\\lambda'\\left(R\\beta-r\\right),\\label{eq:Lagran}$$\n",
    "where $L\\left(\\beta,\\lambda\\right)$ is called the Lagrangian, and\n",
    "$\\lambda$ is the Lagrangian multiplier.\n",
    "\n",
    "The LM test is also called the *score test*, because the derivation is\n",
    "based on the score function of the restricted OLS estimator. Set the\n",
    "first-order condition of\n",
    "<a href=\"#eq:Lagran\" data-reference-type=\"eqref\" data-reference=\"eq:Lagran\">[eq:Lagran]</a>\n",
    "as zero: $$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial\\beta}L & =-\\frac{1}{n}X'\\left(y-X\\tilde{\\beta}\\right)+\\tilde{\\lambda}R=-\\frac{1}{n}X'e+\\frac{1}{n}X'X\\left(\\tilde{\\beta}-\\beta_{0}\\right)+R'\\tilde{\\lambda}=0.\\\\\n",
    "\\frac{\\partial}{\\partial\\lambda}L & =R\\tilde{\\beta}-r=R\\left(\\tilde{\\beta}-\\beta_{0}\\right)=0\\end{aligned}$$\n",
    "where $\\tilde{\\beta}$ and $\\tilde{\\lambda}$ denote the roots of these\n",
    "equation, and $\\beta_{0}$ is the hypothesized true value. The two\n",
    "equations can be written as a linear system\n",
    "$$\\begin{pmatrix}\\widehat{Q} & R'\\\\\n",
    "R & 0\n",
    "\\end{pmatrix}\\begin{pmatrix}\\tilde{\\beta}-\\beta_{0}\\\\\n",
    "\\tilde{\\lambda}\n",
    "\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{n}X'e\\\\\n",
    "0\n",
    "\\end{pmatrix},$$ where $\\hat{Q}=X'X/n$.\n",
    "\n",
    "$$\\begin{pmatrix}\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1} & \\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}\\\\\n",
    "\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1} & -(R'Q^{-1}R)^{-1}\n",
    "\\end{pmatrix}\\begin{pmatrix}\\widehat{Q} & R'\\\\\n",
    "R & 0\n",
    "\\end{pmatrix}=I_{K+q}.$$\n",
    "\n",
    "Given the above fact, we can explicitly express $$\\begin{aligned}\n",
    "\\begin{pmatrix}\\tilde{\\beta}-\\beta_{0}\\\\\n",
    "\\tilde{\\lambda}\n",
    "\\end{pmatrix}\\begin{aligned}=\\end{aligned}\n",
    " & \\begin{pmatrix}\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1} & \\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}\\\\\n",
    "\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1} & -(R'Q^{-1}R)^{-1}\n",
    "\\end{pmatrix}\\begin{pmatrix}\\frac{1}{n}X'e\\\\\n",
    "0\n",
    "\\end{pmatrix}\\\\\n",
    "= & \\begin{pmatrix}\\widehat{Q}^{-1}\\frac{1}{n}X'e-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{n}X'e\\\\\n",
    "\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{n}X'e\n",
    "\\end{pmatrix}\\end{aligned}$$ The $\\tilde{\\lambda}$ component is\n",
    "$$\\begin{aligned}\n",
    "\\sqrt{n}\\tilde{\\lambda} & =\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\\\\n",
    " & \\stackrel{d}{\\to}N\\left(0,\\left(RQ^{-1}R'\\right)^{-1}RQ^{-1}\\Omega Q^{-1}R'\\left(RQ^{-1}R'\\right)^{-1}\\right)\\end{aligned}$$\n",
    "as $\\widehat{Q}\\stackrel{p}{\\to}Q$. Denote\n",
    "$\\Sigma=\\left(RQ^{-1}R'\\right)^{-1}RQ^{-1}\\Omega Q^{-1}R'\\left(RQ^{-1}R'\\right)^{-1}$,\n",
    "we have\n",
    "$$n\\tilde{\\lambda}'\\Sigma^{-1}\\tilde{\\lambda}\\stackrel{d}{\\to}\\chi_{q}^{2}.$$\n",
    "Let\n",
    "$$\\widehat{\\Sigma}=\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\widehat{\\Omega}\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}.$$\n",
    "If $\\widehat{\\Omega}\\stackrel{p}{\\to}\\Omega$, we have $$\\begin{aligned}\n",
    "\\mathcal{LM} & =n\\tilde{\\lambda}'\\widehat{\\Sigma}^{-1}\\tilde{\\lambda}=n\\tilde{\\lambda}'\\Sigma^{-1}\\tilde{\\lambda}+n\\tilde{\\lambda}'\\left(\\widehat{\\Sigma}^{-1}-\\Sigma^{-1}\\right)\\tilde{\\lambda}\\\\\n",
    " & =n\\tilde{\\lambda}'\\Sigma^{-1}\\tilde{\\lambda}+o_{p}\\left(1\\right)\\stackrel{d}{\\to}\\chi_{q}^{2}.\\end{aligned}$$\n",
    "This is the general expression of the LM test.\n",
    "\n",
    "In the special case of homoskedasticity,\n",
    "$\\Sigma=\\sigma^{2}\\left(RQ^{-1}R'\\right)^{-1}RQ^{-1}QQ^{-1}R'\\left(RQ^{-1}R'\\right)^{-1}=\\sigma^{2}\\left(RQ^{-1}R'\\right)^{-1}.$\n",
    "Replace $\\Sigma$ with the estimated $\\hat{\\Sigma}$, we have\n",
    "$$\\begin{aligned}\\frac{n\\tilde{\\lambda}'R\\hat{Q}^{-1}R'\\tilde{\\lambda}}{\\hat{\\sigma}^{2}} & =\\frac{1}{n\\hat{\\sigma}^{2}}\\left(y-X\\tilde{\\beta}\\right)'X\\hat{Q}^{-1}R'(R\\hat{Q}^{-1}R')^{-1}R\\hat{Q}^{-1}X'\\left(y-X\\tilde{\\beta}\\right)\\stackrel{d}{\\to}\\chi_{q}^{2}.\\end{aligned}$$\n",
    "\n",
    "If we test the hypothesis that the optimal experience level is 20 years;\n",
    "$\\mbox{experience}^{*}=-\\frac{\\beta_{3}}{2\\beta_{4}}=20.$ We can replace\n",
    "$\\beta_{3}$ by $-40\\beta_{4}$ so we only need to estimate 3 slope\n",
    "coefficients in the OLS to construct the LM test. Moreover, the LM test\n",
    "is invariant to re-parametrization.\n",
    "\n",
    "### Likelihood-Ratio Test for Regression\n",
    "\n",
    "In the previous section we have discussed the LRT. Here we put it into\n",
    "the context regression with Gaussian error. Let $\\gamma=\\sigma_{e}^{2}$.\n",
    "Under the classical assumptions of normal regression model,\n",
    "$$L_{n}\\left(\\beta,\\gamma\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\gamma-\\frac{1}{2\\gamma}\\left(Y-X\\beta\\right)'\\left(Y-X\\beta\\right).$$\n",
    "For the unrestricted estimator, we know\n",
    "$$\\widehat{\\gamma}=\\gamma\\left(\\widehat{\\beta}\\right)=n^{-1}\\left(Y-X\\widehat{\\beta}\\right)'\\left(Y-X\\widehat{\\beta}\\right)$$\n",
    "and the sample log-likelihood function evaluated at the MLE is\n",
    "$$\\widehat{L}_{n}=L_{n}\\left(\\widehat{\\beta},\\widehat{\\gamma}\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\widehat{\\gamma}-\\frac{n}{2}$$\n",
    "and the restricted estimator\n",
    "$\\tilde{L}_{n}=L_{n}\\left(\\tilde{\\beta},\\tilde{\\gamma}\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\tilde{\\gamma}-\\frac{n}{2}$.\n",
    "The likelihood ratio is $$\\begin{aligned}\n",
    "\\mathcal{LR} & =2\\left(\\widehat{L}_{n}-\\tilde{L}_{n}\\right)=n\\log\\left(\\tilde{\\gamma}/\\widehat{\\gamma}\\right).\\end{aligned}$$\n",
    "If the normal regression is correctly specified, we can immediately\n",
    "conclude $\\mathcal{LR}\\stackrel{d}{\\to}\\chi_{q}^{2}.$\n",
    "\n",
    "Now we drop the Gaussian error assumption while keep the conditional\n",
    "homoskedasticity. In this case, the classical results is not applicable\n",
    "because $L_{n}\\left(\\beta,\\gamma\\right)$ is not a (genuine)\n",
    "log-likelihood function; instead it is the *quasi log-likelihood\n",
    "function*. Notice $$\\begin{aligned}\n",
    "\\mathcal{LR} & =n\\log\\left(1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}\\right)=n\\left(\\log1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}+O\\left(\\frac{\\left|\\tilde{\\gamma}-\\widehat{\\gamma}\\right|^{2}}{\\widehat{\\gamma}^{2}}\\right)\\right)\\nonumber \\\\\n",
    " & =n\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}+o_{p}\\left(1\\right)\\label{eq:LRT1}\\end{aligned}$$\n",
    "by a Taylor expansion of\n",
    "$\\log\\left(1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}\\right)$\n",
    "around $\\log1=0$. We focus on $$\\begin{aligned}\n",
    "n\\left(\\tilde{\\gamma}-\\widehat{\\gamma}\\right) & =n\\left(\\gamma\\left(\\tilde{\\beta}\\right)-\\gamma\\left(\\widehat{\\beta}\\right)\\right)\\nonumber \\\\\n",
    " & =n\\left(\\frac{\\partial\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+\\frac{1}{2}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)'\\frac{\\partial^{2}\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta\\partial\\beta'}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+O\\left(\\left\\Vert \\tilde{\\beta}-\\widehat{\\beta}\\right\\Vert _{2}^{3}\\right)\\right)\\nonumber \\\\\n",
    " & =\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)'\\widehat{Q}\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+o_{p}\\left(1\\right)\\label{eq:LRT2}\\end{aligned}$$\n",
    "where the last line follows by\n",
    "$\\frac{\\partial\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta}=-\\frac{2}{n}X'\\left(Y-X\\widehat{\\beta}\\right)=-\\frac{2}{n}X'\\widehat{e}=0$\n",
    "and\n",
    "$\\frac{1}{2}\\cdot\\frac{\\partial^{2}\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta\\partial\\beta'}=\\frac{1}{2}\\cdot\\frac{2}{n}X'X=\\widehat{Q}$.\n",
    "\n",
    "From the derivation of LM test, we have\n",
    "$$\\begin{aligned}\\sqrt{n}\\left(\\tilde{\\beta}-\\beta_{0}\\right) & =\\left(\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\right)\\frac{1}{\\sqrt{n}}X'e\\\\\n",
    " & =\\frac{1}{\\sqrt{n}}\\left(X'X\\right)^{-1}X'e-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\\\\n",
    " & =\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{0}\\right)-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e.\n",
    "\\end{aligned}$$ Rearrange the above equation to obtain\n",
    "$$\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)=-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e$$\n",
    "and thus the quadratic form $$\\begin{aligned}\n",
    " &  & \\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)'\\widehat{Q}\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)\\nonumber \\\\\n",
    " & = & \\frac{1}{\\sqrt{n}}e'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\widehat{Q}\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\nonumber \\\\\n",
    " & = & \\frac{1}{\\sqrt{n}}e'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\nonumber \\\\\n",
    " & = & \\frac{1}{\\sqrt{n}}e'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e.\\label{eq:LRT3}\\end{aligned}$$\n",
    "Collecting\n",
    "(<a href=\"#eq:LRT1\" data-reference-type=\"ref\" data-reference=\"eq:LRT1\">[eq:LRT1]</a>),\n",
    "(<a href=\"#eq:LRT2\" data-reference-type=\"ref\" data-reference=\"eq:LRT2\">[eq:LRT2]</a>)\n",
    "and\n",
    "(<a href=\"#eq:LRT3\" data-reference-type=\"ref\" data-reference=\"eq:LRT3\">[eq:LRT3]</a>),\n",
    "we have $$\\begin{aligned}\n",
    "\\mathcal{LR} & =n\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\cdot\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\sigma_{e}^{2}}+o_{p}\\left(1\\right)\\\\\n",
    " & =\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\frac{1}{\\sqrt{n}}\\frac{e}{\\sigma_{e}}'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'\\frac{e}{\\sigma_{e}}+o_{p}\\left(1\\right)\\end{aligned}$$\n",
    "Notice that under homoskedasticity, CLT gives $$\\begin{aligned}\n",
    "R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'\\frac{e}{\\sigma_{e}} & =R\\widehat{Q}^{-1/2}\\widehat{Q}^{-1/2}\\frac{1}{\\sqrt{n}}X'\\frac{e}{\\sigma_{e}}\\\\\n",
    " & \\stackrel{d}{\\to}RQ^{-1/2}\\times N\\left(0,I_{K}\\right)\\sim N\\left(0,RQ^{-1}R'\\right),\\end{aligned}$$\n",
    "and thus\n",
    "$$\\frac{1}{\\sqrt{n}}\\frac{e}{\\sigma_{e}}'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'\\frac{e}{\\sigma_{e}}\\stackrel{d}{\\to}\\chi_{q}^{2}.$$\n",
    "Moreover, $\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\stackrel{p}{\\to}1$.\n",
    "By Slutsky’s theorem, we conclude\n",
    "$$\\mathcal{LR}\\stackrel{d}{\\to}\\chi_{q}^{2}.$$ under homoskedasticity.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Applied econometrics is a field obsessed of hypothesis testing, in the\n",
    "hope to establish at least statistical association and ideally\n",
    "causality. Hypothesis testing is a fundamentally important topic in\n",
    "statistics. The states and the decisions in Table\n",
    "<a href=\"#tab:Decisions-and-States\" data-reference-type=\"ref\" data-reference=\"tab:Decisions-and-States\">[tab:Decisions-and-States]</a>\n",
    "remind us the intrinsic connections with game theory in economics. I, a\n",
    "game player, plays a sequential game against the “nature”.\n",
    "\n",
    "Step0:  \n",
    "The parameter space $\\Theta$ is partitioned into the null hypothesis\n",
    "$\\Theta_{0}$ and the alternative hypothesis $\\Theta_{1}$ according to a\n",
    "scientific theory.\n",
    "\n",
    "Step1:  \n",
    "Before I observe the data, I design a test function $\\phi$ according to\n",
    "$\\Theta_{0}$ and $\\Theta_{1}$. In game theory terminology, the\n",
    "contingency plan $\\phi$ is my *strategy*.\n",
    "\n",
    "Step2:  \n",
    "Once I observe the fixed data $\\mathbf{x}$, I act according to the\n",
    "instruction of $\\phi\\left(\\mathbf{x}\\right)$ — either accept\n",
    "$\\Theta_{0}$ or reject $\\Theta_{0}$.\n",
    "\n",
    "Step3:  \n",
    "Nature reveals the true parameter $\\theta^{*}$ behind $\\mathbf{x}$. Then\n",
    "I can evaluate the gain/loss of my decision\n",
    "$\\phi\\left(\\mathbf{x}\\right)$.\n",
    "\n",
    "When the loss function (negative payoff) is specified as\n",
    "$$\\mathscr{L}\\left(\\theta,\\phi\\left(\\mathbf{x}\\right)\\right)=\\phi\\left(\\mathbf{x}\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\phi\\left(\\mathbf{x}\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} ,$$\n",
    "the randomness of the data will incur the risk (expected loss)\n",
    "$$\\mathscr{R}\\left(\\theta,\\phi\\right)=E\\left[\\mathscr{L}\\left(\\theta,\\phi\\left(\\mathbf{x}\\right)\\right)\\right]=\\beta_{\\phi}\\left(\\theta\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\beta_{\\phi}\\left(\\theta\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} .$$\n",
    "I am a rational person. I understand the structure of the game and I\n",
    "want to do a good job in Step 1 in designing my strategy. I want to\n",
    "minimize my risk.\n",
    "\n",
    "If I am a frequentist, one and only one of\n",
    "$1\\left\\{ \\theta\\in\\Theta_{0}\\right\\}$ and\n",
    "$1\\left\\{ \\theta\\in\\Theta_{1}\\right\\}$ can happen. An unbiased test\n",
    "makes sure\n",
    "$\\sup_{\\theta\\in\\Theta_{0}}\\beta_{\\phi}\\left(\\theta\\right)\\leq\\alpha$.\n",
    "When many tests are unbiased, ideally I would like to pick the best one.\n",
    "If it exists, in a class $\\Psi_{\\alpha}$ of unbiased tests of size\n",
    "$\\alpha$ the uniformly most power test $\\phi^{*}$ satisfies\n",
    "$\\mathscr{R}\\left(\\theta,\\phi^{*}\\right)\\geq\\sup_{\\phi\\in\\Psi_{\\alpha}}\\mathscr{R}\\left(\\theta,\\phi\\right)$\n",
    "for every $\\theta\\in\\Theta_{1}$. For simple versus simple tests, LRT is\n",
    "the uniformly most powerful test according to Neyman-Pearson Lemma.\n",
    "\n",
    "If I am a Bayesian, I do not mind imposing probability (weight) on the\n",
    "parameter space, which is my prior belief $\\pi\\left(\\theta\\right)$. My\n",
    "Bayesian risk becomes $$\\begin{aligned}\n",
    "\\mathscr{BR}\\left(\\pi,\\phi\\right) & =E_{\\pi\\left(\\theta\\right)}\\left[\\mathscr{R}\\left(\\theta,\\phi\\right)\\right]=\\int\\left[\\beta_{\\phi}\\left(\\theta\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\beta_{\\phi}\\left(\\theta\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} \\right]\\pi\\left(\\theta\\right)d\\theta\\\\\n",
    " & =\\int_{\\left\\{ \\theta\\in\\Theta_{0}\\right\\} }\\beta_{\\phi}\\left(\\theta\\right)\\pi\\left(\\theta\\right)d\\theta+\\int_{\\left\\{ \\theta\\in\\Theta_{1}\\right\\} }(1-\\beta_{\\phi}\\left(\\theta\\right))\\pi\\left(\\theta\\right)d\\theta.\\end{aligned}$$\n",
    "This is the average (with respect to $\\pi\\left(\\theta\\right)$) risk over\n",
    "the null and the alternative.\n",
    "\n",
    "**Historical notes**: Hypothesis testing started to take the modern\n",
    "shape at the beginning of the 20th century. Karl Pearson (1957–1936)\n",
    "laid the foundation of hypothesis testing and introduced the $\\chi^{2}$\n",
    "test, the $p$-value, among many other concepts that we keep using today.\n",
    "Neyman-Pearson Lemma was named after Jerzy Neyman (1894–1981) and Egon\n",
    "Pearson (1895–1980), Karl’s son.\n",
    "\n",
    "**Further reading**: @young2005essentials is a concise but in-depth\n",
    "reference for statistical inference.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### Neyman-Pearson Lemma\n",
    "\n",
    "We have discussed an example of the uniformly most power test in the\n",
    "Gaussian location model. Under the likelihood principle, if the test is\n",
    "a simple versus simple (the null hypothesis is a singleton $\\theta_{0}$\n",
    "and the alternative hypothesis is another single point $\\theta_{1}$),\n",
    "then LRT $$\\begin{aligned}\n",
    "\\phi\\left(\\mathbf{X}\\right) & :=1\\left\\{ \\mathcal{LR}\\geq c_{LR}\\right\\} ,\\end{aligned}$$\n",
    "where $c_{LR}$ is the critical value depending on the size of the the\n",
    "test, is a uniformly most powerful test. This result is the well-known\n",
    "Neyman-Pearson Lemma.\n",
    "\n",
    "Notice\n",
    "$\\exp\\left(L_{n}\\left(\\theta\\right)\\right)=\\Pi_{i}f\\left(x_{i};\\theta\\right)=f\\left(\\mathbf{x};\\theta\\right)$\n",
    "where $f\\left(\\mathbf{x};\\theta_{0}\\right)$ is the joint density of\n",
    "$\\left(x_{1},\\ldots,x_{n}\\right)$, the LRT can be equivalently written\n",
    "in likelihood ratio form (without log)\n",
    "$$\\phi\\left(\\mathbf{X}\\right)=1\\left\\{ f\\left(\\mathbf{X};\\widehat{\\theta}\\right)/f\\left(\\mathbf{X};\\theta_{0}\\right)\\geq c\\right\\}$$\n",
    "where $c:=\\exp\\left(c_{LR}/2\\right)$.\n",
    "\n",
    "To see its is the most power test in the simple to simple context,\n",
    "consider another test $\\psi$ of the same size at the single null\n",
    "hypothesis\n",
    "$\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\theta_{0}\\right)=\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)=\\alpha$,\n",
    "where $f\\left(\\mathbf{x};\\theta_{0}\\right)=$ is the joint density of the\n",
    "sample $\\mathbf{X}$. For any constant $c>0$, the power of $\\phi$ at the\n",
    "alternative $\\theta_{1}$ is $$\\begin{aligned}\n",
    "E_{\\theta_{1}}\\left[\\phi\\left(\\mathbf{X}\\right)\\right] & =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)\\nonumber \\\\\n",
    " & =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)-c\\left[\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)-\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\right]\\nonumber \\\\\n",
    " & =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)-c\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\nonumber \\\\\n",
    " & =\\int\\phi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right).\\label{eq:NP1}\\end{aligned}$$\n",
    "Define\n",
    "$\\xi_{c}:=f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)$.\n",
    "The fact that $\\phi\\left(\\mathbf{x}\\right)=1$ if $\\xi_{c}\\geq0$ and\n",
    "$\\phi\\left(\\mathbf{x}\\right)=0$ if $\\xi_{c}<0$ implies $$\\begin{aligned}\n",
    " &  & \\int\\phi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)=\\int\\phi\\left(\\mathbf{x}\\right)\\xi_{c}\\\\\n",
    " & = & \\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\phi\\left(\\mathbf{x}\\right)\\xi_{c}+\\int_{\\left\\{ \\xi_{c}<0\\right\\} }\\phi\\left(\\mathbf{x}\\right)\\xi_{c}=\\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\xi_{c}=\\int\\xi_{c}\\cdot1\\left\\{ \\xi_{c}\\geq0\\right\\} \\\\\n",
    " & \\geq & \\int\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\cdot1\\left\\{ \\xi_{c}\\geq0\\right\\} =\\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\\\\n",
    " & \\geq & \\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}+\\int_{\\left\\{ \\xi_{c}<0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}=\\int\\psi\\left(x\\right)\\xi_{c}\\\\\n",
    " & = & \\int\\psi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)\\end{aligned}$$\n",
    "where the first inequality follows because the test function\n",
    "$0\\leq\\psi\\left(\\mathbf{x}\\right)\\leq1$ for any realization of\n",
    "$\\mathbf{x}$, and where the second inequality holds because\n",
    "$\\int_{\\left\\{ \\xi_{c}<0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\leq0$.\n",
    "We continue\n",
    "<a href=\"#eq:NP1\" data-reference-type=\"eqref\" data-reference=\"eq:NP1\">[eq:NP1]</a>:\n",
    "$$\\begin{aligned}\n",
    "E_{\\theta_{1}}\\left[\\phi\\left(\\mathbf{X}\\right)\\right] & \\geq & \\int\\psi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\\\\n",
    " & = & \\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)=E_{\\theta_{1}}\\left[\\psi\\left(\\mathbf{X}\\right)\\right].\\end{aligned}$$\n",
    "In other words, $\\phi\\left(\\mathbf{X}\\right)$ is more powerful at\n",
    "$\\theta_{1}$ than any other test $\\psi$ of the same size at the null.\n",
    "\n",
    "Neyman-Pearson lemma establishes the optimality of LRT in single versus\n",
    "simple hypothesis testing. It can be generalized to show the existence\n",
    "of the uniformly most power test in one sided composite null hypothesis\n",
    "$H_{0}:\\theta\\leq\\theta_{0}$ or $H_{0}:\\theta\\geq\\theta_{0}$ in the\n",
    "parametric class of distributions exhibiting *monotone likelihood\n",
    "ratio*.\n",
    "\n",
    "`Zhentao Shi. Nov 4, 2020.`"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
